`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
[codecarbon INFO @ 00:22:44] Energy consumed for RAM : 0.000196 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:22:44] Energy consumed for all GPUs : 0.001038 kWh. Total GPU Power : 249.07603937658658 W
[codecarbon INFO @ 00:22:44] Energy consumed for all CPUs : 0.000229 kWh. Total CPU Power : 54.97827430757787 W
[codecarbon INFO @ 00:22:44] 0.001464 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:22:59] Energy consumed for RAM : 0.000392 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:22:59] Energy consumed for all GPUs : 0.002114 kWh. Total GPU Power : 258.30156356479944 W
[codecarbon INFO @ 00:22:59] Energy consumed for all CPUs : 0.000458 kWh. Total CPU Power : 54.79669335272753 W
[codecarbon INFO @ 00:22:59] 0.002964 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:23:14] Energy consumed for RAM : 0.000588 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:23:14] Energy consumed for all GPUs : 0.003197 kWh. Total GPU Power : 259.9380557950335 W
[codecarbon INFO @ 00:23:14] Energy consumed for all CPUs : 0.000687 kWh. Total CPU Power : 55.17818262747288 W
[codecarbon INFO @ 00:23:14] 0.004472 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:23:29] Energy consumed for RAM : 0.000784 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:23:29] Energy consumed for all GPUs : 0.004286 kWh. Total GPU Power : 261.7043301137596 W
[codecarbon INFO @ 00:23:29] Energy consumed for all CPUs : 0.000917 kWh. Total CPU Power : 55.00415976473849 W
[codecarbon INFO @ 00:23:29] 0.005987 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:23:44] Energy consumed for RAM : 0.000980 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:23:44] Energy consumed for all GPUs : 0.005381 kWh. Total GPU Power : 262.92991854744497 W
[codecarbon INFO @ 00:23:44] Energy consumed for all CPUs : 0.001145 kWh. Total CPU Power : 54.80267997857254 W
[codecarbon INFO @ 00:23:44] 0.007506 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:23:59] Energy consumed for RAM : 0.001176 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:23:59] Energy consumed for all GPUs : 0.006482 kWh. Total GPU Power : 264.4441495075702 W
[codecarbon INFO @ 00:23:59] Energy consumed for all CPUs : 0.001374 kWh. Total CPU Power : 54.97606164660213 W
[codecarbon INFO @ 00:23:59] 0.009032 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:24:14] Energy consumed for RAM : 0.001372 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:24:14] Energy consumed for all GPUs : 0.007593 kWh. Total GPU Power : 266.8294972179751 W
[codecarbon INFO @ 00:24:14] Energy consumed for all CPUs : 0.001603 kWh. Total CPU Power : 54.99981264153858 W
[codecarbon INFO @ 00:24:14] 0.010567 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:24:29] Energy consumed for RAM : 0.001568 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:24:29] Energy consumed for all GPUs : 0.008696 kWh. Total GPU Power : 265.08078555913704 W
[codecarbon INFO @ 00:24:29] Energy consumed for all CPUs : 0.001832 kWh. Total CPU Power : 55.034957123077355 W
[codecarbon INFO @ 00:24:29] 0.012096 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:24:44] Energy consumed for RAM : 0.001764 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:24:44] Energy consumed for all GPUs : 0.009793 kWh. Total GPU Power : 263.44144427893997 W
[codecarbon INFO @ 00:24:44] Energy consumed for all CPUs : 0.002060 kWh. Total CPU Power : 54.6630684734902 W
[codecarbon INFO @ 00:24:44] 0.013617 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:24:59] Energy consumed for RAM : 0.001960 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:24:59] Energy consumed for all GPUs : 0.010893 kWh. Total GPU Power : 263.990778910315 W
[codecarbon INFO @ 00:24:59] Energy consumed for all CPUs : 0.002287 kWh. Total CPU Power : 54.60682757995554 W
[codecarbon INFO @ 00:24:59] 0.015140 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:25:14] Energy consumed for RAM : 0.002156 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:25:14] Energy consumed for all GPUs : 0.011995 kWh. Total GPU Power : 264.71968752457525 W
[codecarbon INFO @ 00:25:14] Energy consumed for all CPUs : 0.002516 kWh. Total CPU Power : 54.77104964480384 W
[codecarbon INFO @ 00:25:14] 0.016666 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:25:29] Energy consumed for RAM : 0.002352 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:25:29] Energy consumed for all GPUs : 0.013107 kWh. Total GPU Power : 266.98578435683515 W
[codecarbon INFO @ 00:25:29] Energy consumed for all CPUs : 0.002744 kWh. Total CPU Power : 54.884080348675546 W
[codecarbon INFO @ 00:25:29] 0.018203 kWh of electricity used since the beginning.

***** Running Evaluation *****
  Num examples = 987
  Batch size = 2
[codecarbon INFO @ 00:25:44] Energy consumed for RAM : 0.002547 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:25:44] Energy consumed for all GPUs : 0.014222 kWh. Total GPU Power : 267.6951427988135 W
[codecarbon INFO @ 00:25:44] Energy consumed for all CPUs : 0.002973 kWh. Total CPU Power : 55.03351707115661 W
[codecarbon INFO @ 00:25:44] 0.019743 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:25:59] Energy consumed for RAM : 0.002743 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:25:59] Energy consumed for all GPUs : 0.015379 kWh. Total GPU Power : 277.91852751671973 W
[codecarbon INFO @ 00:25:59] Energy consumed for all CPUs : 0.003206 kWh. Total CPU Power : 55.85024401087162 W
[codecarbon INFO @ 00:25:59] 0.021329 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:26:14] Energy consumed for RAM : 0.002939 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:26:14] Energy consumed for all GPUs : 0.016535 kWh. Total GPU Power : 277.53879098087776 W
[codecarbon INFO @ 00:26:14] Energy consumed for all CPUs : 0.003439 kWh. Total CPU Power : 55.83209089574693 W
[codecarbon INFO @ 00:26:14] 0.022913 kWh of electricity used since the beginning.
Saving model checkpoint to /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-fine-tuned_with_labels/checkpoint-375
loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-fine-tuned_with_labels/checkpoint-375/tokenizer_config.json
Special tokens file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-fine-tuned_with_labels/checkpoint-375/special_tokens_map.json
/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
[codecarbon INFO @ 00:26:29] Energy consumed for RAM : 0.003135 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:26:29] Energy consumed for all GPUs : 0.017622 kWh. Total GPU Power : 260.9799215425818 W
[codecarbon INFO @ 00:26:29] Energy consumed for all CPUs : 0.003662 kWh. Total CPU Power : 53.56658980083434 W
[codecarbon INFO @ 00:26:29] 0.024419 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:26:44] Energy consumed for RAM : 0.003331 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:26:44] Energy consumed for all GPUs : 0.018732 kWh. Total GPU Power : 266.6607375471429 W
[codecarbon INFO @ 00:26:44] Energy consumed for all CPUs : 0.003890 kWh. Total CPU Power : 54.866108494622985 W
[codecarbon INFO @ 00:26:44] 0.025954 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:26:59] Energy consumed for RAM : 0.003527 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:26:59] Energy consumed for all GPUs : 0.019846 kWh. Total GPU Power : 267.49177522515896 W
[codecarbon INFO @ 00:26:59] Energy consumed for all CPUs : 0.004119 kWh. Total CPU Power : 54.81013373802462 W
[codecarbon INFO @ 00:26:59] 0.027492 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:27:14] Energy consumed for RAM : 0.003723 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:27:14] Energy consumed for all GPUs : 0.020961 kWh. Total GPU Power : 267.70637131044174 W
[codecarbon INFO @ 00:27:14] Energy consumed for all CPUs : 0.004347 kWh. Total CPU Power : 54.837555176993774 W
[codecarbon INFO @ 00:27:14] 0.029031 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:27:29] Energy consumed for RAM : 0.003919 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:27:29] Energy consumed for all GPUs : 0.022071 kWh. Total GPU Power : 266.5010657254479 W
[codecarbon INFO @ 00:27:29] Energy consumed for all CPUs : 0.004575 kWh. Total CPU Power : 54.639935047458366 W
[codecarbon INFO @ 00:27:29] 0.030565 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:27:44] Energy consumed for RAM : 0.004115 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:27:44] Energy consumed for all GPUs : 0.023181 kWh. Total GPU Power : 266.7411552092491 W
[codecarbon INFO @ 00:27:44] Energy consumed for all CPUs : 0.004803 kWh. Total CPU Power : 54.8821557454866 W
[codecarbon INFO @ 00:27:44] 0.032100 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:27:59] Energy consumed for RAM : 0.004311 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:27:59] Energy consumed for all GPUs : 0.024292 kWh. Total GPU Power : 266.7719660464141 W
[codecarbon INFO @ 00:27:59] Energy consumed for all CPUs : 0.005032 kWh. Total CPU Power : 54.916303871906464 W
[codecarbon INFO @ 00:27:59] 0.033636 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:28:14] Energy consumed for RAM : 0.004507 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:28:14] Energy consumed for all GPUs : 0.025402 kWh. Total GPU Power : 266.33411280413395 W
[codecarbon INFO @ 00:28:14] Energy consumed for all CPUs : 0.005261 kWh. Total CPU Power : 54.84585916502165 W
[codecarbon INFO @ 00:28:14] 0.035169 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:28:29] Energy consumed for RAM : 0.004703 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:28:29] Energy consumed for all GPUs : 0.026511 kWh. Total GPU Power : 266.46728023320776 W
[codecarbon INFO @ 00:28:29] Energy consumed for all CPUs : 0.005489 kWh. Total CPU Power : 54.736499647811456 W
[codecarbon INFO @ 00:28:29] 0.036703 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:28:44] Energy consumed for RAM : 0.004899 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:28:44] Energy consumed for all GPUs : 0.027623 kWh. Total GPU Power : 267.02680524364195 W
[codecarbon INFO @ 00:28:44] Energy consumed for all CPUs : 0.005718 kWh. Total CPU Power : 54.960769854836286 W
[codecarbon INFO @ 00:28:44] 0.038240 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:28:59] Energy consumed for RAM : 0.005095 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:28:59] Energy consumed for all GPUs : 0.028734 kWh. Total GPU Power : 266.60169417527277 W
[codecarbon INFO @ 00:28:59] Energy consumed for all CPUs : 0.005946 kWh. Total CPU Power : 54.86967159744227 W
[codecarbon INFO @ 00:28:59] 0.039775 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:29:14] Energy consumed for RAM : 0.005291 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:29:14] Energy consumed for all GPUs : 0.029844 kWh. Total GPU Power : 266.5760692492227 W
[codecarbon INFO @ 00:29:14] Energy consumed for all CPUs : 0.006176 kWh. Total CPU Power : 55.039339406725944 W
[codecarbon INFO @ 00:29:14] 0.041311 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:29:29] Energy consumed for RAM : 0.005487 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:29:29] Energy consumed for all GPUs : 0.030965 kWh. Total GPU Power : 269.2124474931711 W
[codecarbon INFO @ 00:29:29] Energy consumed for all CPUs : 0.006403 kWh. Total CPU Power : 54.57083024751968 W
[codecarbon INFO @ 00:29:29] 0.042855 kWh of electricity used since the beginning.
Saving model checkpoint to /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-fine-tuned_with_labels/checkpoint-750
loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-fine-tuned_with_labels/checkpoint-750/tokenizer_config.json
Special tokens file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-fine-tuned_with_labels/checkpoint-750/special_tokens_map.json

***** Running Evaluation *****
  Num examples = 987
  Batch size = 2
[codecarbon INFO @ 00:29:44] Energy consumed for RAM : 0.005683 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:29:44] Energy consumed for all GPUs : 0.032090 kWh. Total GPU Power : 270.20956509529304 W
[codecarbon INFO @ 00:29:44] Energy consumed for all CPUs : 0.006629 kWh. Total CPU Power : 54.344377342002346 W
[codecarbon INFO @ 00:29:44] 0.044402 kWh of electricity used since the beginning.
[codecarbon INFO @ 00:29:59] Energy consumed for RAM : 0.005879 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:29:59] Energy consumed for all GPUs : 0.033255 kWh. Total GPU Power : 279.79306925228116 W
[codecarbon INFO @ 00:29:59] Energy consumed for all CPUs : 0.006861 kWh. Total CPU Power : 55.670884467506966 W
[codecarbon INFO @ 00:29:59] 0.045995 kWh of electricity used since the beginning.
Saving model checkpoint to /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-fine-tuned_with_labels/checkpoint-750
loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-fine-tuned_with_labels/checkpoint-750/tokenizer_config.json
Special tokens file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-fine-tuned_with_labels/checkpoint-750/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


[codecarbon INFO @ 00:30:11] Energy consumed for RAM : 0.006035 kWh. RAM Power : 47.068857192993164 W
[codecarbon INFO @ 00:30:11] Energy consumed for all GPUs : 0.034140 kWh. Total GPU Power : 267.44508657619554 W
[codecarbon INFO @ 00:30:11] Energy consumed for all CPUs : 0.007039 kWh. Total CPU Power : 53.78917239950431 W
[codecarbon INFO @ 00:30:11] 0.047213 kWh of electricity used since the beginning.
Saving model checkpoint to /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-fine-tuned_with_labels
loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-fine-tuned_with_labels/tokenizer_config.json
Special tokens file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-fine-tuned_with_labels/special_tokens_map.json
