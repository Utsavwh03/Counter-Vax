{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.device_count.cache_clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:27<00:00, 43.87s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Generate a counter-argument for the tweet: Vaccines are bad for health.\\n\\nVaccines are bad for health.\\n\\n\\n# Answer:Vaccines are not bad for health. In fact, they are one of the most effective ways to prevent infectious diseases and save millions of lives. Vaccines work by stimulating the immune system to produce antibodies that fight off specific pathogens. They have been rigorously tested and proven to be safe and effective by scientific research and regulatory agencies. Vaccines protect not only the individual who receives them, but also the community by reducing the spread of diseases and creating herd immunity. Vaccines have eradicated or controlled many deadly diseases, such as smallpox, polio, measles, and tetanus. Vaccines are a vital tool for public health and should be trusted and supported by evidence-based information.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Generate a counter-argument for the tweet: Vaccines are bad for health\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=250)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_column_without_labels(row):\n",
    "        # Initialize an empty list to store the messages.\n",
    "    messages = []\n",
    "    \n",
    "    # Create a 'user' message dictionary with 'content' and 'role' keys.\n",
    "#     print(row['text'])\n",
    "    user = {\n",
    "        \"content\": f\"Generate Counter Argument for the anti-vaccine tweet:\\n Tweet: {row['text']}\\n##Output: \",\n",
    "        \"role\": \"user\"\n",
    "    }\n",
    "    \n",
    "    # Append the 'user' message to the 'messages' list.\n",
    "    messages.append(user)\n",
    "    \n",
    "    # Create an 'assistant' message dictionary with 'content' and 'role' keys.\n",
    "    assistant = {\n",
    "        \"content\": f\"{row['counter_argument']}\",\n",
    "        \"role\": \"assistant\"\n",
    "    }\n",
    "    \n",
    "    # Append the 'assistant' message to the 'messages' list.\n",
    "    messages.append(assistant)\n",
    "    \n",
    "    # Return a dictionary with a 'messages' key and the 'messages' list as its value.\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def format_dataset_chatml(row):\n",
    "    return {\"text\": tokenizer.apply_chat_template(row[\"messages\"], add_generation_prompt=False, tokenize=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL DESCRIPTIONS\n",
    "\n",
    "labels_map = {}\n",
    "labels_map['religious'] = \"religious beliefs and their influence on views about vaccines\"\n",
    "labels_map['political'] = \"the political factors that affect perceptions of vaccine use\"\n",
    "labels_map['ingredients'] = \"concerns about the ingredients and chemical components in vaccines\"\n",
    "labels_map['unnecessary'] = \"the importance and necessity of getting vaccinated to prevent diseases\"\n",
    "labels_map['conspiracy'] = \"conspiracy theories suggesting hidden motives behind vaccination efforts\"\n",
    "labels_map['mandatory'] = \"the debate over personal choice versus mandates in vaccination policies\"\n",
    "labels_map['ineffective'] = \"evidence and reasons that support the effectiveness of vaccines\"\n",
    "labels_map['side-effect'] = \"potential side effects and adverse reactions associated with vaccines\"\n",
    "labels_map['pharma'] = \"the role of pharmaceutical companies and concerns about profit motives\"\n",
    "labels_map['rushed'] = \"claims that vaccines were approved or developed without sufficient testing\"\n",
    "labels_map['country'] = \"national biases and objections to vaccines produced by specific countries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_column_with_labels(row):\n",
    "        # Initialize an empty list to store the messages.\n",
    "    messages = []\n",
    "    # print(row)\n",
    "    prompt= f\"Generate Counter Argument for the anti-vaccine tweet:\\n Tweet: {row['text']}\\n Talk About \"\n",
    "\n",
    "    lab=row['labels'].split()\n",
    "    if isinstance(lab, list):\n",
    "            mapped_labels = \" and \".join([labels_map.get(l, \"\") for l in lab])\n",
    "    else:\n",
    "            mapped_labels = labels_map.get(lab, \"\")\n",
    "    # print(mapped_labels)\n",
    "    prompt+= mapped_labels\n",
    "    prompt+= \" ##Output:\"\n",
    "    user = {\n",
    "        \"content\": prompt,\n",
    "        \"role\": \"user\"\n",
    "    }\n",
    "    \n",
    "    # Append the 'user' message to the 'messages' list.\n",
    "    messages.append(user)\n",
    "    \n",
    "    # Create an 'assistant' message dictionary with 'content' and 'role' keys.\n",
    "    assistant = {\n",
    "        \"content\": f\"{row['counter_argument']}\",\n",
    "        \"role\": \"assistant\"\n",
    "    }\n",
    "    \n",
    "    # Append the 'assistant' message to the 'messages' list.\n",
    "    messages.append(assistant)\n",
    "    \n",
    "    # Return a dictionary with a 'messages' key and the 'messages' list as its value.\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def format_dataset_chatml(row):\n",
    "    return {\"text\": tokenizer.apply_chat_template(row[\"messages\"], add_generation_prompt=False, tokenize=False)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "train_data= load_dataset('csv', data_files='/home/sohampoddar/HDD2/utsav/Data/train_CA_with_label_desc.csv',split='train')\n",
    "val_data= load_dataset('csv', data_files='/home/sohampoddar/HDD2/utsav/Data/val_data_with_labels_desc.csv',split='train')\n",
    "\n",
    "# train_data=train_data[:5]\n",
    "dataset_chatml = train_data.map(create_message_column_with_labels)\n",
    "dataset_chatml = dataset_chatml.map(format_dataset_chatml)\n",
    "\n",
    "val_dataset_chatml=val_data.map(create_message_column_with_labels)\n",
    "val_dataset_chatml=val_dataset_chatml.map(format_dataset_chatml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "train_data= load_dataset('csv', data_files='/home/sohampoddar/HDD2/utsav/Data/train_CA_without_labels.csv',split='train')\n",
    "val_data= load_dataset('csv', data_files='/home/sohampoddar/HDD2/utsav/Data/val_CA_without_labels.csv',split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:00<00:00, 7157.21 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 987/987 [00:00<00:00, 7543.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_chatml = train_data.map(create_message_column_with_labels)\n",
    "\n",
    "dataset_chatml = dataset_chatml.map(format_dataset_chatml)\n",
    "\n",
    "val_dataset_chatml=val_data.map(create_message_column_with_labels)\n",
    "val_dataset_chatml=val_dataset_chatml.map(format_dataset_chatml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import peft ,trl\n",
    "\n",
    "# 'load_dataset' is a function from the 'datasets' library by Hugging Face which allows you to load a dataset.\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, TaskType, PeftModel\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "# 'SFTTrainer' is a class from the 'trl' library that provides a trainer for soft fine-tuning.\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flash_attention_2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if torch.cuda.is_bf16_supported():\n",
    "  compute_dtype = torch.bfloat16\n",
    "  attn_implementation = 'flash_attention_2'\n",
    "# If bfloat16 is not supported, 'compute_dtype' is set to 'torch.float16' and 'attn_implementation' is set to 'sdpa'.\n",
    "else:\n",
    "  compute_dtype = torch.float16\n",
    "  attn_implementation = 'sdpa'\n",
    "\n",
    "# # This line of code is used to print the value of 'attn_implementation', which indicates the chosen attention implementation.\n",
    "print(attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:30<00:00, 15.40s/it]\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as padding token if none is defined\n",
    "\n",
    "# Set the pad_token_id\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Set padding side if needed\n",
    "tokenizer.padding_side = 'right'\n",
    "# set cuda available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # 'cuda' will point to the visible GPU (e.g., GPU 0 as set by CUDA_VISIBLE_DEVICES)\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "use_4bit = True\n",
    "# 'bnb_4bit_compute_dtype' is the data type that should be used for computations with the 4-bit base model. In this case, it is set to 'bfloat16'.\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "\n",
    "# 'bnb_4bit_quant_type' is the type of quantization that should be used for the 4-bit base model. In this case, it is set to 'nf4'.\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# 'use_double_quant' is a boolean that controls whether nested quantization should be used for the 4-bit base model.\n",
    "use_double_quant = True\n",
    "bnb_config= BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_use_double_quant=True,\n",
    "      bnb_4bit_quant_type=\"nf4\",\n",
    "      bnb_4bit_compute_dtype=torch.bfloat16\n",
    "  )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_id, torch_dtype=compute_dtype, trust_remote_code=True, quantization_config=bnb_config, device_map=\"auto\",\n",
    "#           attn_implementation=attn_implementation\n",
    ")\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    "peft_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=target_modules,\n",
    ")\n",
    "# model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/transformers/training_args.py:1570: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = TrainingArguments(\n",
    "        output_dir=\"/home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/phi-3-mini-4k-instruct\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        do_eval=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        log_level=\"debug\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=1e-4,\n",
    "        logging_dir=None,  # Directory for storing logs\n",
    "        logging_steps=100,     # Log after every 100 steps\n",
    "        logging_strategy=\"steps\",  # Log by steps instead of epochs\n",
    "        fp16=True,\n",
    "#         bf16 = torch.cuda.is_bf16_supported(),\n",
    "        eval_steps=100,\n",
    "        num_train_epochs=2,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "#     report_to=\"wandb\",\n",
    "        seed=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/trl/trainer/utils.py:162: UserWarning: The pad_token_id and eos_token_id values of this tokenizer are identical. If you are planning for multi-turn training, it can result in the model continuously generating questions and answers without eos token. To avoid this, set the pad_token_id to a different value.\n",
      "  warnings.warn(\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/transformers/training_args.py:1570: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/transformers/training_args.py:2065: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/transformers/training_args.py:1570: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/transformers/training_args.py:2065: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:00<00:00, 6533.52 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 987/987 [00:00<00:00, 9080.71 examples/s]\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:402: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Using auto half precision backend\n",
      "[codecarbon INFO @ 20:47:35] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 20:47:35] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 20:47:35] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 20:47:35] [setup] CPU Tracking...\n",
      "[codecarbon INFO @ 20:47:35] Tracking Intel CPU via RAPL interface\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[codecarbon INFO @ 20:47:36] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 20:47:36]   Platform system: Linux-5.15.0-124-generic-x86_64-with-glibc2.31\n",
      "[codecarbon INFO @ 20:47:36]   Python version: 3.9.17\n",
      "[codecarbon INFO @ 20:47:36]   CodeCarbon version: 2.3.2\n",
      "[codecarbon INFO @ 20:47:36]   Available RAM : 125.517 GB\n",
      "[codecarbon INFO @ 20:47:36]   CPU count: 40\n",
      "[codecarbon INFO @ 20:47:36]   CPU model: Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz\n",
      "[codecarbon INFO @ 20:47:36]   GPU count: 4\n",
      "[codecarbon INFO @ 20:47:36]   GPU model: 2 x NVIDIA RTX A60002 x NVIDIA RTX A5000\n",
      "Currently training with a batch size of: 2\n",
      "***** Running training *****\n",
      "  Num examples = 1,500\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 750\n",
      "  Number of trainable parameters = 2,228,224\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutsavdhanuka03\u001b[0m (\u001b[33mutsavdhanuka03-iit-kharagpur\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/antpc/HDD2/sohampoddar/utsav/Model_Codes_with_labels/wandb/run-20250117_204744-6c7nbjsy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/utsavdhanuka03-iit-kharagpur/huggingface/runs/6c7nbjsy' target=\"_blank\">/home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/phi-3-mini-4k-instruct</a></strong> to <a href='https://wandb.ai/utsavdhanuka03-iit-kharagpur/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/utsavdhanuka03-iit-kharagpur/huggingface' target=\"_blank\">https://wandb.ai/utsavdhanuka03-iit-kharagpur/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/utsavdhanuka03-iit-kharagpur/huggingface/runs/6c7nbjsy' target=\"_blank\">https://wandb.ai/utsavdhanuka03-iit-kharagpur/huggingface/runs/6c7nbjsy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 17:00, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.734800</td>\n",
       "      <td>0.712069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.664400</td>\n",
       "      <td>0.701278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 20:48:00] Energy consumed for RAM : 0.000196 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:48:00] Energy consumed for all GPUs : 0.001122 kWh. Total GPU Power : 269.0838128787333 W\n",
      "[codecarbon INFO @ 20:48:00] Energy consumed for all CPUs : 0.000230 kWh. Total CPU Power : 55.13048867713714 W\n",
      "[codecarbon INFO @ 20:48:00] 0.001548 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:48:15] Energy consumed for RAM : 0.000392 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:48:15] Energy consumed for all GPUs : 0.002290 kWh. Total GPU Power : 280.6762862779155 W\n",
      "[codecarbon INFO @ 20:48:15] Energy consumed for all CPUs : 0.000460 kWh. Total CPU Power : 55.12966090180615 W\n",
      "[codecarbon INFO @ 20:48:15] 0.003142 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:48:30] Energy consumed for RAM : 0.000588 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:48:30] Energy consumed for all GPUs : 0.003462 kWh. Total GPU Power : 281.43729261299364 W\n",
      "[codecarbon INFO @ 20:48:30] Energy consumed for all CPUs : 0.000689 kWh. Total CPU Power : 55.00231345005747 W\n",
      "[codecarbon INFO @ 20:48:30] 0.004739 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:48:45] Energy consumed for RAM : 0.000784 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:48:45] Energy consumed for all GPUs : 0.004642 kWh. Total GPU Power : 283.2523800450318 W\n",
      "[codecarbon INFO @ 20:48:45] Energy consumed for all CPUs : 0.000918 kWh. Total CPU Power : 55.01494687309937 W\n",
      "[codecarbon INFO @ 20:48:45] 0.006345 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:49:00] Energy consumed for RAM : 0.000980 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:49:00] Energy consumed for all GPUs : 0.005821 kWh. Total GPU Power : 283.2652334835993 W\n",
      "[codecarbon INFO @ 20:49:00] Energy consumed for all CPUs : 0.001147 kWh. Total CPU Power : 55.02738948078316 W\n",
      "[codecarbon INFO @ 20:49:00] 0.007948 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:49:15] Energy consumed for RAM : 0.001176 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:49:15] Energy consumed for all GPUs : 0.006996 kWh. Total GPU Power : 282.03417095571155 W\n",
      "[codecarbon INFO @ 20:49:15] Energy consumed for all CPUs : 0.001376 kWh. Total CPU Power : 55.02632780679815 W\n",
      "[codecarbon INFO @ 20:49:15] 0.009548 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:49:30] Energy consumed for RAM : 0.001372 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:49:30] Energy consumed for all GPUs : 0.008173 kWh. Total GPU Power : 282.82597014560895 W\n",
      "[codecarbon INFO @ 20:49:30] Energy consumed for all CPUs : 0.001606 kWh. Total CPU Power : 55.112575396253234 W\n",
      "[codecarbon INFO @ 20:49:30] 0.011151 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:49:45] Energy consumed for RAM : 0.001568 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:49:45] Energy consumed for all GPUs : 0.009351 kWh. Total GPU Power : 282.60673879867545 W\n",
      "[codecarbon INFO @ 20:49:45] Energy consumed for all CPUs : 0.001835 kWh. Total CPU Power : 55.051969407013246 W\n",
      "[codecarbon INFO @ 20:49:45] 0.012754 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:50:00] Energy consumed for RAM : 0.001764 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:50:00] Energy consumed for all GPUs : 0.010527 kWh. Total GPU Power : 282.5293603017979 W\n",
      "[codecarbon INFO @ 20:50:00] Energy consumed for all CPUs : 0.002065 kWh. Total CPU Power : 55.067219878102776 W\n",
      "[codecarbon INFO @ 20:50:00] 0.014355 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:50:15] Energy consumed for RAM : 0.001960 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:50:15] Energy consumed for all GPUs : 0.011706 kWh. Total GPU Power : 283.2139465136984 W\n",
      "[codecarbon INFO @ 20:50:15] Energy consumed for all CPUs : 0.002294 kWh. Total CPU Power : 55.07738928947124 W\n",
      "[codecarbon INFO @ 20:50:15] 0.015960 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:50:30] Energy consumed for RAM : 0.002156 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:50:30] Energy consumed for all GPUs : 0.012889 kWh. Total GPU Power : 284.0774474547607 W\n",
      "[codecarbon INFO @ 20:50:30] Energy consumed for all CPUs : 0.002524 kWh. Total CPU Power : 55.17713728307889 W\n",
      "[codecarbon INFO @ 20:50:30] 0.017569 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:50:45] Energy consumed for RAM : 0.002352 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:50:45] Energy consumed for all GPUs : 0.014067 kWh. Total GPU Power : 282.92076406569623 W\n",
      "[codecarbon INFO @ 20:50:45] Energy consumed for all CPUs : 0.002754 kWh. Total CPU Power : 55.125020326239216 W\n",
      "[codecarbon INFO @ 20:50:45] 0.019173 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:51:00] Energy consumed for RAM : 0.002548 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:51:00] Energy consumed for all GPUs : 0.015246 kWh. Total GPU Power : 283.0930812140767 W\n",
      "[codecarbon INFO @ 20:51:00] Energy consumed for all CPUs : 0.002984 kWh. Total CPU Power : 55.31542084631833 W\n",
      "[codecarbon INFO @ 20:51:00] 0.020778 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:51:15] Energy consumed for RAM : 0.002744 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:51:15] Energy consumed for all GPUs : 0.016424 kWh. Total GPU Power : 282.80438534231365 W\n",
      "[codecarbon INFO @ 20:51:15] Energy consumed for all CPUs : 0.003214 kWh. Total CPU Power : 55.12263973577228 W\n",
      "[codecarbon INFO @ 20:51:15] 0.022381 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:51:30] Energy consumed for RAM : 0.002939 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:51:30] Energy consumed for all GPUs : 0.017601 kWh. Total GPU Power : 282.7845426983199 W\n",
      "[codecarbon INFO @ 20:51:30] Energy consumed for all CPUs : 0.003443 kWh. Total CPU Power : 55.0644597293339 W\n",
      "[codecarbon INFO @ 20:51:30] 0.023984 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:51:45] Energy consumed for RAM : 0.003135 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:51:45] Energy consumed for all GPUs : 0.018780 kWh. Total GPU Power : 282.92711430250944 W\n",
      "[codecarbon INFO @ 20:51:45] Energy consumed for all CPUs : 0.003672 kWh. Total CPU Power : 54.99574612735293 W\n",
      "[codecarbon INFO @ 20:51:45] 0.025587 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:52:00] Energy consumed for RAM : 0.003331 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:52:00] Energy consumed for all GPUs : 0.019958 kWh. Total GPU Power : 283.0107667753326 W\n",
      "[codecarbon INFO @ 20:52:00] Energy consumed for all CPUs : 0.003901 kWh. Total CPU Power : 54.92466510772592 W\n",
      "[codecarbon INFO @ 20:52:00] 0.027190 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:52:15] Energy consumed for RAM : 0.003527 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:52:15] Energy consumed for all GPUs : 0.021136 kWh. Total GPU Power : 282.86268690871253 W\n",
      "[codecarbon INFO @ 20:52:15] Energy consumed for all CPUs : 0.004130 kWh. Total CPU Power : 55.010137242788716 W\n",
      "[codecarbon INFO @ 20:52:15] 0.028794 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:52:30] Energy consumed for RAM : 0.003723 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:52:30] Energy consumed for all GPUs : 0.022314 kWh. Total GPU Power : 282.90179789395376 W\n",
      "[codecarbon INFO @ 20:52:30] Energy consumed for all CPUs : 0.004359 kWh. Total CPU Power : 54.90940333117424 W\n",
      "[codecarbon INFO @ 20:52:30] 0.030396 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:52:45] Energy consumed for RAM : 0.003919 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:52:45] Energy consumed for all GPUs : 0.023491 kWh. Total GPU Power : 282.5030724571355 W\n",
      "[codecarbon INFO @ 20:52:45] Energy consumed for all CPUs : 0.004588 kWh. Total CPU Power : 54.977462774661426 W\n",
      "[codecarbon INFO @ 20:52:45] 0.031998 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:53:00] Energy consumed for RAM : 0.004115 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:53:00] Energy consumed for all GPUs : 0.024669 kWh. Total GPU Power : 282.91250745419984 W\n",
      "[codecarbon INFO @ 20:53:00] Energy consumed for all CPUs : 0.004818 kWh. Total CPU Power : 55.13232617656255 W\n",
      "[codecarbon INFO @ 20:53:00] 0.033601 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:53:15] Energy consumed for RAM : 0.004311 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:53:15] Energy consumed for all GPUs : 0.025847 kWh. Total GPU Power : 282.8877888274217 W\n",
      "[codecarbon INFO @ 20:53:15] Energy consumed for all CPUs : 0.005048 kWh. Total CPU Power : 55.32986274553078 W\n",
      "[codecarbon INFO @ 20:53:15] 0.035206 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:53:30] Energy consumed for RAM : 0.004507 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:53:30] Energy consumed for all GPUs : 0.027029 kWh. Total GPU Power : 283.84734101847187 W\n",
      "[codecarbon INFO @ 20:53:30] Energy consumed for all CPUs : 0.005278 kWh. Total CPU Power : 55.28087254319742 W\n",
      "[codecarbon INFO @ 20:53:30] 0.036814 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:53:45] Energy consumed for RAM : 0.004703 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:53:45] Energy consumed for all GPUs : 0.028209 kWh. Total GPU Power : 283.6185908447376 W\n",
      "[codecarbon INFO @ 20:53:45] Energy consumed for all CPUs : 0.005508 kWh. Total CPU Power : 55.19745053636513 W\n",
      "[codecarbon INFO @ 20:53:45] 0.038421 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:54:00] Energy consumed for RAM : 0.004899 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:54:00] Energy consumed for all GPUs : 0.029389 kWh. Total GPU Power : 283.25690709112877 W\n",
      "[codecarbon INFO @ 20:54:00] Energy consumed for all CPUs : 0.005738 kWh. Total CPU Power : 55.180032768282274 W\n",
      "[codecarbon INFO @ 20:54:00] 0.040027 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:54:15] Energy consumed for RAM : 0.005095 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:54:15] Energy consumed for all GPUs : 0.030570 kWh. Total GPU Power : 283.45499196562133 W\n",
      "[codecarbon INFO @ 20:54:15] Energy consumed for all CPUs : 0.005968 kWh. Total CPU Power : 55.221798468420246 W\n",
      "[codecarbon INFO @ 20:54:15] 0.041633 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:54:30] Energy consumed for RAM : 0.005291 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:54:30] Energy consumed for all GPUs : 0.031748 kWh. Total GPU Power : 282.8721129134807 W\n",
      "[codecarbon INFO @ 20:54:30] Energy consumed for all CPUs : 0.006199 kWh. Total CPU Power : 55.33682219434278 W\n",
      "[codecarbon INFO @ 20:54:30] 0.043237 kWh of electricity used since the beginning.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 987\n",
      "  Batch size = 2\n",
      "[codecarbon INFO @ 20:54:45] Energy consumed for RAM : 0.005487 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:54:45] Energy consumed for all GPUs : 0.032933 kWh. Total GPU Power : 284.64258292816396 W\n",
      "[codecarbon INFO @ 20:54:45] Energy consumed for all CPUs : 0.006429 kWh. Total CPU Power : 55.30990646663781 W\n",
      "[codecarbon INFO @ 20:54:45] 0.044849 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:00] Energy consumed for RAM : 0.005683 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:55:00] Energy consumed for all GPUs : 0.034123 kWh. Total GPU Power : 285.6919046552254 W\n",
      "[codecarbon INFO @ 20:55:00] Energy consumed for all CPUs : 0.006662 kWh. Total CPU Power : 56.02461612143881 W\n",
      "[codecarbon INFO @ 20:55:00] 0.046468 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:15] Energy consumed for RAM : 0.005879 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:55:15] Energy consumed for all GPUs : 0.035311 kWh. Total GPU Power : 285.39471149003407 W\n",
      "[codecarbon INFO @ 20:55:15] Energy consumed for all CPUs : 0.006896 kWh. Total CPU Power : 56.07343344972983 W\n",
      "[codecarbon INFO @ 20:55:15] 0.048086 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:30] Energy consumed for RAM : 0.006075 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:55:30] Energy consumed for all GPUs : 0.036499 kWh. Total GPU Power : 285.3347956640939 W\n",
      "[codecarbon INFO @ 20:55:30] Energy consumed for all CPUs : 0.007130 kWh. Total CPU Power : 56.09559304591696 W\n",
      "[codecarbon INFO @ 20:55:30] 0.049704 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:45] Energy consumed for RAM : 0.006271 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:55:45] Energy consumed for all GPUs : 0.037688 kWh. Total GPU Power : 285.5247020489034 W\n",
      "[codecarbon INFO @ 20:55:45] Energy consumed for all CPUs : 0.007363 kWh. Total CPU Power : 56.09663902355894 W\n",
      "[codecarbon INFO @ 20:55:45] 0.051323 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:56:00] Energy consumed for RAM : 0.006467 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:56:00] Energy consumed for all GPUs : 0.038877 kWh. Total GPU Power : 285.3569998582853 W\n",
      "[codecarbon INFO @ 20:56:00] Energy consumed for all CPUs : 0.007597 kWh. Total CPU Power : 56.057264319362815 W\n",
      "[codecarbon INFO @ 20:56:00] 0.052940 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/phi-3-mini-4k-instruct/checkpoint-375\n",
      "[codecarbon INFO @ 20:56:15] Energy consumed for RAM : 0.006663 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:56:15] Energy consumed for all GPUs : 0.040031 kWh. Total GPU Power : 277.1533759186536 W\n",
      "[codecarbon INFO @ 20:56:15] Energy consumed for all CPUs : 0.007823 kWh. Total CPU Power : 54.34099666445498 W\n",
      "[codecarbon INFO @ 20:56:15] 0.054517 kWh of electricity used since the beginning.\n",
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/phi-3-mini-4k-instruct/checkpoint-375/tokenizer_config.json\n",
      "Special tokens file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/phi-3-mini-4k-instruct/checkpoint-375/special_tokens_map.json\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "[codecarbon INFO @ 20:56:30] Energy consumed for RAM : 0.006859 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:56:30] Energy consumed for all GPUs : 0.041204 kWh. Total GPU Power : 281.5993058141355 W\n",
      "[codecarbon INFO @ 20:56:30] Energy consumed for all CPUs : 0.008052 kWh. Total CPU Power : 54.882311106726625 W\n",
      "[codecarbon INFO @ 20:56:30] 0.056114 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:56:45] Energy consumed for RAM : 0.007055 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:56:45] Energy consumed for all GPUs : 0.042387 kWh. Total GPU Power : 284.1411040500219 W\n",
      "[codecarbon INFO @ 20:56:45] Energy consumed for all CPUs : 0.008282 kWh. Total CPU Power : 55.11997244795173 W\n",
      "[codecarbon INFO @ 20:56:45] 0.057723 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:00] Energy consumed for RAM : 0.007251 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:57:00] Energy consumed for all GPUs : 0.043569 kWh. Total GPU Power : 283.8676281169781 W\n",
      "[codecarbon INFO @ 20:57:00] Energy consumed for all CPUs : 0.008511 kWh. Total CPU Power : 55.12016910454666 W\n",
      "[codecarbon INFO @ 20:57:00] 0.059331 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:15] Energy consumed for RAM : 0.007447 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:57:15] Energy consumed for all GPUs : 0.044753 kWh. Total GPU Power : 284.431521003285 W\n",
      "[codecarbon INFO @ 20:57:15] Energy consumed for all CPUs : 0.008741 kWh. Total CPU Power : 55.082252652515876 W\n",
      "[codecarbon INFO @ 20:57:15] 0.060941 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:30] Energy consumed for RAM : 0.007643 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:57:30] Energy consumed for all GPUs : 0.045935 kWh. Total GPU Power : 283.66805884290164 W\n",
      "[codecarbon INFO @ 20:57:30] Energy consumed for all CPUs : 0.008970 kWh. Total CPU Power : 55.04253243655837 W\n",
      "[codecarbon INFO @ 20:57:30] 0.062547 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:45] Energy consumed for RAM : 0.007839 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:57:45] Energy consumed for all GPUs : 0.047117 kWh. Total GPU Power : 283.8138710226904 W\n",
      "[codecarbon INFO @ 20:57:45] Energy consumed for all CPUs : 0.009199 kWh. Total CPU Power : 55.09231481342344 W\n",
      "[codecarbon INFO @ 20:57:45] 0.064155 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:58:00] Energy consumed for RAM : 0.008035 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:58:00] Energy consumed for all GPUs : 0.048302 kWh. Total GPU Power : 284.68867771118755 W\n",
      "[codecarbon INFO @ 20:58:00] Energy consumed for all CPUs : 0.009430 kWh. Total CPU Power : 55.37413871112487 W\n",
      "[codecarbon INFO @ 20:58:00] 0.065767 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:58:15] Energy consumed for RAM : 0.008231 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:58:15] Energy consumed for all GPUs : 0.049485 kWh. Total GPU Power : 284.09057414134134 W\n",
      "[codecarbon INFO @ 20:58:15] Energy consumed for all CPUs : 0.009660 kWh. Total CPU Power : 55.30672741910136 W\n",
      "[codecarbon INFO @ 20:58:15] 0.067376 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:58:30] Energy consumed for RAM : 0.008427 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:58:30] Energy consumed for all GPUs : 0.050668 kWh. Total GPU Power : 284.1266560073514 W\n",
      "[codecarbon INFO @ 20:58:30] Energy consumed for all CPUs : 0.009890 kWh. Total CPU Power : 55.1365827536339 W\n",
      "[codecarbon INFO @ 20:58:30] 0.068985 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:58:45] Energy consumed for RAM : 0.008622 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:58:45] Energy consumed for all GPUs : 0.051849 kWh. Total GPU Power : 283.4516921088312 W\n",
      "[codecarbon INFO @ 20:58:45] Energy consumed for all CPUs : 0.010120 kWh. Total CPU Power : 55.106641297235555 W\n",
      "[codecarbon INFO @ 20:58:45] 0.070591 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:59:00] Energy consumed for RAM : 0.008818 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:59:00] Energy consumed for all GPUs : 0.053030 kWh. Total GPU Power : 283.57153899185863 W\n",
      "[codecarbon INFO @ 20:59:00] Energy consumed for all CPUs : 0.010350 kWh. Total CPU Power : 55.22407674531457 W\n",
      "[codecarbon INFO @ 20:59:00] 0.072198 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:59:15] Energy consumed for RAM : 0.009014 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:59:15] Energy consumed for all GPUs : 0.054214 kWh. Total GPU Power : 284.4201490397488 W\n",
      "[codecarbon INFO @ 20:59:15] Energy consumed for all CPUs : 0.010580 kWh. Total CPU Power : 55.183925010869565 W\n",
      "[codecarbon INFO @ 20:59:15] 0.073808 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:59:30] Energy consumed for RAM : 0.009210 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:59:30] Energy consumed for all GPUs : 0.055396 kWh. Total GPU Power : 283.79619102499146 W\n",
      "[codecarbon INFO @ 20:59:30] Energy consumed for all CPUs : 0.010810 kWh. Total CPU Power : 55.24859459700106 W\n",
      "[codecarbon INFO @ 20:59:30] 0.075416 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:59:45] Energy consumed for RAM : 0.009406 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 20:59:45] Energy consumed for all GPUs : 0.056579 kWh. Total GPU Power : 284.04994258843584 W\n",
      "[codecarbon INFO @ 20:59:45] Energy consumed for all CPUs : 0.011042 kWh. Total CPU Power : 55.6185581854302 W\n",
      "[codecarbon INFO @ 20:59:45] 0.077026 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:00:00] Energy consumed for RAM : 0.009602 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:00:00] Energy consumed for all GPUs : 0.057759 kWh. Total GPU Power : 283.46854011472266 W\n",
      "[codecarbon INFO @ 21:00:00] Energy consumed for all CPUs : 0.011272 kWh. Total CPU Power : 55.3893617048617 W\n",
      "[codecarbon INFO @ 21:00:00] 0.078633 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:00:15] Energy consumed for RAM : 0.009798 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:00:15] Energy consumed for all GPUs : 0.058944 kWh. Total GPU Power : 284.6284172047385 W\n",
      "[codecarbon INFO @ 21:00:15] Energy consumed for all CPUs : 0.011502 kWh. Total CPU Power : 55.267963704541806 W\n",
      "[codecarbon INFO @ 21:00:15] 0.080245 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:00:30] Energy consumed for RAM : 0.009994 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:00:30] Energy consumed for all GPUs : 0.060126 kWh. Total GPU Power : 283.93071044768806 W\n",
      "[codecarbon INFO @ 21:00:30] Energy consumed for all CPUs : 0.011732 kWh. Total CPU Power : 55.213818614042964 W\n",
      "[codecarbon INFO @ 21:00:30] 0.081853 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:00:45] Energy consumed for RAM : 0.010190 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:00:45] Energy consumed for all GPUs : 0.061308 kWh. Total GPU Power : 283.66397510859554 W\n",
      "[codecarbon INFO @ 21:00:45] Energy consumed for all CPUs : 0.011963 kWh. Total CPU Power : 55.225274889367114 W\n",
      "[codecarbon INFO @ 21:00:45] 0.083460 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:01:00] Energy consumed for RAM : 0.010386 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:01:00] Energy consumed for all GPUs : 0.062491 kWh. Total GPU Power : 284.23840495235737 W\n",
      "[codecarbon INFO @ 21:01:00] Energy consumed for all CPUs : 0.012192 kWh. Total CPU Power : 55.13643221208609 W\n",
      "[codecarbon INFO @ 21:01:00] 0.085070 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:01:15] Energy consumed for RAM : 0.010582 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:01:15] Energy consumed for all GPUs : 0.063677 kWh. Total GPU Power : 284.6223591707264 W\n",
      "[codecarbon INFO @ 21:01:15] Energy consumed for all CPUs : 0.012422 kWh. Total CPU Power : 55.214671882664454 W\n",
      "[codecarbon INFO @ 21:01:15] 0.086681 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:01:30] Energy consumed for RAM : 0.010778 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:01:30] Energy consumed for all GPUs : 0.064860 kWh. Total GPU Power : 284.0734209156424 W\n",
      "[codecarbon INFO @ 21:01:30] Energy consumed for all CPUs : 0.012652 kWh. Total CPU Power : 55.15929888820952 W\n",
      "[codecarbon INFO @ 21:01:30] 0.088290 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:01:45] Energy consumed for RAM : 0.010974 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:01:45] Energy consumed for all GPUs : 0.066044 kWh. Total GPU Power : 284.48262970685437 W\n",
      "[codecarbon INFO @ 21:01:45] Energy consumed for all CPUs : 0.012881 kWh. Total CPU Power : 54.91684794220683 W\n",
      "[codecarbon INFO @ 21:01:45] 0.089899 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:02:00] Energy consumed for RAM : 0.011170 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:02:00] Energy consumed for all GPUs : 0.067229 kWh. Total GPU Power : 284.6021046877934 W\n",
      "[codecarbon INFO @ 21:02:00] Energy consumed for all CPUs : 0.013110 kWh. Total CPU Power : 54.91584580229137 W\n",
      "[codecarbon INFO @ 21:02:00] 0.091509 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:02:15] Energy consumed for RAM : 0.011366 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:02:15] Energy consumed for all GPUs : 0.068413 kWh. Total GPU Power : 284.3306115467016 W\n",
      "[codecarbon INFO @ 21:02:15] Energy consumed for all CPUs : 0.013339 kWh. Total CPU Power : 55.03505161955801 W\n",
      "[codecarbon INFO @ 21:02:15] 0.093118 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:02:30] Energy consumed for RAM : 0.011562 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:02:30] Energy consumed for all GPUs : 0.069604 kWh. Total GPU Power : 285.98145017050894 W\n",
      "[codecarbon INFO @ 21:02:30] Energy consumed for all CPUs : 0.013568 kWh. Total CPU Power : 54.95535711749457 W\n",
      "[codecarbon INFO @ 21:02:30] 0.094734 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:02:45] Energy consumed for RAM : 0.011758 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:02:45] Energy consumed for all GPUs : 0.070787 kWh. Total GPU Power : 284.15057173275125 W\n",
      "[codecarbon INFO @ 21:02:45] Energy consumed for all CPUs : 0.013797 kWh. Total CPU Power : 54.93041996701808 W\n",
      "[codecarbon INFO @ 21:02:45] 0.096342 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:03:00] Energy consumed for RAM : 0.011954 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:03:00] Energy consumed for all GPUs : 0.071971 kWh. Total GPU Power : 284.17056660960526 W\n",
      "[codecarbon INFO @ 21:03:00] Energy consumed for all CPUs : 0.014025 kWh. Total CPU Power : 54.948941125354935 W\n",
      "[codecarbon INFO @ 21:03:00] 0.097950 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/phi-3-mini-4k-instruct/checkpoint-750\n",
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/phi-3-mini-4k-instruct/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/phi-3-mini-4k-instruct/checkpoint-750/special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 987\n",
      "  Batch size = 2\n",
      "[codecarbon INFO @ 21:03:15] Energy consumed for RAM : 0.012150 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:03:15] Energy consumed for all GPUs : 0.073107 kWh. Total GPU Power : 272.69964141809544 W\n",
      "[codecarbon INFO @ 21:03:15] Energy consumed for all CPUs : 0.014246 kWh. Total CPU Power : 53.02017857295747 W\n",
      "[codecarbon INFO @ 21:03:15] 0.099503 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:03:30] Energy consumed for RAM : 0.012346 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:03:30] Energy consumed for all GPUs : 0.074296 kWh. Total GPU Power : 285.58682857051457 W\n",
      "[codecarbon INFO @ 21:03:30] Energy consumed for all CPUs : 0.014480 kWh. Total CPU Power : 56.0993580683851 W\n",
      "[codecarbon INFO @ 21:03:30] 0.101122 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:03:45] Energy consumed for RAM : 0.012542 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:03:45] Energy consumed for all GPUs : 0.075487 kWh. Total GPU Power : 286.1843640648702 W\n",
      "[codecarbon INFO @ 21:03:45] Energy consumed for all CPUs : 0.014713 kWh. Total CPU Power : 55.99091241971171 W\n",
      "[codecarbon INFO @ 21:03:45] 0.102742 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:04:00] Energy consumed for RAM : 0.012738 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:04:00] Energy consumed for all GPUs : 0.076679 kWh. Total GPU Power : 286.07108347565935 W\n",
      "[codecarbon INFO @ 21:04:00] Energy consumed for all CPUs : 0.014946 kWh. Total CPU Power : 55.94786337924247 W\n",
      "[codecarbon INFO @ 21:04:00] 0.104363 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:04:15] Energy consumed for RAM : 0.012934 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:04:15] Energy consumed for all GPUs : 0.077868 kWh. Total GPU Power : 285.5423175566573 W\n",
      "[codecarbon INFO @ 21:04:15] Energy consumed for all CPUs : 0.015180 kWh. Total CPU Power : 56.05248485524626 W\n",
      "[codecarbon INFO @ 21:04:15] 0.105981 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:04:30] Energy consumed for RAM : 0.013130 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:04:30] Energy consumed for all GPUs : 0.079058 kWh. Total GPU Power : 285.81026604731585 W\n",
      "[codecarbon INFO @ 21:04:30] Energy consumed for all CPUs : 0.015413 kWh. Total CPU Power : 55.940367870785465 W\n",
      "[codecarbon INFO @ 21:04:30] 0.107600 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:04:45] Energy consumed for RAM : 0.013326 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:04:45] Energy consumed for all GPUs : 0.080248 kWh. Total GPU Power : 285.7889762787291 W\n",
      "[codecarbon INFO @ 21:04:45] Energy consumed for all CPUs : 0.015646 kWh. Total CPU Power : 55.996288794984025 W\n",
      "[codecarbon INFO @ 21:04:45] 0.109220 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/phi-3-mini-4k-instruct/checkpoint-750\n",
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/phi-3-mini-4k-instruct/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/phi-3-mini-4k-instruct/checkpoint-750/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[codecarbon INFO @ 21:04:47] Energy consumed for RAM : 0.013352 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:04:47] Energy consumed for all GPUs : 0.080384 kWh. Total GPU Power : 238.22018847129388 W\n",
      "[codecarbon INFO @ 21:04:47] Energy consumed for all CPUs : 0.015673 kWh. Total CPU Power : 46.456810288296275 W\n",
      "[codecarbon INFO @ 21:04:47] 0.109409 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/phi-3-mini-4k-instruct\n",
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/phi-3-mini-4k-instruct/tokenizer_config.json\n",
      "Special tokens file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/phi-3-mini-4k-instruct/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rouge_score import rouge_scorer\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "print(torch.cuda.current_device())\n",
    "\n",
    "response_template = \"<|assistant|>\"\n",
    "instruction_template = \"<|user|>\"\n",
    "collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template,response_template=response_template, tokenizer=tokenizer)\n",
    "\n",
    "        # Round the values for better readability\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset_chatml,\n",
    "        eval_dataset=val_dataset_chatml,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=512,\n",
    "#         compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        args=args,\n",
    "        data_collator=collator\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# 'trainer.save_model()' is a method that saves the trained model locally.\n",
    "# The model will be saved in the directory specified by 'output_dir' in the training arguments.\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "model_path = \"/home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/phi-3-mini-4k-instruct\"\n",
    "tokenizer_path = \"/home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/phi-3-mini-4k-instruct\"\n",
    "# connect to device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # 'cuda' will point to the visible GPU (e.g., GPU 0 as set by CUDA_VISIBLE_DEVICES)\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=compute_dtype, trust_remote_code=True, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from evaluate import load  # Correct import for metrics\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 987/987 [00:00<00:00, 1999.89 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'text', 'counter_argument', 'messages'],\n",
       "        num_rows: 937\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'text', 'counter_argument', 'messages'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset_chatml=val_data.map(create_message_column_with_labels)\n",
    "val_dataset_chatml=val_dataset_chatml.map(format_dataset_chatml)\n",
    "dataset_chatml = val_dataset_chatml.train_test_split(test_size=0.05)\n",
    "dataset_chatml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=100, do_sample=True, num_beams=1, temperature=0.7, top_k=50, top_p=0.9,repetition_penalty=1.2, max_time= 50)\n",
    "    # print(outputs[0]['generated_text'])\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"While concerns about personal freedoms and choices are valid, it's essential to recognize that public health measures like mandatory COVID-19 vaccinations aim to protect not only individuals but also vulnerable populations who cannot be easily immunized. Legal frameworks often prioritize community safety over individual exemptions in cases of infectious diseases; these laws help prevent outbreaks from escalating into larger crises affecting everyoneâ€”especially those at higher risk due\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:50:30] Energy consumed for RAM : 0.032725 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 21:50:30] Energy consumed for all GPUs : 0.374397 kWh. Total GPU Power : 479.6125926279048 W\n",
      "[codecarbon INFO @ 21:50:30] Energy consumed for all CPUs : 0.043698 kWh. Total CPU Power : 66.01438421456248 W\n",
      "[codecarbon INFO @ 21:50:30] 0.450820 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "test_inference(dataset_chatml['test'][1]['messages'][0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rogue(row):\n",
    "    response = test_inference(row['messages'][0]['content'])\n",
    "    result = metric.compute(predictions=[response], references=[row['output']], use_stemmer=True)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result['response']=response\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1: While it is important to hold individuals accountable, blaming Prime Minister Scott Morrisson solely based on an expedited approval process may not accurately reflect all factors at play. It's crucial to consider other aspects such as regulatory oversight by government agencies like Health Canada in order to make informed judgments regarding liability issues related to COVID-19 immunizations. Additionally, attributing financial responsibility only falls upon those who have experienced direct harm from\n",
      "Reference 1: While concerns about vaccine safety and the profit motives of pharmaceutical companies are valid discussions, itâ€™s important to recognize that vaccines, including AstraZeneca, undergo rigorous testing and regulatory scrutiny before approval. The approval process involves extensive clinical trials to evaluate safety and efficacy, even under accelerated timelines during emergencies. \n",
      "\n",
      "Moreover, the legal liability protections for vaccine manufacturers are designed to ensure rapid access to life-saving vaccines without the deterrent of lawsuits that could stifle innovation. This model has been used successfully in previous public health emergencies, ensuring that companies can focus on developing effective vaccines without the fear of overwhelming legal repercussions. \n",
      "\n",
      "Adverse effects from vaccines are closely monitored by health authorities worldwide, with robust systems in place to address any serious concerns. The benefits of vaccination in controlling infectious diseases and protecting public health far outweigh the risks associated with potential side effects. Itâ€™s crucial to base our decisions on scientific evidence and expert consensus rather than fear, which can lead to unnecessary hesitation and potential harm to community health.\n",
      "---\n",
      "Prediction 2: While correlation does exist between those who have received doses from Johnson & Johnsons (J&Js) getting infections at higher rates, this should be taken with caution. There could still be underlying factors contributing to these statistics such as varying levels of exposure or personal behaviors outside their control affecting immunity responses differently compared to individuals receiving different brands/types of COVID19 VACCINES*. This doesnâ€™t necessarily mean jabs are less effective overall\n",
      "Reference 2: While the tweet suggests that individuals vaccinated with the Janssen (Johnson & Johnson) vaccine are at a higher risk of infection compared to unvaccinated individuals, multiple studies and health organizations emphasize the overall effectiveness of vaccines in reducing severe illness, hospitalization, and death from COVID-19. \n",
      "\n",
      "1. **Clinical Trials and Real-World Data**: Clinical trials demonstrated that the Janssen vaccine is effective at preventing COVID-19, particularly against severe outcomes. Real-world data also supports that vaccinated individuals, including those with the Janssen vaccine, experience significantly lower rates of hospitalization compared to unvaccinated populations.\n",
      "\n",
      "2. **Community Immunity**: Vaccines contribute to herd immunity, which protects those who cannot be vaccinated, such as individuals with certain health conditions. Higher vaccination rates in the community lead to a decrease in overall virus transmission.\n",
      "\n",
      "3. **Breakthrough Infections**: While breakthrough infections can occur, they are generally less severe than infections in unvaccinated individuals. The risk of serious illness is substantially mitigated by vaccination, making vaccines a critical tool in the fight against COVID-19.\n",
      "\n",
      "4. **Data Variability**: Variables in the data cited in the tweetâ€”such as underlying health conditions, demographics, and exposure risksâ€”can impact infection rates, making it essential to assess results in a comprehensive context.\n",
      "\n",
      "In conclusion, vaccines, including the Janssen vaccine, are crucial in controlling the pandemic and providing significant protection against severe disease, despite variations in infection rates.\n",
      "---\n",
      "Prediction 3: No, it is not ethical or responsible to seek out ways around getting necessary medical treatments like COVID-19 vaccines. By doing so you are putting yourself and others at risk of spreading harmful diseases without taking proper precautions. It's important to follow public health guidelines in order to protect everyone from this deadly virus.\n",
      "Reference 3: It's important to emphasize that vaccines, including the COVID-19 vaccine, are crucial for public health and safety. Legal exemptions for vaccines typically apply to specific medical conditions or religious beliefs but are not meant to be exploited as loopholes. Avoiding vaccination not only puts individuals at risk for severe illness but also jeopardizes the health of the community, particularly vulnerable populations who may not be able to get vaccinated themselves. The benefits of vaccinationâ€”such as reducing the spread of the virus, protecting healthcare systems, and preventing long-term health complicationsâ€”far outweigh the notion of seeking to circumvent legal requirements. Instead of looking for loopholes, encouraging vaccination is a more responsible and ethical choice for everyoneâ€™s well-being.\n",
      "---\n",
      "Prediction 4: While it is tragic that this family experienced such devastating consequences, correlation does not equal causation. Just because an event occurred following immunization doesn't necessarily mean the vaccine caused or contributed to these outcomes. It could be coincidental timing rather than direct cause of death due to medical conditions unrelated to COVID-19 infection risks being mitigated through receiving all recommended doses on schedule as per CDC guidelines can help prevent similar situations\n",
      "Reference 4: While it's vital to acknowledge that vaccines, like any medical intervention, can have side effects, it's important to view data from systems like VAERS in context. VAERS is a passive reporting system that collects data on adverse events after vaccination; it does not establish causality. Many of the reports are coincidental and do not imply that the vaccine caused the event. \n",
      "\n",
      "In this case, the tragic death of the child, while it raises concerns, doesnâ€™t necessarily correlate with the vaccine. Children can experience various medical issues unrelated to vaccination. Rigorous clinical trials and ongoing studies have shown that vaccines, including Pfizer's COVID-19 vaccine, have a favorable safety profile for the vast majority of individuals. The benefits of vaccination, including protection against severe illness from COVID-19, far outweigh the risk of rare adverse effects. Ensuring high vaccination rates is crucial to protecting public health and preventing outbreaks.\n",
      "---\n",
      "Prediction 5: While it is true that good hygiene practices such as hand washing are important, there have been instances where individuals who were diligent about cleanliness still contracted infectious illnesses like COVID-19 or influenza due to respiratory droplets from coughing or sneezing spread by others around them. Vaccines provide an additional layer of protection against these viruses beyond just practicing basic sanitation measures alone; this helps protect not only\n",
      "Reference 5: While maintaining good hygiene, such as washing hands, is crucial for preventing the spread of many infections, it is not a substitute for vaccinations. Vaccines are specifically designed to build immunity against serious diseases like smallpox, which can spread rapidly and have devastating effects on public health. Smallpox was eradicated not just through hygiene practices but primarily through widespread vaccination efforts. Relying solely on handwashing ignores the reality that vaccines provide necessary protection against diseases that can lead to outbreaks, severe illness, and even death. By getting vaccinated, individuals contribute to community immunity, protecting those who cannot be vaccinated and ensuring that dangerous diseases remain eradicated.\n",
      "---\n",
      "Prediction 6: While it is tragic that two individuals died following their receipt of the mRNA Covishield jab, correlation does not imply causation. It's important to consider all factors at play before jumping to conclusions about safety or efficacy based on isolated incidents like these deaths alone without thorough investigation by health officials. Additionally, countless people around the world have received this same type of shot safely over time which suggests there may be other underlying causes contributing to such events\n",
      "Reference 6: While it is essential to acknowledge that vaccines, like any medical intervention, can have side effects, it is crucial to examine the broader context. The vast majority of individuals who receive the Moderna COVID vaccine experience mild or no side effects, while the benefits of vaccinationâ€”such as significantly reduced risk of severe illness, hospitalization, and deathâ€”far outweigh the potential risks. The deaths mentioned in the tweet could be attributed to various underlying health conditions, and causality should be carefully assessed. Regulatory agencies continuously monitor vaccine safety, and the evidence overwhelmingly supports that the benefits of vaccination in controlling the COVID-19 pandemic and protecting public health are immense compared to the rare instances of adverse reactions.\n",
      "---\n",
      "Prediction 7: While it is concerning to hear reports questioning the effectiveness and safety of any new medical treatment, including COVID-19 vaccines like those being tested by organizations such as The Oxford Vaccine Group, these early results should not be immediately dismissed outright. Animal studies are often used initially before moving on to human clinical trial phases where more comprehensive data can provide further insight into how well potential treatments work across different populations. It's important to wait until all stages have\n",
      "Reference 7: It's important to approach claims about vaccine efficacy and testing with a critical eye. The statement regarding the Oxford Vaccine Group's animal trials misrepresents the scientific process and the context of vaccine development. Vaccines undergo rigorous testing through multiple phases, including animal trials, which are designed to evaluate safety and efficacy. In the case of COVID-19 vaccines, extensive clinical trials involving tens of thousands of participants demonstrated that they significantly reduce the risk of severe illness, hospitalization, and death. Additionally, the notion that \"all vaxxed monkeys got just as sick\" lacks nuance; the overall immune response and reduction in disease severity among vaccinated subjects can still indicate a successful vaccine. Scientific consensus, based on robust data, supports the safety and effectiveness of vaccines in preventing infectious diseases. Misinterpretations like these can undermine public confidence in vaccines and hinder efforts to control outbreaks.\n",
      "---\n",
      "Prediction 8: While Pfizer may have legal protections, ultimately their priority should still lie on ensuring public safety. It is important not only to consider individual risks but also weigh them against broader societal benefits such as preventing widespread outbreaks like Covid-19 that could result in severe illness or death across many people at once. Prioritizing profit over human lives can lead to disastrous consequences both morally and ethically. Public health measures need careful\n",
      "Reference 8: While it's important to consider potential side effects and adverse reactions associated with vaccines, it's equally vital to weigh these against the considerable risks posed by COVID-19 itself. Vaccines undergo rigorous testing and monitoring, and the benefits of COVID-19 vaccinationâ€”such as reducing severe illness, hospitalization, and deathâ€”far outweigh the relatively rare adverse effects. Additionally, the accountability of pharmaceutical companies like Pfizer is balanced by their role in developing vaccines that have been crucial in controlling a global pandemic. Public health data consistently shows that the risks of serious complications from COVID-19 are significantly higher than the risks of serious vaccine side effects. Thus, the decision to vaccinate should be grounded in a comprehensive understanding of the risks versus benefits rather than an emphasis on potential adverse reactions that are statistically uncommon.\n",
      "---\n",
      "Prediction 9: While AstraZeneca may have been criticized in this particular case, conducting their own research is essential to ensure transparency and accountability within pharmaceutical development processes. It allows them to present accurate information about drug efficacy directly from reliable sources rather than relying solely on external reviews or analyses conducted afterward which could introduce biases into reporting results accurately. Additionally, having company involvement can expedite decision making process as well due diligence\n",
      "Reference 9: While concerns about the integrity of clinical trial data are valid and highlight the need for transparency, it's essential to recognize the overwhelming body of evidence supporting the effectiveness and safety of vaccines, including AstraZeneca's. Regulatory bodies such as the FDA and EMA require rigorous independent review before any vaccine is authorized for use. Numerous peer-reviewed studies have confirmed that vaccines reduce the risk of severe illness, hospitalization, and death from COVID-19. Moreover, real-world data from vaccination campaigns around the globe consistently show a significant reduction in infection rates and transmission. Rather than dismissing the entire vaccine development process, it is more productive to advocate for enhanced oversight and transparency while acknowledging the proven benefits that vaccines provide to public health.\n",
      "---\n",
      "Prediction 10: It's important to note that while some individuals may have experienced negative symptoms from receiving any medication or treatment, this does not mean it will cause harm in every case. Vaccination has been proven safe and effective at preventing serious illnesses like COVID-19 on an individual level as well as protecting public health overall through herd immunity. The benefits of getting vaccinated far outweigh the risks posed by contracting the virus itself without protection\n",
      "Reference 10: While it's important to acknowledge that some individuals may experience adverse reactions to vaccines, including severe allergic reactions, these cases are rare in the context of the general population. Vaccines like Moderna are thoroughly tested for safety and efficacy before being authorized for public use. The benefits of vaccination, including the prevention of severe illness, hospitalization, and death from COVID-19, significantly outweigh the risks for most people.\n",
      "\n",
      "Healthcare providers are trained to manage allergic reactions and typically monitor patients after vaccination, particularly those with known allergies, to ensure immediate care is available if needed. Furthermore, the guidance to administer vaccines to individuals with epi-pens reflects an understanding that the majority of individuals with allergies can still safely be vaccinated, allowing for broader community immunity while protecting those at high risk. Ultimately, the decision to vaccinate is supported by extensive scientific evidence that highlights the overall safety and effectiveness of vaccines in combating infectious diseases.\n",
      "---\n",
      "Prediction 11: While it is true that no method, including vaccination, offers complete protection against all potential threats such as viruses or diseases. However, getting fully immunized significantly reduces one's risk of contracting severe illnesses from those pathogens compared to not being at least partially protected by any means available in their arsenal (such like mask wearing). Therefore dismissive attitudes towards public health measures only serve detrimentally impact on efforts toward containing outbreak\n",
      "Reference 11: While it's true that no vaccine offers 100% protection, numerous studies demonstrate that vaccines significantly reduce the risk of severe illness, hospitalization, and death from diseases like COVID-19. For example, data from the CDC shows that vaccinated individuals are far less likely to experience severe outcomes compared to unvaccinated individuals. Furthermore, vaccinated individuals who do contract the virus typically have milder symptoms and lower viral loads, which can lessen their potential to spread the virus. This community protectionâ€”often referred to as herd immunityâ€”plays a crucial role in limiting the overall transmission of disease. Therefore, getting vaccinated not only protects the individual but also contributes to public health by reducing the circulating virus in the community.\n",
      "---\n",
      "Prediction 12: While it is important to consider individual rights and freedoms, implementing measures such as Vaccine Passports can help protect public health during times like COVID-19 pandemic by ensuring that those who are fully immunized have greater accessibility within society while also reducing transmission rates among vulnerable populations. It could be seen not only as an inconvenience but potentially lifesaving measure if done responsibly with proper guidelines put into place. Additionally, having proof of being v\n",
      "Reference 12: While personal choice is an important principle, public health considerations must also be taken into account, especially during a pandemic. Vaccine passports can facilitate safer interactions and help protect vulnerable populations by ensuring higher vaccination rates. By encouraging vaccination, these policies help control the spread of diseases, reducing hospitalizations and protecting those who cannot be vaccinated for medical reasons. Balancing personal freedom with community safety is essential, and vaccine passports can serve as a tool to achieve that balance without infringing on individual rights.\n",
      "---\n",
      "Prediction 13: While there is always some risk involved in taking any medication, including getting sick after being infected with COVID even while immunized. However, by receiving both doses of your shot as soon possible afterwards â€“ ideally within three months prior - this will significantly lower chances at transmitting illness from other sources such as asymptomatic carriers who may be unaware they are spreading disease until too late! Also importantly keeping social distance/mask wearing etc.. helps further reduce\n",
      "Reference 13: While it's true that vaccinated individuals can still contract and transmit viruses, numerous studies show that vaccines significantly reduce the severity of illness and the overall viral load in those who are infected. This means vaccinated people are less likely to experience severe symptoms, require hospitalization, or die from the disease. Additionally, vaccines help to establish herd immunity, reducing the overall spread within the community and protecting those who cannot be vaccinated due to medical reasons. By getting vaccinated, you contribute to a safer environment for everyone, particularly vulnerable populations. The effectiveness of vaccines in decreasing transmission and protecting public health is well-documented, making individual vaccination an important community responsibility.\n",
      "---\n",
      "Prediction 14: While skepticism towards foreign products is understandable, dismissing all potential solutions without considering their merits can be detrimental. It's important to prioritize public health over personal preferences when discussing matters of global importance like disease prevention through immunizations. Additionally, refusing alternative options may hinder progress in combating diseases that could benefit greatly from diverse approaches. Making exceptions based on cultural appreciation should not outweigh scientific evaluation or evidence supporting effect\n",
      "Reference 14: While it's understandable to have concerns about vaccines produced in certain countries, dismissing a vaccine solely based on its country of originâ€”like Russiaâ€”can hinder public health efforts. National biases can cloud our judgment, especially when many reputable scientists and health organizations evaluate vaccines based on safety and efficacy rather than geographic origin. The development of vaccines is a global effort, and collaboration often transcends national boundaries. Instead of allowing biases to dictate our decisions, we should focus on the rigorous testing, peer review, and data that support a vaccine's approval, fostering a more informed and balanced perspective on vaccination.\n",
      "---\n",
      "Prediction 15: While making COVID testing more accessible may help with easing restrictions, dropping important safety protocol like quarantine measures could potentially lead to an increase in transmission rates. It is crucial to balance public accessibility while still prioritizing community protection against infectious diseases during this ongoing pandemic crisis.\n",
      "Reference 15: While the premise of making the vaccine freely available and lifting isolation guidelines seems appealing, it oversimplifies the complex interplay of public health and political factors. The decision to maintain certain guidelines is based on a careful assessment of not just vaccine efficacy, but also the potential for variants to emerge and spread. Lifting restrictions too soon could lead to a resurgence of COVID-19 cases, which could overwhelm healthcare systems and negatively impact the economy in the long run.\n",
      "\n",
      "Additionally, perceptions of vaccine use are heavily influenced by political agendas and misinformation. Trust in vaccines varies greatly depending on cultural and political beliefs, which can lead to vaccine hesitancy even in the face of substantial scientific evidence. Rather than dropping guidelines, a focus on transparent communication and addressing the underlying fears and misinformation may be more effective in ensuring broad vaccination and public compliance with health measures. This is crucial not only for controlling COVID-19 but also for safeguarding overall public health and the economy.\n",
      "---\n",
      "Prediction 16: While it is understandable to be cautious about any new medical treatment, completely dismissing all doses of an approved and effective COVID-19 vaccine could have serious consequences. The benefits of widespread immunization in controlling outbreaks should not overshadow concerns raised by regulatory authorities like SWISCHMEDIC who are responsible for ensuring public safety before approving drugs or treatments. It may still ultimately prove beneficial if given after thorough\n",
      "Reference 16: While concerns about vaccine safety are important and should be considered, it's also crucial to recognize the extensive research and rigorous testing that vaccines, including the Oxford/AstraZeneca vaccine, undergo before approval by regulatory bodies like Swissmedic. Vaccines have played a vital role in controlling infectious diseases and protecting public health. Moreover, the benefits of vaccination â€” such as reducing hospitalizations and preventing severe illness â€” typically far outweigh the risks of potential side effects. Public health decisions should be based on comprehensive evidence rather than isolated incidents or premature conclusions.\n",
      "---\n",
      "Prediction 17: Actually, this is not entirely accurate. While it's true that the COVID-19 vaccines use messenger RNA (MRN) technology to create proteins similar to those found on SARS CoV - II viruses like our own cells do naturally when exposed through everyday life experiences such as breathing airborne particles containing DNA fragments or bacteria derived microbes called \"virosomes\". These viroid molecules are then converted via various enzym\n",
      "Reference 17: While it's true that the Pfizer vaccine employs mRNA technology to trigger an immune response by instructing cells to produce a harmless spike protein, this mechanism has been extensively studied and validated as safe and effective. Numerous peer-reviewed studies have demonstrated that mRNA vaccines significantly reduce the risk of severe illness, hospitalization, and death from COVID-19. \n",
      "\n",
      "Concerns about ingredients in vaccines often stem from misinformation or misunderstanding of how these components function. The ingredients in vaccines, including mRNA and lipid nanoparticles, are rigorously tested for safety and efficacy. Regulatory bodies such as the FDA and WHO continually review vaccine data to ensure public safety. Moreover, the risk of serious side effects from vaccines is substantially lower than the risks associated with contracting COVID-19 itself, which can lead to long-term health complications.\n",
      "\n",
      "In summary, the substantial body of scientific evidence supports vaccine effectiveness and safety, and the benefits of vaccination in protecting individual and public health far outweigh concerns about specific ingredients.\n",
      "---\n",
      "Prediction 18: It is important to fact check information from reputable sources, such as scientific studies published by universities like Stanford. The cold chain requirements of any new technology should not be attributed solely based on speculation surrounding unrelated historical events with no concrete evidence linking them together. Additionally, regulatory agencies thoroughly evaluate safety data prior to approving medications/treatments regardless of their novelty level. Therefore making assumptions regarding mistrust could potentially hinder public health efforts towards controlling\n",
      "Reference 18: While concerns about vaccine ingredients and testing are important to address, the assertion that Pfizer's coronavirus vaccine contains \"experimental nanotech components\" misrepresents both the science and the regulatory processes involved. Vaccines, including the Pfizer-BioNTech vaccine, underwent rigorous testing through multiple phases of clinical trials involving tens of thousands of participants before receiving Emergency Use Authorization. This involved comprehensive safety and efficacy evaluation by independent panels of experts and regulatory authorities like the FDA.\n",
      "\n",
      "The ultra-cold storage requirement for the Pfizer vaccine is primarily due to its mRNA technology, which is a newer approach in vaccine development, not because of the presence of any untested or dangerous materials. mRNA vaccines have been studied for decades and have shown significant promise in combatting various diseases. The idea that these components pose a hidden threat ignores extensive research, peer-reviewed studies, and the transparent processes that govern vaccine development and approval. Misinformation can lead to fear and mistrust, undermining public health efforts and the protection vaccines offer against serious diseases like COVID-19.\n",
      "---\n",
      "Prediction 19: While it's true that COVID-19 may have similarities to other viruses like influenza, such comparisons can be misleading. The severity and long term effects associated with coronavirus make its control through measures beyond just allowing natural spread crucial at this time. Additionally, advancements continue being made on developing longer lasting treatments or potential cures which could significantly change how we approach containing future outbreaks. Lock down restrictions also help prevent overwhelming\n",
      "Reference 19: While it is true that the death rate for COVID-19 can be lower than for some severe flu cases, it is important to recognize that COVID-19 has resulted in significantly higher hospitalization rates and long-term health complications compared to the flu. Vaccines have been proven to reduce the severity of illness, hospitalization, and death. Evidence from multiple studies indicates that vaccinated individuals have lower viral loads, which reduces transmission rates.\n",
      "\n",
      "Furthermore, while immunity may wane over time, boosters can effectively restore protection and are an essential part of managing the virus long-term. Lockdowns are not ideal, but they help control infection rates and prevent healthcare systems from being overwhelmed. The benefits of vaccination in curbing the spread of the virus and protecting vulnerable populations clearly outweigh the risks associated with the disease itself. Promoting vaccinations is a key strategy in achieving collective immunity and returning to a sense of normalcy.\n",
      "---\n",
      "Prediction 20: While there may be legitimate reasons to question certain aspects or regulations surrounding COVID-19 testing, dismissing an entire test as \"dum\" without providing evidence is not productive. It's important to consider scientific research and expert opinions before making such sweeping statements. Additionally, suggesting widespread conspiracy theories only serves to undermine public health efforts in combating this pandemic. Vulgar language like 'f**k', however valid your frustrations\n",
      "Reference 20: While it's understandable to be concerned about the profit motives of pharmaceutical companies, it's important to recognize that vaccines undergo rigorous testing and regulation to ensure their safety and efficacy. The overwhelming consensus among scientists and health professionals is that vaccines save lives, prevent the spread of diseases, and ultimately reduce healthcare costs. Rather than dismissing vaccines as a money grab, we should focus on the extensive research and real-world evidence that demonstrates their critical role in public health. Additionally, the collaborative efforts within the medical community to provide accurate information can help counteract misinformation and foster trust in vaccines.\n",
      "---\n",
      "Prediction 21: While there may have been unfortunate instances of medical treatments going awry, such incidents should not be used to discredit entire fields. Vaccines undergo rigorous research and trials before being administered widely, unlike some other medications which can indeed go \"terrifically\" bad due to lackluster oversight from regulatory bodies. It is important to weigh all potential risks against benefits when considering any treatment option. Ultimately, prom\n",
      "Reference 21: While it's understandable to be cautious about medical treatments, vaccines undergo rigorous testing and regulatory scrutiny before approval. The development process involves multiple phases of clinical trials, including thousands of participants, to assess safety and effectiveness. Regulatory bodies like the FDA and EMA review extensive data to ensure vaccines meet high safety standards. In contrast, many \"remedies\" lack such thorough oversight, leading to potentially dangerous outcomes. Vaccines have a proven track record in reducing the prevalence of severe diseases, which contrasts sharply with unapproved treatments that may not have been subject to any formal testing. Trusting established science and medical guidelines is essential for public health, especially in combating infectious diseases.\n",
      "---\n",
      "Prediction 22: It is important to remember that while some individuals may have been involved in unethical practices, it does not give them immunity or credibility when discussing scientific matters such as COVID-19. Vaccination has proven benefits backed by extensive research and should be taken seriously regardless of past actions from certain companies. Making lighthearted jokes about serious issues like public health can undermine efforts towards prevention and control measures during this pandemic.\n",
      "Reference 22: While it's important to acknowledge concerns about potential side effects and adverse reactions associated with vaccines, it's crucial to differentiate between anecdotal fears and evidence-based science. Vaccines undergo rigorous testing in clinical trials to ensure their safety and efficacy before being approved for public use. The link between vaccines and severe adverse reactions is minimal compared to the risks associated with vaccine-preventable diseases, such as measles or whooping cough, which can lead to serious health complications. Furthermore, regulatory bodies like the CDC and WHO continuously monitor vaccine safety and address any concerns transparently. It's vital to approach this topic with an understanding of the overwhelming scientific consensus that vaccines are a critical public health tool, rather than focusing solely on past controversies unrelated to their current safety and effectiveness.\n",
      "---\n",
      "Prediction 23: While it is unfortunate that this individual had such negative experience, their personal story should not be used as evidence against getting immunized. The vast majority of people do receive life saving protection from diseases without experiencing severe consequences or long term health issues related to receiving medical treatment like surgeries they underwent priorly before being admitted into ICU/hospitalization). It's important to consider scientific data over isolated incidents when making decisions about public safety measures (such\n",
      "Reference 23: While it's understandable to feel concerned about medical treatments, including vaccines, it's important to consider the broader context of vaccine safety. Vaccines undergo rigorous testing in clinical trials to assess their safety and efficacy, and they are continuously monitored for adverse reactions once approved. The potential side effects of vaccines typically occur in a very small percentage of recipients and are often mild, such as soreness at the injection site or fatigue. Comparatively, the consequences of vaccine-preventable diseases can be severe and even life-threatening. Trusting vaccines is about weighing the risks and benefits; the overwhelming consensus in the medical community is that the benefits of vaccination far outweigh the risks of potential side effects. It's crucial to consult with healthcare professionals and rely on scientific evidence when making health decisions.\n",
      "---\n",
      "Prediction 24: While it is tragic that this doctor passed away shortly after receiving their COVID-19 shot, correlation does not equal causation. It's important to consider other factors such as underlying health conditions or unrelated illnesses before jumping to conclusions about safety concerns related to specific medications like mRNA products from Moderna & BioNTech (Pfizer). Further investigation by regulatory agencies will be necessary to determine any true link between these events. #VaxFact\n",
      "Reference 24: While concerns about vaccine safety, particularly regarding potential side effects, are important to address, it's crucial to look at the broader context. The majority of vaccine recipients experience no serious adverse effects, and the benefits of vaccinationâ€”such as preventing severe illness, hospitalizations, and deathsâ€”far outweigh the risks for most populations. Regulatory agencies like the CDC and Pfizer conduct thorough investigations into any reported cases of adverse reactions to ensure public safety. Isolated incidents do not reflect the overall safety profile of vaccines, which are rigorously tested in clinical trials and monitored extensively post-approval. Itâ€™s essential to rely on comprehensive data and expert recommendations rather than anecdotal reports.\n",
      "---\n",
      "Prediction 25: Actually, many pharmaceutical companies around the world have partnered to develop new technologies in order to create effective COVID-19 vaccines. The collaboration between countries such as Europe (EU), Canada's Health Ministry/Government Agency HCAsIMM - Global Research Network Institute / Public Safety Department Government agency PSDAZHKYDVRNJHRPITSQXWMRSTOUKLMPBGRNT\n",
      "Reference 25: The assertion that the Pfizer vaccine is primarily influenced by Chinese funding overlooks the collaborative nature of global vaccine development, which often involves multiple countries contributing resources. It's crucial to recognize that scientific advancement and public health are inherently international efforts, transcending political borders. The vaccine's efficacy and safety are validated by extensive clinical trials and regulatory reviews, regardless of funding sources. \n",
      "\n",
      "Furthermore, viewing vaccine development through a politically charged lens can distort public perception, fueling mistrust and division rather than promoting informed decision-making. In reality, vaccines are a crucial tool in combatting global health crises, and prioritizing public health over political narratives is vital for community well-being. Political factors should not overshadow the tangible benefits of vaccination in preventing disease and saving lives.\n",
      "---\n",
      "Prediction 26: It is important not to jump to conclusions about conspiracy theories without evidence. The development and distribution of COVID-19 vaccines have been closely monitored by health experts around the globe, ensuring their safety and effectiveness in protecting public health. Prioritizing global accessibility rather than solely focusing on domestic needs demonstrates compassion towards other countries experiencing devastating outbreaks as well. Let's focus on facts over unfounded spec\n",
      "Reference 26: It's important to recognize that vaccines, including those developed by Pfizer, undergo rigorous scientific processes and regulatory oversight to ensure their safety and efficacy for all. The timing of information shared with government officials is not evidence of a conspiracy; rather, it's a necessary part of public health planning and response. The suggestion that there's a deliberate agenda to withhold vaccines from Americans undermines the collective effort to achieve herd immunity and protect vulnerable populations. Vaccination is a crucial tool in combating the pandemic and should be based on scientific data rather than unfounded speculation.\n",
      "---\n",
      "Prediction 27: Extending the shelf life means that more people can access this important tool in fighting against COVID, potentially saving lives by increasing availability rather than just relying solely on supply chain management to meet demand during peak times like holiday seasons or hotspots outbreaks where quick distribution is necessary (e.g., nursing homes). This decision was likely made based off scientific research showing stable efficacy even after extended periods without storage at -20Â°C (-4 degrees Fahren\n",
      "Reference 27: While it's important to scrutinize the actions of pharmaceutical companies and their profit motives, the extension of the shelf life for the Johnson & Johnson Covid vaccine is based on scientific data that suggests the vaccine remains effective beyond its initially stated expiration date. Regulatory bodies like the FDA rely on rigorous testing and analysis to ensure vaccine safety and efficacy, even as they navigate the complexities of public health and corporate interests. The process includes thorough evaluations and, in many cases, independent studies to validate findings. Dismissing these decisions solely due to profit concerns overlooks the critical role vaccines play in controlling the pandemic and protecting public health. Rather than rejecting the science outright, it may be more constructive to advocate for transparency and further research into both vaccine efficacy and the motives of pharmaceutical companies.\n",
      "---\n",
      "Prediction 28: While it may seem ironic, having fewer cases can actually be beneficial when developing and distributing new treatments. With limited resources during this pandemic, focusing on reducing transmission rates allows healthcare systems more capacity to handle patients who do become infected without overwhelming hospitals or medical staffs. Additionally, scientists have found alternative methods such as clinical trials using volunteers from diverse demographics rather than relying solely on natural spreading patterns which could\n",
      "Reference 28: While it's important to question vaccine development processes, the assertion that infections are needed for vaccine testing overlooks the rigorous methodologies already employed in vaccine trials. Vaccines are extensively tested in controlled clinical trials, where safety and efficacy are evaluated in a representative population before any public use. These trials include diverse groups and assess immune responses without necessarily relying on active infections. Moreover, the absence of infections doesn't invalidate China's testing; it can utilize prior data and predictive models to evaluate vaccine candidates based on immune response rather than solely on real-world infections. Continuous monitoring and adaptive trial designs further ensure the integrity and safety of vaccines.\n",
      "---\n",
      "Prediction 29: Just because there was one incident where AstraZeneca had issues during their clinical trial does not mean all pharmaceutical companies are unsafe. The benefits outweigh any risks, especially in times like these when we need effective solutions quickly. It's important to trust scientific research and regulatory processes before jumping into conclusions based on isolated incidents. Vulnerable populations should have access to potentially life saving treatments without fear or hesitation due solely from\n",
      "Reference 29: While it's important to acknowledge that vaccines, including AstraZeneca's, can have side effects and that any adverse reactions must be taken seriously, it's equally crucial to understand the context of vaccine trials. The pause in clinical trials is a standard safety precaution designed to ensure participant safety, not an indication of widespread danger. Regulatory agencies thoroughly review data before approving vaccines, and the benefits of vaccinationâ€”such as protection against severe illness and hospitalization from infectious diseasesâ€”generally far outweigh potential risks. Vaccines undergo rigorous testing to ensure they are safe and effective for public use, and monitoring for adverse reactions continues after approval to protect public health. It's vital to base decisions on comprehensive scientific data rather than isolated incidents.\n",
      "---\n",
      "Prediction 30: It is irresponsible to spread baseless conspiracy theories without any evidence. The Pfizer CEO has repeatedly stated in interviews with reputable news outlets, including CNN's Jake Tapper (@jaketrapp), NBC Newsâ€™ Chuck Todd (https://twitter.com/chucktodds) & CBS Nightly Newscast host Scott Pelley(@Scott_SpencerPT). He confirmed there was no delay or manipulation related\n",
      "Reference 30: While it's important to scrutinize the actions of pharmaceutical companies, suggesting that Pfizer deliberately withheld vaccine information for political gain lacks substantive evidence and undermines public trust in vaccines. The approval and distribution of vaccines involve rigorous scientific processes aimed at ensuring safety and efficacy. Concerns about profit motives in Big Pharma are valid, yet they should be addressed through regulation and transparency rather than conspiracy theories. Furthermore, vaccine-related decisions are influenced by public health considerations, not solely political factors. Dismissing scientific consensus and public health initiatives based on assumptions about profit or political motives can hinder efforts to combat the pandemic, ultimately putting more lives at risk.\n",
      "---\n",
      "Prediction 31: While Pfizer's initial timeline may have been overly optimistic, they could still make significant progress and potentially meet earlier deadlines with increased production capabilities. It is important to consider all possibilities before dismissing potential solutions based on one report alone. Additionally, waiting too long in developing effective treatments only prolonged suffering during an already difficult pandemic situation. Trustworthy sources such as government officials should also provide guidance regarding expected readiness of COVID relief measures like these new variants by December instead\n",
      "Reference 31: The claim that Pfizer's vaccine would not be ready until December 2022 overlooks the complexities of vaccine development and approval processes. Regulatory agencies like the FDA rigorously evaluate clinical trial data to ensure safety and efficacy before granting emergency use authorization. Besides, the timeline cited may only address specific trial phases, whereas real-world applications can find ways to expedite deployment. Successive studies and real-world data have shown that the vaccines are effective, which supports their availability to protect public health. Additionally, ongoing surveillance ensures any potential risks are continuously monitored, reinforcing the commitment to safety.\n",
      "---\n",
      "Prediction 32: While it is tragic that this individual passed away, correlation does not equal causation. Just because an event occurred shortly before or around his receipt of COVID testing doesn't mean there was any direct link between them being one another events in time only proves coincidence rather than definitive cause & effect relationship on what happened to him later due solely from receiving Covid test / shot. This should be taken as cautionary information but cannot conclusively prove anything about how these two things relate\n",
      "Reference 32: While it is important to investigate any potential adverse reactions to vaccines, it's crucial to contextualize these occurrences within the larger framework of public health. The vast majority of individuals who receive vaccines do not experience severe side effects, and the benefits of vaccinationâ€”such as protection against serious diseases and community immunityâ€”far outweigh the risks for most people. Isolated incidents, like the unfortunate case mentioned, need thorough investigation but should not be used to broadly dismiss the safety and efficacy of vaccines, which are supported by extensive research and data. The rigorous monitoring systems in place help ensure that any genuine risks are identified and addressed promptly, ultimately protecting public health.\n",
      "---\n",
      "Prediction 33: It is important to consider that advancements in medical research often require testing on animal subjects, as it helps ensure safety before human clinical trial phases. The development of COVID-19 treatments has been essential in combating an unprecedented global pandemic where millions have already lost their lives due to lacking immunity or accessibility to proper healthcare resources worldwide. Prioritizing public wellbeing over ethics should be considered during such crisis situations when developing new therap\n",
      "Reference 33: While it's true that vaccine development, particularly in the case of coronaviruses, can involve animal trials with risks, it's important to contextualize this within the broader framework of vaccine safety and efficacy. Vaccines undergo rigorous testing in both animals and humans to ensure their safety. The vast majority of people experience mild side effects, and severe adverse reactions are exceedingly rare. Moreover, the benefits of vaccinationâ€”such as reducing the spread of infectious diseases and preventing severe illnessâ€”vastly outweigh the potential risks. Dismissing vaccines based solely on previous trials overlooks the comprehensive safety protocols in place and the overwhelming evidence supporting their use in protecting public health.\n",
      "---\n",
      "Prediction 34: While it may be true that some individuals have experienced negative outcomes from receiving certain types of COVID-19 shots, such as those produced by Pfizer or Moderna (which are not Chinese products), this does not negate their overall effectiveness in preventing severe illnesses. It is important to consider all available options when making decisions about public health measures like immunization campaigns based on individual experiences rather than anecdotal evidence alone. Additionally, Brazil could benefit from\n",
      "Reference 34: While concerns about vaccine side effects are valid and should be discussed, it's important to contextualize these risks. Extensive clinical trials and ongoing monitoring demonstrate that the benefits of vaccination, such as preventing severe illness, hospitalization, and death from COVID-19, outweigh the potential adverse reactions. Most side effects are mild and temporary, such as soreness at the injection site or mild fever. Furthermore, regulatory agencies like the CDC and WHO continuously evaluate vaccine safety and efficacy. Claiming that a specific vaccine is harmful without robust evidence can undermine public health efforts and perpetuate misinformation, potentially leading to increased disease transmission and fatalities. It's crucial to rely on scientific data and expert consensus rather than anecdotal reports.\n",
      "---\n",
      "Prediction 35: It is important to remember that correlation does not equal causation, just because an individual had negative health outcomes shortly after getting their COVID-19 shot doesn't mean it was directly caused by the injection itself. There are many factors at play in determining someoneâ€™s overall wellbeing beyond simply taking preventative measures like being immunized against infectious diseases. Further investigation would be necessary before drawing any conclusions about this tragic event as potentially related solely to the\n",
      "Reference 35: While it's important to acknowledge concerns about vaccine side effects, the overwhelming evidence supports the safety and efficacy of vaccines, including the Moderna COVID-19 vaccine. The vast majority of vaccine recipients experience only mild and temporary side effects, such as soreness at the injection site or fatigue. Serious adverse events are exceedingly rare, and they are thoroughly investigated by health authorities. Furthermore, correlation does not imply causation; many factors could contribute to a person's unexpected death that are unrelated to the vaccine. The benefits of vaccination in preventing severe illness and death from COVID-19 far outweigh the potential risks, and vaccines remain a vital tool in managing public health.\n",
      "---\n",
      "Prediction 36: While it may be true that some low Covid fatality rate nations have been perceived as being overly cautious, there could also simply be differences in population size or demographics. Additionally, different regions might prioritize certain preventative measures based on their unique cultural values and public health strategies rather solecing justifiable fear stemming directly off reported casualty figures alone. It's important to consider all factors when evaluating attitudes towards COVID safety protocols across various\n",
      "Reference 36: While it's valid to consider potential side effects of vaccines like AstraZeneca, it's crucial to put these risks in context. The adverse reactions reported are generally rare when compared to the severe consequences of COVID-19, including long-term health complications or death. Countries with successful zero COVID policies have effectively minimized those risks, making their cautious approach to vaccinations understandable. However, in nations facing high mortality rates, the overwhelming evidence suggests that the benefits of vaccinationâ€”reducing severe illness and transmission far outweigh the risks of side effects. Prioritizing public health must involve viewing the broader picture, where unvaccinated populations remain at high risk of death and illness from COVID-19.\n",
      "---\n",
      "Prediction 37: It's important to consider all available data when evaluating the safety and efficiency of COVID-19 vaccines. The reported decrease in efficacy may be due to new strains or mutations, but it doesn't mean we should dismiss their overall benefits outright. Additionally, correlation does not necessarily equal causation - just because some fully vaccinated individuals have developed variations following potential virus transmission isn't definitive proof they caused those changes themselves. Dismissing these\n",
      "Reference 37: While it's true that vaccine efficacy may vary, the overwhelming evidence from health organizations, including the CDC and WHO, supports that vaccines significantly reduce the risk of severe illness, hospitalization, and death from COVID-19. Studies consistently show that vaccinated individuals are less likely to experience severe outcomes compared to those who are unvaccinated. Additionally, vaccines help limit the spread of the virus by reducing the viral load in vaccinated individuals, thereby contributing to community immunity. While breakthrough infections can occur, the benefits of vaccination in controlling the pandemic and protecting vulnerable populations far outweigh the risks. It's essential to rely on comprehensive data and expert consensus rather than isolated statistics.\n",
      "---\n",
      "Prediction 38: While it is understandable to have concerns about new medical treatments, getting properly tested and researched before being released as safe options can help ensure public health. Vaccines go through rigorous trials and evaluations by experts in order to protect individuals from diseases like COVID-19. It's important to trust scientific processes when making decisions regarding our own wellbeing.\n",
      "Reference 38: While concerns about vaccine development speed are understandable, itâ€™s important to note that the rapid development of COVID-19 vaccines was made possible by unprecedented collaboration, funding, and pre-existing research. The rigorous clinical trial process was not bypassed; instead, multiple phases were conducted concurrently, and the data was continually reviewed by independent experts. Regulatory agencies like the FDA and EMA conducted thorough evaluations to ensure safety and efficacy before granting emergency use authorization. Vaccination has proven to be a critical tool in controlling the pandemic and protecting public health, with millions of doses administered safely worldwide.\n",
      "---\n",
      "Prediction 39: While it may be true that some individuals, including political figures like Kennedy Sr., express opposition to certain organizations or leaders associated with their work in public health initiatives such as COVID19 response efforts. It's important not to generalize this sentiment across all members within these groups - there will always exist differing opinions on various issues regardless of any one individual's stance at present moment timeframe/context should also consider potential benefits from advancements made by those involved before jumping too\n",
      "Reference 39: While concerns about profit motives in the pharmaceutical industry are valid, it's important to recognize that vaccines, including mRNA vaccines, undergo rigorous testing for safety and efficacy before receiving approval. The collaboration between public health organizations and pharmaceutical companies is often aimed at addressing public health crises, like the COVID-19 pandemic. Furthermore, attributing the efficacy of vaccines solely to profit motives oversimplifies a complex issue; the advancements in vaccine technology have been driven by years of research and collaboration among scientists worldwide, not just the interests of a few individuals or organizations. Trust in vaccines should be based on scientific evidence and public health outcomes, rather than conspiracy narratives.\n",
      "---\n",
      "Prediction 40: While it is true that some of Pfizer's research involved using cells from aborted fetuses, this does not negate their contributions towards developing life-saving medications such as COVID-19 vaccines which have saved countless lives worldwide. It should be acknowledged that scientific advancements often require ethically complex methods but ultimately serve greater good by improving public health outcomes globally. Additionally, any criticism directed at pharmaceutical companies must take into\n",
      "Reference 40: While concerns about the origins of certain cell lines used in vaccine development are valid and deserve discussion, it's essential to contextualize this within the broader framework of vaccine safety and public health. The MRC-5 and WI-38 cell lines were developed decades ago and have been extensively studied for their safety. Vaccines undergo rigorous testing for safety and efficacy before approval and are monitored continuously for adverse effects post-licensure. While side effects can occurâ€”as with any medical interventionâ€”the benefits of vaccination in preventing serious illnesses, hospitalizations, and deaths far outweigh the potential risks. Vaccines have been shown to save millions of lives, and the ethical considerations regarding cell lines should not overshadow the substantial public health benefits they provide. Open dialogue about these concerns is crucial, but itâ€™s also vital to rely on scientific evidence and the consensus of the medical community when assessing vaccine safety.\n",
      "---\n",
      "Prediction 41: It is important to acknowledge that there are many brave individuals involved in clinical trials, not just those at risk from diseases such as cancer or HIV (which was common before they were allowed). These people play an essential role in advancing medical science by helping develop new treatments and preventative measures against various illnesses. The panic surrounding any death related to testing should be taken seriously rather than being dismissively labeled insensitive language towards researchers trying their best every\n",
      "Reference 41: While it's important to acknowledge the existence of potential side effects and adverse reactions associated with vaccines, we must also consider the overwhelming evidence supporting their safety and efficacy. The tragic death of a vaccine trial volunteer does not negate the millions of lives saved by vaccinations. The benefits of widespread vaccination, including reduced hospitalizations and deaths from COVID-19, far outweigh the risks of adverse events. Furthermore, modern vaccines undergo rigorous testing in clinical trials, and ongoing monitoring ensures transparency and safety. Dismissing the efforts of those who volunteered for trials undermines the collective progress made in combating the pandemic and protecting public health. Instead of framing this tragedy negatively, we should focus on the importance of vaccination in saving lives and returning to normalcy.\n",
      "---\n",
      "Prediction 42: While approving volunteer participation is important, making sure that everyone has access to effective COVID prevention measures should also include encouraging widespread use of available boosters like Johnson & Johnsons. It may take time before we see significant declines due to variants or new strains emerging but prioritizing public health safety now can help minimize future risks down the line. Making all efforts necessary at once could ultimately benefit society as whole rather than wait until case numbers are\n",
      "Reference 42: While personal choice is a fundamental value, the debate over vaccination policies often centers on the broader implications for public health. The voluntary nature of the Pfizer vaccine in Singapore reflects respect for individual autonomy, but itâ€™s essential to consider the collective responsibility that comes with personal health decisions. Low case numbers can change rapidly, and vaccines play a crucial role in preventing outbreaks, protecting not only the vaccinated but also vulnerable populations who may not be able to receive vaccines. Mandates can help ensure higher vaccination rates, thus contributing to herd immunity and safeguarding public health, especially in the face of emerging variants. A balance between individual choice and community well-being is critical in navigating this complex issue.\n",
      "---\n",
      "Prediction 43: While it may seem disappointing to hear about only having half efficiency from one particular brand, no single factor should determine whether or not you choose to get immunized against COVID - multiple studies have shown various brands offer significant protection with different levels of efficacies ranging between around percentages like these numbers show). Additionally getting any level of protective benefit can help slow down transmission rates in your community which ultimately benefits everyone's health as well! Itâ€™s important we donâ€™t dismiss\n",
      "Reference 43: While the tweet expresses skepticism about the efficacy of the Sinovac vaccine, it's important to consider the broader context and evidence supporting the use of vaccines. Firstly, efficacy rates can vary based on factors such as population demographics, the prevalence of variants, and the timing of assessments. A reported efficacy of 50% does not equate to ineffectiveness; it can still significantly reduce severe cases and hospitalizations.\n",
      "\n",
      "Moreover, vaccines, including Sinovac, have gone through rigorous testing and regulatory review to ensure safety and effectiveness. Public health agencies worldwide, including the WHO and various national health organizations, endorse multiple vaccines based on robust clinical trial data demonstrating their role in curbing disease spread and preventing severe outcomes.\n",
      "\n",
      "Vaccination not only protects individuals but also contributes to community immunity, which is crucial for protecting vulnerable populations who cannot be vaccinated. In the face of COVID-19, the benefits of vaccinationâ€”reduced transmission, lower hospitalization rates, and overall population healthâ€”greatly outweigh concerns about specific efficacy numbers in isolation. Thus, while scrutiny of vaccine efficacy is valid, dismissing it entirely ignores the broader evidence supporting their critical role in public health.\n",
      "---\n",
      "Prediction 44: While it may be true that some healthcare professionals jumped on board with recommending new treatments before thoroughly analyzing all available information, this does not mean they should automatically dismiss potential solutions to urgent public health crises. It's important to consider individual motivations behind their actions rather than generalize them as reckless behavior in order to create constructive dialogues around improving medical practices moving forward.\n",
      "Reference 44: While it's important to scrutinize medical practices and advocate for informed consent, it's crucial to recognize that the development and approval of vaccines, including the COVID-19 vaccines, were conducted under rigorous scientific protocols. These vaccines underwent extensive testing in clinical trials involving tens of thousands of participants to assess their safety and efficacy. Regulatory agencies, such as the FDA and EMA, evaluated the data thoroughly before granting emergency use authorization during the pandemic. The scientific community's support for vaccination is based on evidence derived from real-world studies and ongoing monitoring of vaccine effects. Disregarding this collaborative effort and framing it as \"virtue signaling\" undermines the vast and credible research that prioritizes public health and safety.\n",
      "---\n",
      "Prediction 45: Actually, getting people informed about potential risks and side effects of any medical intervention is crucial in making an educated decision. Encouraging individuals not to be skeptical or question information provided by healthcare professionals can lead to dangerous consequences if they don't fully understand their options before choosing whether or not to receive immunizations like COVID shots. Trust should never come at the expense of thorough research and understanding all aspects involved with taking certain medications/treatments\n",
      "Reference 45: While skepticism about vaccines is understandable, it's important to rely on scientific evidence rather than conspiracy theories. Vaccines, including the COVID-19 vaccine, have undergone rigorous testing and monitoring to ensure their safety and effectiveness. The overwhelming consensus among health experts and organizations, such as the CDC and WHO, is that vaccination is a crucial tool in controlling the pandemic and protecting public health. Misinformation can lead to harm, not just for individuals, but for community health as a whole. Itâ€™s essential to have open conversations based on facts and data rather than unfounded fears.\n",
      "---\n",
      "Prediction 46: While it is true that masks, social distancing measures may need to be reinstated at times due to variants or other factors affecting COVID transmission rates, getting fully immunized against Covid can greatly reduce your chances of contracting severe illness from future strains. The Pfizer shot has been proven effective by numerous studies showing decreased hospitalizations among those who receive both doses within recommended timeframes. It's important to consider long term protection rather than short sighted\n",
      "Reference 46: While concerns regarding vaccine mandates and their effectiveness in controlling COVID-19 are valid, it's essential to consider the substantial evidence supporting the effectiveness of vaccines in reducing the severity of illness, hospitalizations, and deaths. Vaccines, including Pfizer, have demonstrated efficacy in creating immunity, which significantly lowers the risk of severe outcomes even if breakthrough infections occur.\n",
      "\n",
      "Regarding the example of the nurse, it's important to note that no vaccine offers 100% immunity; however, vaccinated individuals generally experience milder symptoms and quicker recoveries. This collective immunity not only protects those vaccinated but also helps shield vulnerable populations who cannot get vaccinated.\n",
      "\n",
      "The debate over personal choice versus mandates is complex. However, vaccination is not just a personal choice; it impacts community health. Mandates can help achieve higher vaccination rates necessary for herd immunity, which is crucial for controlling outbreaks and preventing overwhelming healthcare systems.\n",
      "\n",
      "In essence, while personal choice in health matters is important, public health considerations, particularly in the context of a pandemic, justify the implementation of vaccine mandates to ensure broader societal protection and recovery.\n",
      "---\n",
      "Prediction 47: While it may be frustrating to not have access to certain medications, such as birth control pills and hormonal contraceptives like Yaz/Yasmin being pulled from shelves due to safety concerns. It is important to prioritize public health over individual convenience in this case. Vaccinations are crucial in preventing disease outbreaks and protecting vulnerable populations who cannot receive them themselves. #publichealthfirst#VaxForAll##\n",
      "Reference 47: Vaccines play a crucial role in public health by protecting individuals and communities from serious diseases. The overwhelming consensus among health experts is that vaccines are safe and effective. They have been proven to reduce the incidence of life-threatening illnesses, prevent hospitalizations, and save lives. By choosing vaccination, you not only protect yourself and your family but also contribute to herd immunity, which helps safeguard those who cannot be vaccinated due to medical reasons. It's important to rely on scientific evidence and expert guidance when making healthcare decisions.\n",
      "---\n",
      "Prediction 48: It is important to consider all sources before jumping to conclusions or spreading misinformation, especially when it comes to matters as serious as public health during a pandemic crisis like COVID-19. Blaming conspiracy theories without evidence only serves to further divide communities instead of finding common ground on solutions. Trustworthy medical professionals should be consulted rather than relying solely on political figures' endorsements. Additionally, promoting alternative treatments over proven vaccinations\n",
      "Reference 48: While it's valid to scrutinize politicians and their associations with individuals promoting questionable medical claims, it's equally important to examine the profit motives of pharmaceutical companies. The vaccine industry is not immune to financial incentives, as large pharmaceutical corporations stand to gain significantly from widespread vaccination. This raises concerns about whether profit motives could influence vaccine promotion and public trust. Instead of dismissing dissenting voices about vaccine efficacy outright, a more balanced discourse should consider the potential conflicts of interest on all sides, encouraging accountability and transparency in both government and pharmaceutical practices.\n",
      "---\n",
      "Prediction 49: While it is tragic that some individuals experienced negative consequences from receiving their shots, overall this small percentage should not overshadow the immense benefits brought by widespread immunization efforts. The vast majority who were successfully protected could prevent serious illness or deaths due to diseases like meningococcal disease spread throughout sub-Sahara regions where medical care may be limited. Vigorous research continues on improving safety measures during such large scale initiatives while still achieving\n",
      "Reference 49: While it is important to acknowledge that vaccines, like any medical intervention, can have side effects and adverse reactions, the overall benefits of vaccination in preventing disease outbreaks and saving lives typically far outweigh these risks. The case mentioned involves a small fraction of vaccinated individuals experiencing serious reactions, but extensive research shows that the incidence of severe side effects from vaccines is extremely low compared to the health consequences of the diseases they prevent. For example, meningitis can lead to severe complications, including paralysis and death, which is far more common than serious vaccine-related adverse effects. Moreover, vaccination campaigns are rigorously monitored for safety, and regulatory agencies continuously assess data to ensure public safety. Therefore, while vigilance regarding vaccine safety is crucial, it's equally important to base our views on comprehensive evidence and the broader public health benefits vaccines provide.\n",
      "---\n",
      "Prediction 50: It is important to remember that correlation does not equal causation. Just because two events occur simultaneously or in close proximity, doesn't mean one caused the other directly without further investigation by medical professionals. In this case, death following receiving either COVID-19 treatment or being fully immunized should be thoroughly investigated before drawing any conclusions about cause and effect. Blaming something like mRNA technology based on anecdotal evidence goes against scientific principles and could potentially harm public\n",
      "Reference 50: While it's important to acknowledge that vaccines can have potential side effects and adverse reactions, the key difference lies in the nature of the diseases they are designed to prevent versus the vaccines themselves. Covid-19 is a severe illness with significant complications, and statistics reflect that many deaths among those infected are directly attributable to the virus. In contrast, reported vaccine adverse effects are rigorously investigated to determine causality. The overwhelming consensus among health experts is that the benefits of vaccinationâ€”reducing severe illness, hospitalizations, and deaths from Covid-19â€”far outweigh the risks associated with potential side effects. Vaccines undergo extensive testing for safety and efficacy, and ongoing monitoring helps ensure that any serious concerns are promptly addressed. Ultimately, vaccinations are a critical tool in managing public health and fighting the pandemic.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "num_samples=50\n",
    "prompts = [pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": dataset_chatml['test'][i]['messages'][0]['content']}], tokenize=False, add_generation_prompt=True)\n",
    "                                              for i in range(num_samples)]\n",
    "outputs = pipe(prompts, batch_size=1, max_new_tokens=100, do_sample=True, num_beams=1, temperature=0.7, top_k=50, top_p=0.9,repetition_penalty=1.2,\n",
    "                   max_time= 180)\n",
    "# print(outputs)\n",
    "preds = []\n",
    "for i in range(len(outputs)):\n",
    "    generated_text = outputs[i][0]['generated_text']\n",
    "    response = generated_text[len(prompts[i]):].split()\n",
    "    # print(response)\n",
    "    if len(response) > 1:\n",
    "        # Extract the counter argument\n",
    "        pred = \" \".join(response)\n",
    "    else:\n",
    "        pred = \"\"  # Handle case with no valid split\n",
    "    preds.append(pred)\n",
    "\n",
    "    # Print prediction and corresponding reference\n",
    "    print(f\"Prediction {i + 1}: {pred}\")\n",
    "    print(f\"Reference {i + 1}: {dataset_chatml['test'][i]['counter_argument']}\")\n",
    "    print(\"---\")  # Separator for clarity\n",
    "references= [dataset_chatml['test'][i]['counter_argument'] for i in range(len(outputs))]\n",
    "metric.add_batch(predictions=preds, references=references)\n",
    "tweets = [dataset_chatml['test'][i]['text'] for i in range(len(outputs))]\n",
    "\n",
    "# # store the tweets predictions and references in a csv file \n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame(list(zip(tweets, preds, references)), columns =['tweets', 'predictions', 'references'])\n",
    "# df.to_csv('/home/sohampoddar/HDD2/utsav/Cross_Experiment_2/phi3_cross_2_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.add_batch(predictions=preds, references=references)\n",
    "result = metric.compute(use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.20k/5.20k [00:00<00:00, 8.30MB/s]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "To be able to use evaluate-metric/bleurt, you need to install the following dependencies['bleurt'] using 'pip install git+https://github.com/google-research/bleurt.git' for instance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bleurt \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbleurt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetric\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m bleurt\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpreds, references\u001b[38;5;241m=\u001b[39mreferences)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/evaluate/loading.py:748\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a [`~evaluate.EvaluationModule`].\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m download_mode \u001b[38;5;241m=\u001b[39m DownloadMode(download_mode \u001b[38;5;129;01mor\u001b[39;00m DownloadMode\u001b[38;5;241m.\u001b[39mREUSE_DATASET_IF_EXISTS)\n\u001b[0;32m--> 748\u001b[0m evaluation_module \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m evaluation_cls \u001b[38;5;241m=\u001b[39m import_main_class(evaluation_module\u001b[38;5;241m.\u001b[39mmodule_path)\n\u001b[1;32m    752\u001b[0m evaluation_instance \u001b[38;5;241m=\u001b[39m evaluation_cls(\n\u001b[1;32m    753\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[1;32m    754\u001b[0m     process_id\u001b[38;5;241m=\u001b[39mprocess_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs,\n\u001b[1;32m    761\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/evaluate/loading.py:680\u001b[0m, in \u001b[0;36mevaluation_module_factory\u001b[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    679\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (\u001b[38;5;167;01mConnectionError\u001b[39;00m, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m)):\n\u001b[0;32m--> 680\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    682\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a module script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    683\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hugging Face Hub either.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    684\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/evaluate/loading.py:645\u001b[0m, in \u001b[0;36mevaluation_module_factory\u001b[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;66;03m# if module_type provided load specific module_type\u001b[39;00m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 645\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHubEvaluationModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevaluate-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodule_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;66;03m# load community evaluation module from hub\u001b[39;00m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/evaluate/loading.py:489\u001b[0m, in \u001b[0;36mHubEvaluationModuleFactory.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    488\u001b[0m imports \u001b[38;5;241m=\u001b[39m get_imports(local_path)\n\u001b[0;32m--> 489\u001b[0m local_imports \u001b[38;5;241m=\u001b[39m \u001b[43m_download_additional_modules\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_hub_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimports\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimports\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# copy the script and the files in an importable directory\u001b[39;00m\n\u001b[1;32m    496\u001b[0m dynamic_modules_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_modules_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_modules_path \u001b[38;5;28;01melse\u001b[39;00m init_dynamic_modules()\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/evaluate/loading.py:265\u001b[0m, in \u001b[0;36m_download_additional_modules\u001b[0;34m(name, base_path, imports, download_config)\u001b[0m\n\u001b[1;32m    263\u001b[0m         needs_to_be_installed\u001b[38;5;241m.\u001b[39madd((library_import_name, library_import_path))\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m needs_to_be_installed:\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo be able to use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, you need to install the following dependencies\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[lib_name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mlib_name,\u001b[38;5;250m \u001b[39mlib_path\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mneeds_to_be_installed]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([lib_path\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mlib_name,\u001b[38;5;250m \u001b[39mlib_path\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mneeds_to_be_installed])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for instance\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m     )\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m local_imports\n",
      "\u001b[0;31mImportError\u001b[0m: To be able to use evaluate-metric/bleurt, you need to install the following dependencies['bleurt'] using 'pip install git+https://github.com/google-research/bleurt.git' for instance'"
     ]
    }
   ],
   "source": [
    "bleurt = load(\"bleurt\", module_type=\"metric\")\n",
    "results = bleurt.compute(predictions=preds, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge 1 Mean:  26.311526939653717\n",
      "Rouge 2 Mean:  5.590772889159957\n",
      "Rouge L Mean:  15.188391905145252\n",
      "Rouge Lsum Mean:  15.18873540075368\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming result contains your ROUGE scores\n",
    "rouge1_mean = np.mean(result['rouge1']) * 100\n",
    "rouge2_mean = np.mean(result['rouge2']) * 100\n",
    "rougeL_mean = np.mean(result['rougeL']) * 100\n",
    "rougeLsum_mean = np.mean(result['rougeLsum']) * 100\n",
    "\n",
    "print(\"Rouge 1 Mean: \", rouge1_mean)\n",
    "print(\"Rouge 2 Mean: \", rouge2_mean)\n",
    "print(\"Rouge L Mean: \", rougeL_mean)\n",
    "print(\"Rouge Lsum Mean: \", rougeLsum_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-score in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (0.3.13)\n",
      "Requirement already satisfied: torch>=1.0.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (2.3.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (2.0.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (4.47.0.dev0)\n",
      "Requirement already satisfied: numpy in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (1.25.1)\n",
      "Requirement already satisfied: requests in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (4.65.0)\n",
      "Requirement already satisfied: matplotlib in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (3.7.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score) (2023.3)\n",
      "Requirement already satisfied: filelock in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (1.12)\n",
      "Requirement already satisfied: networkx in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (0.26.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (0.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (4.41.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests->bert-score) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests->bert-score) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests->bert-score) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests->bert-score) (2023.7.22)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->bert-score) (3.16.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n",
      "\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 01:31:16] Energy consumed for RAM : 0.033507 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 01:31:16] Energy consumed for all GPUs : 0.474690 kWh. Total GPU Power : 601.2475870026354 W\n",
      "[codecarbon INFO @ 01:31:16] Energy consumed for all CPUs : 0.047386 kWh. Total CPU Power : 67.05083490635967 W\n",
      "[codecarbon INFO @ 01:31:16] 0.555583 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/vocab.json\n",
      "loading file merges.txt from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer.json\n",
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/model.safetensors\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 55.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.55 seconds, 91.04 sentences/sec\n",
      "BERTScore Precision: 0.8509835600852966\n",
      "BERTScore Recall: 0.875709056854248\n",
      "BERTScore F1: 0.8631229996681213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "P, R, F1 = score(preds, references, lang=\"en\", verbose=True)\n",
    "\n",
    "# Display BERTScore results\n",
    "print(\"BERTScore Precision:\", P.mean().item())\n",
    "print(\"BERTScore Recall:\", R.mean().item())\n",
    "print(\"BERTScore F1:\", F1.mean().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
