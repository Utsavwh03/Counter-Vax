{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferencing Gemma-2-2bit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "\n",
    "modelName = \"google/gemma-2b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    modelName,\n",
    "    device_map=\"auto\" , # Map all model layers to GPU 1,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer=AutoTokenizer.from_pretrained(modelName)\n",
    "# Example anti-vaccine comment\n",
    "# input_text = \"Generate counter argument for tweet: Another Covid 19 vaccine casualty Illaria Pappa 31 school teacher from #Italy died from thromboembolism 10 days after receiving #AstraZeneca #vaccine\"\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "# input_ids = {key: value.to(device) for key, value in input_ids.items()}\n",
    "\n",
    "# # print the input text\n",
    "# print(input_text)\n",
    "# # Generate the counter-argument\n",
    "# outputs = model.generate(\n",
    "#     **input_ids,\n",
    "#     max_length=150,  # Increase the maximum length of the output\n",
    "#     num_return_sequences=1,  # Generate one output\n",
    "#     temperature=0.8,  # Control randomness (lower values = more deterministic)\n",
    "#     top_p=0.9,  # Use nucleus sampling\n",
    "#     top_k=50,  # Use top-k sampling\n",
    "#     repetition_penalty=1.2,  # Penalize repeated phrases\n",
    "#     do_sample=True  # Enable sampling for diverse outputs\n",
    "# )\n",
    "\n",
    "# # Decode the output\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "train_data= load_dataset('csv', data_files='/home/sohampoddar/HDD2/utsav/Data/train_CA_without_labels.csv',split='train')\n",
    "val_data= load_dataset('csv', data_files='/home/sohampoddar/HDD2/utsav/Data/val_CA_without_labels.csv',split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-2b-it\"\n",
    "model_name = \"google/gemma-2b-it\"\n",
    "\n",
    "tokenizer_id = \"google/gemma-2b-it\"\n",
    "tokenizer=AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.paddeing_side='right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_column_without_labels(row):\n",
    "        # Initialize an empty list to store the messages.\n",
    "    messages = []\n",
    "    \n",
    "    # Create a 'user' message dictionary with 'content' and 'role' keys.\n",
    "#     print(row['text'])\n",
    "    user = {\n",
    "        \"content\": f\"Generate Counter Argument for the anti-vaccine tweet:\\n Tweet: {row['text']}\\n##Output: \",\n",
    "        \"role\": \"user\"\n",
    "    }\n",
    "    \n",
    "    # Append the 'user' message to the 'messages' list.\n",
    "    messages.append(user)\n",
    "    \n",
    "    # Create an 'assistant' message dictionary with 'content' and 'role' keys.\n",
    "    assistant = {\n",
    "        \"content\": f\"{row['counter_argument']}\",\n",
    "        \"role\": \"assistant\"\n",
    "    }\n",
    "    \n",
    "    # Append the 'assistant' message to the 'messages' list.\n",
    "    messages.append(assistant)\n",
    "    \n",
    "    # Return a dictionary with a 'messages' key and the 'messages' list as its value.\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def format_dataset_chatml(row):\n",
    "    return {\"text\": tokenizer.apply_chat_template(row[\"messages\"], add_generation_prompt=False, tokenize=False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include Label Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {}\n",
    "labels_map['religious'] = \"religious beliefs and their influence on views about vaccines\"\n",
    "labels_map['political'] = \"the political factors that affect perceptions of vaccine use\"\n",
    "labels_map['ingredients'] = \"concerns about the ingredients and chemical components in vaccines\"\n",
    "labels_map['unnecessary'] = \"the importance and necessity of getting vaccinated to prevent diseases\"\n",
    "labels_map['conspiracy'] = \"conspiracy theories suggesting hidden motives behind vaccination efforts\"\n",
    "labels_map['mandatory'] = \"the debate over personal choice versus mandates in vaccination policies\"\n",
    "labels_map['ineffective'] = \"evidence and reasons that support the effectiveness of vaccines\"\n",
    "labels_map['side-effect'] = \"potential side effects and adverse reactions associated with vaccines\"\n",
    "labels_map['pharma'] = \"the role of pharmaceutical companies and concerns about profit motives\"\n",
    "labels_map['rushed'] = \"claims that vaccines were approved or developed without sufficient testing\"\n",
    "labels_map['country'] = \"national biases and objections to vaccines produced by specific countries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_column_with_labels(row):\n",
    "        # Initialize an empty list to store the messages.\n",
    "    messages = []\n",
    "    # print(row)\n",
    "    prompt= f\"Generate Counter Argument for the anti-vaccine tweet:\\n Tweet: {row['text']}\\n Talk About \"\n",
    "\n",
    "    lab=row['labels'].split()\n",
    "    if isinstance(lab, list):\n",
    "            mapped_labels = \" and \".join([labels_map.get(l, \"\") for l in lab])\n",
    "    else:\n",
    "            mapped_labels = labels_map.get(lab, \"\")\n",
    "    # print(mapped_labels)\n",
    "    prompt+= mapped_labels\n",
    "    prompt+= \" ##Output:\"\n",
    "    user = {\n",
    "        \"content\": prompt,\n",
    "        \"role\": \"user\"\n",
    "    }\n",
    "    \n",
    "    # Append the 'user' message to the 'messages' list.\n",
    "    messages.append(user)\n",
    "    \n",
    "    # Create an 'assistant' message dictionary with 'content' and 'role' keys.\n",
    "    assistant = {\n",
    "        \"content\": f\"{row['counter_argument']}\",\n",
    "        \"role\": \"assistant\"\n",
    "    }\n",
    "    \n",
    "    # Append the 'assistant' message to the 'messages' list.\n",
    "    messages.append(assistant)\n",
    "    \n",
    "    # Return a dictionary with a 'messages' key and the 'messages' list as its value.\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def format_dataset_chatml(row):\n",
    "    return {\"text\": tokenizer.apply_chat_template(row[\"messages\"], add_generation_prompt=False, tokenize=False)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# train_data= load_dataset('csv', data_files='/home/sohampoddar/HDD2/utsav/Data/train_CA_with_label_desc.csv',split='train')\n",
    "# val_data= load_dataset('csv', data_files='/home/sohampoddar/HDD2/utsav/Data/val_data_with_labels_desc.csv',split='train')\n",
    "\n",
    "# train_data=train_data[:5]\n",
    "dataset_chatml = train_data.map(create_message_column_with_labels)\n",
    "dataset_chatml = dataset_chatml.map(format_dataset_chatml)\n",
    "\n",
    "val_dataset_chatml=val_data.map(create_message_column_with_labels)\n",
    "val_dataset_chatml=val_dataset_chatml.map(format_dataset_chatml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_chatml = train_data.map(create_message_column_without_labels)\n",
    "\n",
    "dataset_chatml = dataset_chatml.map(format_dataset_chatml)\n",
    "\n",
    "val_dataset_chatml=val_data.map(create_message_column_without_labels)\n",
    "val_dataset_chatml=val_dataset_chatml.map(format_dataset_chatml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': '1330212529956671491t',\n",
       " 'text': '<bos><start_of_turn>user\\nGenerate Counter Argument for the anti-vaccine tweet:\\n Tweet: #CORRUPTION by pushing through #Pfizer vaccine which didn â€™ t do a peer review & were able to push to the finish line & why ? Because , they are #BigPhama 2 many ? About the data ! @InovioPharma is transparent & submitted a peer review over 4 months ago & still no answer ? The news\\n##Output:<end_of_turn>\\n<start_of_turn>model\\nWhile it is important to have transparency and peer review in the development of vaccines, it is also crucial to prioritize speed in a situation like a global pandemic. Pfizer may have pushed through their vaccine quickly because the need for a vaccine was urgent. Additionally, Pfizer has a long history of successful vaccine development and regulatory approval. Just because Inovio Pharma submitted a peer review earlier does not necessarily mean their vaccine is a better option. Fast-tracking the Pfizer vaccine could have been a necessary step in quickly providing protection to the public.<end_of_turn>\\n',\n",
       " 'labels': 'pharma rushed',\n",
       " 'counter_argument': 'While it is important to have transparency and peer review in the development of vaccines, it is also crucial to prioritize speed in a situation like a global pandemic. Pfizer may have pushed through their vaccine quickly because the need for a vaccine was urgent. Additionally, Pfizer has a long history of successful vaccine development and regulatory approval. Just because Inovio Pharma submitted a peer review earlier does not necessarily mean their vaccine is a better option. Fast-tracking the Pfizer vaccine could have been a necessary step in quickly providing protection to the public.',\n",
       " 'messages': [{'content': 'Generate Counter Argument for the anti-vaccine tweet:\\n Tweet: #CORRUPTION by pushing through #Pfizer vaccine which didn â€™ t do a peer review & were able to push to the finish line & why ? Because , they are #BigPhama 2 many ? About the data ! @InovioPharma is transparent & submitted a peer review over 4 months ago & still no answer ? The news\\n##Output: ',\n",
       "   'role': 'user'},\n",
       "  {'content': 'While it is important to have transparency and peer review in the development of vaccines, it is also crucial to prioritize speed in a situation like a global pandemic. Pfizer may have pushed through their vaccine quickly because the need for a vaccine was urgent. Additionally, Pfizer has a long history of successful vaccine development and regulatory approval. Just because Inovio Pharma submitted a peer review earlier does not necessarily mean their vaccine is a better option. Fast-tracking the Pfizer vaccine could have been a necessary step in quickly providing protection to the public.',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_chatml[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import peft ,trl\n",
    "\n",
    "# 'load_dataset' is a function from the 'datasets' library by Hugging Face which allows you to load a dataset.\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, TaskType, PeftModel\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "# 'SFTTrainer' is a class from the 'trl' library that provides a trainer for soft fine-tuning.\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flash_attention_2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_bf16_supported():\n",
    "  compute_dtype = torch.bfloat16\n",
    "  attn_implementation = 'flash_attention_2'\n",
    "# If bfloat16 is not supported, 'compute_dtype' is set to 'torch.float16' and 'attn_implementation' is set to 'sdpa'.\n",
    "else:\n",
    "  compute_dtype = torch.float16\n",
    "  attn_implementation = 'sdpa'\n",
    "\n",
    "# # This line of code is used to print the value of 'attn_implementation', which indicates the chosen attention implementation.\n",
    "print(attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer.pad_token_id=tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "# set cuda available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # 'cuda' will point to the visible GPU (e.g., GPU 0 as set by CUDA_VISIBLE_DEVICES)\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "use_4bit = True\n",
    "# 'bnb_4bit_compute_dtype' is the data type that should be used for computations with the 4-bit base model. In this case, it is set to 'bfloat16'.\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "\n",
    "# 'bnb_4bit_quant_type' is the type of quantization that should be used for the 4-bit base model. In this case, it is set to 'nf4'.\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# 'use_double_quant' is a boolean that controls whether nested quantization should be used for the 4-bit base model.\n",
    "use_double_quant = True\n",
    "bnb_config= BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_use_double_quant=True,\n",
    "      bnb_4bit_quant_type=\"nf4\",\n",
    "      bnb_4bit_compute_dtype=torch.bfloat16\n",
    "  )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_id, torch_dtype=compute_dtype, trust_remote_code=True, quantization_config=bnb_config, device_map=\"auto\",\n",
    "#           attn_implementation=attn_implementation\n",
    ")\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    "peft_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=target_modules,\n",
    ")\n",
    "# model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/transformers/training_args.py:1570: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = TrainingArguments(\n",
    "        output_dir=\"/home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/gemma-2b-bit\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        do_eval=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        log_level=\"debug\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=1e-4,\n",
    "        logging_dir=\"./logs\",  # Directory for storing logs\n",
    "        logging_steps=100,     # Log after every 100 steps\n",
    "        logging_strategy=\"steps\",  # Log by steps instead of epochs\n",
    "        fp16=True,\n",
    "        eval_steps=100,\n",
    "        num_train_epochs=2,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        report_to= None,\n",
    "        seed=42,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/transformers/training_args.py:1570: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/transformers/training_args.py:2065: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/transformers/training_args.py:1570: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/transformers/training_args.py:2065: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:397: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:402: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Using auto half precision backend\n",
      "[codecarbon INFO @ 17:04:01] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 17:04:01] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 17:04:01] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 17:04:01] [setup] CPU Tracking...\n",
      "[codecarbon INFO @ 17:04:01] Tracking Intel CPU via RAPL interface\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[codecarbon INFO @ 17:04:03] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 17:04:03]   Platform system: Linux-5.15.0-124-generic-x86_64-with-glibc2.31\n",
      "[codecarbon INFO @ 17:04:03]   Python version: 3.9.17\n",
      "[codecarbon INFO @ 17:04:03]   CodeCarbon version: 2.3.2\n",
      "[codecarbon INFO @ 17:04:03]   Available RAM : 125.517 GB\n",
      "[codecarbon INFO @ 17:04:03]   CPU count: 40\n",
      "[codecarbon INFO @ 17:04:03]   CPU model: Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz\n",
      "[codecarbon INFO @ 17:04:03]   GPU count: 4\n",
      "[codecarbon INFO @ 17:04:03]   GPU model: 2 x NVIDIA RTX A60002 x NVIDIA RTX A5000\n",
      "Currently training with a batch size of: 2\n",
      "***** Running training *****\n",
      "  Num examples = 1,500\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 750\n",
      "  Number of trainable parameters = 4,902,912\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 10:46, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.172500</td>\n",
       "      <td>1.112122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.791300</td>\n",
       "      <td>1.113014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 17:04:22] Energy consumed for RAM : 0.000196 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:04:22] Energy consumed for all GPUs : 0.001106 kWh. Total GPU Power : 265.17841290687613 W\n",
      "[codecarbon INFO @ 17:04:22] Energy consumed for all CPUs : 0.000226 kWh. Total CPU Power : 54.1770810097197 W\n",
      "[codecarbon INFO @ 17:04:22] 0.001528 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:04:37] Energy consumed for RAM : 0.000392 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:04:37] Energy consumed for all GPUs : 0.002255 kWh. Total GPU Power : 275.8743696777042 W\n",
      "[codecarbon INFO @ 17:04:37] Energy consumed for all CPUs : 0.000452 kWh. Total CPU Power : 54.212400346801715 W\n",
      "[codecarbon INFO @ 17:04:37] 0.003099 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:04:52] Energy consumed for RAM : 0.000588 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:04:52] Energy consumed for all GPUs : 0.003403 kWh. Total GPU Power : 275.62699980808105 W\n",
      "[codecarbon INFO @ 17:04:52] Energy consumed for all CPUs : 0.000677 kWh. Total CPU Power : 54.05534385034878 W\n",
      "[codecarbon INFO @ 17:04:52] 0.004668 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:05:07] Energy consumed for RAM : 0.000784 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:05:07] Energy consumed for all GPUs : 0.004562 kWh. Total GPU Power : 278.33150061790025 W\n",
      "[codecarbon INFO @ 17:05:07] Energy consumed for all CPUs : 0.000902 kWh. Total CPU Power : 54.09276122613045 W\n",
      "[codecarbon INFO @ 17:05:07] 0.006248 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:05:22] Energy consumed for RAM : 0.000980 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:05:22] Energy consumed for all GPUs : 0.005719 kWh. Total GPU Power : 277.8236197849741 W\n",
      "[codecarbon INFO @ 17:05:22] Energy consumed for all CPUs : 0.001128 kWh. Total CPU Power : 54.15690839248599 W\n",
      "[codecarbon INFO @ 17:05:22] 0.007827 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:05:37] Energy consumed for RAM : 0.001176 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:05:37] Energy consumed for all GPUs : 0.006879 kWh. Total GPU Power : 278.5653720654923 W\n",
      "[codecarbon INFO @ 17:05:37] Energy consumed for all CPUs : 0.001353 kWh. Total CPU Power : 54.11251995488109 W\n",
      "[codecarbon INFO @ 17:05:37] 0.009408 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:05:52] Energy consumed for RAM : 0.001372 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:05:52] Energy consumed for all GPUs : 0.008042 kWh. Total GPU Power : 279.35736021720857 W\n",
      "[codecarbon INFO @ 17:05:52] Energy consumed for all CPUs : 0.001579 kWh. Total CPU Power : 54.189064338107755 W\n",
      "[codecarbon INFO @ 17:05:52] 0.010993 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:06:07] Energy consumed for RAM : 0.001568 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:06:07] Energy consumed for all GPUs : 0.009206 kWh. Total GPU Power : 279.39625261151724 W\n",
      "[codecarbon INFO @ 17:06:07] Energy consumed for all CPUs : 0.001805 kWh. Total CPU Power : 54.2418907363426 W\n",
      "[codecarbon INFO @ 17:06:07] 0.012579 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:06:22] Energy consumed for RAM : 0.001764 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:06:22] Energy consumed for all GPUs : 0.010368 kWh. Total GPU Power : 279.1019970432193 W\n",
      "[codecarbon INFO @ 17:06:22] Energy consumed for all CPUs : 0.002031 kWh. Total CPU Power : 54.28065323118695 W\n",
      "[codecarbon INFO @ 17:06:22] 0.014163 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:06:37] Energy consumed for RAM : 0.001960 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:06:37] Energy consumed for all GPUs : 0.011533 kWh. Total GPU Power : 279.709747551503 W\n",
      "[codecarbon INFO @ 17:06:37] Energy consumed for all CPUs : 0.002257 kWh. Total CPU Power : 54.30080697004743 W\n",
      "[codecarbon INFO @ 17:06:37] 0.015750 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:06:52] Energy consumed for RAM : 0.002156 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:06:52] Energy consumed for all GPUs : 0.012698 kWh. Total GPU Power : 279.6620147943303 W\n",
      "[codecarbon INFO @ 17:06:52] Energy consumed for all CPUs : 0.002484 kWh. Total CPU Power : 54.377085457080355 W\n",
      "[codecarbon INFO @ 17:06:52] 0.017338 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:07:07] Energy consumed for RAM : 0.002352 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:07:07] Energy consumed for all GPUs : 0.013862 kWh. Total GPU Power : 279.50490669876723 W\n",
      "[codecarbon INFO @ 17:07:07] Energy consumed for all CPUs : 0.002711 kWh. Total CPU Power : 54.47467613964578 W\n",
      "[codecarbon INFO @ 17:07:07] 0.018925 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:07:22] Energy consumed for RAM : 0.002548 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:07:22] Energy consumed for all GPUs : 0.015024 kWh. Total GPU Power : 278.98792807295615 W\n",
      "[codecarbon INFO @ 17:07:22] Energy consumed for all CPUs : 0.002939 kWh. Total CPU Power : 54.71971165435181 W\n",
      "[codecarbon INFO @ 17:07:22] 0.020511 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:07:37] Energy consumed for RAM : 0.002744 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:07:37] Energy consumed for all GPUs : 0.016188 kWh. Total GPU Power : 279.513724687503 W\n",
      "[codecarbon INFO @ 17:07:37] Energy consumed for all CPUs : 0.003166 kWh. Total CPU Power : 54.456277390318874 W\n",
      "[codecarbon INFO @ 17:07:37] 0.022097 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:07:52] Energy consumed for RAM : 0.002940 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:07:52] Energy consumed for all GPUs : 0.017355 kWh. Total GPU Power : 280.2918322311439 W\n",
      "[codecarbon INFO @ 17:07:52] Energy consumed for all CPUs : 0.003393 kWh. Total CPU Power : 54.479363831149705 W\n",
      "[codecarbon INFO @ 17:07:52] 0.023688 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:08:07] Energy consumed for RAM : 0.003136 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:08:07] Energy consumed for all GPUs : 0.018518 kWh. Total GPU Power : 279.20028380810396 W\n",
      "[codecarbon INFO @ 17:08:07] Energy consumed for all CPUs : 0.003620 kWh. Total CPU Power : 54.626141911927704 W\n",
      "[codecarbon INFO @ 17:08:07] 0.025274 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:08:22] Energy consumed for RAM : 0.003332 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:08:22] Energy consumed for all GPUs : 0.019678 kWh. Total GPU Power : 278.6873456115389 W\n",
      "[codecarbon INFO @ 17:08:22] Energy consumed for all CPUs : 0.003847 kWh. Total CPU Power : 54.52755532881077 W\n",
      "[codecarbon INFO @ 17:08:22] 0.026857 kWh of electricity used since the beginning.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 987\n",
      "  Batch size = 2\n",
      "[codecarbon INFO @ 17:08:37] Energy consumed for RAM : 0.003528 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:08:37] Energy consumed for all GPUs : 0.020855 kWh. Total GPU Power : 282.62795817890674 W\n",
      "[codecarbon INFO @ 17:08:37] Energy consumed for all CPUs : 0.004077 kWh. Total CPU Power : 55.13452459606275 W\n",
      "[codecarbon INFO @ 17:08:37] 0.028460 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:08:52] Energy consumed for RAM : 0.003724 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:08:52] Energy consumed for all GPUs : 0.022044 kWh. Total GPU Power : 285.53725534986967 W\n",
      "[codecarbon INFO @ 17:08:52] Energy consumed for all CPUs : 0.004312 kWh. Total CPU Power : 56.39657289499705 W\n",
      "[codecarbon INFO @ 17:08:52] 0.030080 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:09:07] Energy consumed for RAM : 0.003920 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:09:07] Energy consumed for all GPUs : 0.023230 kWh. Total GPU Power : 284.8921832997223 W\n",
      "[codecarbon INFO @ 17:09:07] Energy consumed for all CPUs : 0.004548 kWh. Total CPU Power : 56.56979043249626 W\n",
      "[codecarbon INFO @ 17:09:07] 0.031698 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:09:22] Energy consumed for RAM : 0.004116 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:09:22] Energy consumed for all GPUs : 0.024414 kWh. Total GPU Power : 284.3238402408614 W\n",
      "[codecarbon INFO @ 17:09:22] Energy consumed for all CPUs : 0.004781 kWh. Total CPU Power : 56.02974587786271 W\n",
      "[codecarbon INFO @ 17:09:22] 0.033311 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/gemma-2b-bit/checkpoint-375\n",
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
      "Model config GemmaConfig {\n",
      "  \"architectures\": [\n",
      "    \"GemmaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_activation\": null,\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 16384,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 18,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/gemma-2b-bit/checkpoint-375/tokenizer_config.json\n",
      "Special tokens file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/gemma-2b-bit/checkpoint-375/special_tokens_map.json\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "[codecarbon INFO @ 17:09:37] Energy consumed for RAM : 0.004312 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:09:37] Energy consumed for all GPUs : 0.025482 kWh. Total GPU Power : 256.4130635543247 W\n",
      "[codecarbon INFO @ 17:09:37] Energy consumed for all CPUs : 0.004998 kWh. Total CPU Power : 52.02928480816996 W\n",
      "[codecarbon INFO @ 17:09:37] 0.034792 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:09:52] Energy consumed for RAM : 0.004508 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:09:52] Energy consumed for all GPUs : 0.026647 kWh. Total GPU Power : 279.60391308396873 W\n",
      "[codecarbon INFO @ 17:09:52] Energy consumed for all CPUs : 0.005227 kWh. Total CPU Power : 55.07522279291064 W\n",
      "[codecarbon INFO @ 17:09:52] 0.036382 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:10:07] Energy consumed for RAM : 0.004704 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:10:07] Energy consumed for all GPUs : 0.027816 kWh. Total GPU Power : 280.7333214589794 W\n",
      "[codecarbon INFO @ 17:10:07] Energy consumed for all CPUs : 0.005471 kWh. Total CPU Power : 58.468579834993676 W\n",
      "[codecarbon INFO @ 17:10:07] 0.037990 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:10:22] Energy consumed for RAM : 0.004900 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:10:22] Energy consumed for all GPUs : 0.028982 kWh. Total GPU Power : 280.06152594504505 W\n",
      "[codecarbon INFO @ 17:10:22] Energy consumed for all CPUs : 0.005703 kWh. Total CPU Power : 55.664040830286496 W\n",
      "[codecarbon INFO @ 17:10:22] 0.039584 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:10:37] Energy consumed for RAM : 0.005096 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:10:37] Energy consumed for all GPUs : 0.030143 kWh. Total GPU Power : 278.84599559372685 W\n",
      "[codecarbon INFO @ 17:10:37] Energy consumed for all CPUs : 0.005937 kWh. Total CPU Power : 56.24849481312581 W\n",
      "[codecarbon INFO @ 17:10:37] 0.041176 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:10:52] Energy consumed for RAM : 0.005292 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:10:52] Energy consumed for all GPUs : 0.031444 kWh. Total GPU Power : 312.217655303393 W\n",
      "[codecarbon INFO @ 17:10:52] Energy consumed for all CPUs : 0.006186 kWh. Total CPU Power : 59.68533803867931 W\n",
      "[codecarbon INFO @ 17:10:52] 0.042921 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:11:07] Energy consumed for RAM : 0.005488 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:11:07] Energy consumed for all GPUs : 0.032813 kWh. Total GPU Power : 328.62419693083774 W\n",
      "[codecarbon INFO @ 17:11:07] Energy consumed for all CPUs : 0.006437 kWh. Total CPU Power : 60.2560429900063 W\n",
      "[codecarbon INFO @ 17:11:07] 0.044737 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:11:22] Energy consumed for RAM : 0.005684 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:11:22] Energy consumed for all GPUs : 0.034012 kWh. Total GPU Power : 288.0757171923265 W\n",
      "[codecarbon INFO @ 17:11:22] Energy consumed for all CPUs : 0.006667 kWh. Total CPU Power : 55.27254453112006 W\n",
      "[codecarbon INFO @ 17:11:22] 0.046363 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:11:37] Energy consumed for RAM : 0.005880 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:11:37] Energy consumed for all GPUs : 0.035175 kWh. Total GPU Power : 279.2867765896019 W\n",
      "[codecarbon INFO @ 17:11:37] Energy consumed for all CPUs : 0.006896 kWh. Total CPU Power : 54.83426923257969 W\n",
      "[codecarbon INFO @ 17:11:37] 0.047951 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:11:52] Energy consumed for RAM : 0.006076 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:11:52] Energy consumed for all GPUs : 0.036338 kWh. Total GPU Power : 279.14917426136503 W\n",
      "[codecarbon INFO @ 17:11:52] Energy consumed for all CPUs : 0.007124 kWh. Total CPU Power : 54.73798620141698 W\n",
      "[codecarbon INFO @ 17:11:52] 0.049537 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:12:07] Energy consumed for RAM : 0.006272 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:12:07] Energy consumed for all GPUs : 0.037499 kWh. Total GPU Power : 278.82313402735065 W\n",
      "[codecarbon INFO @ 17:12:07] Energy consumed for all CPUs : 0.007352 kWh. Total CPU Power : 54.835458394903384 W\n",
      "[codecarbon INFO @ 17:12:07] 0.051123 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:12:22] Energy consumed for RAM : 0.006468 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:12:22] Energy consumed for all GPUs : 0.038660 kWh. Total GPU Power : 278.73231097237476 W\n",
      "[codecarbon INFO @ 17:12:22] Energy consumed for all CPUs : 0.007580 kWh. Total CPU Power : 54.70582008383906 W\n",
      "[codecarbon INFO @ 17:12:22] 0.052707 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:12:37] Energy consumed for RAM : 0.006663 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:12:37] Energy consumed for all GPUs : 0.039823 kWh. Total GPU Power : 279.42367193909337 W\n",
      "[codecarbon INFO @ 17:12:37] Energy consumed for all CPUs : 0.007808 kWh. Total CPU Power : 54.63964583432034 W\n",
      "[codecarbon INFO @ 17:12:37] 0.054294 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:12:52] Energy consumed for RAM : 0.006859 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:12:52] Energy consumed for all GPUs : 0.040987 kWh. Total GPU Power : 279.31761971046234 W\n",
      "[codecarbon INFO @ 17:12:52] Energy consumed for all CPUs : 0.008036 kWh. Total CPU Power : 54.742637940767594 W\n",
      "[codecarbon INFO @ 17:12:52] 0.055882 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:13:07] Energy consumed for RAM : 0.007055 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:13:07] Energy consumed for all GPUs : 0.042152 kWh. Total GPU Power : 279.78262235923887 W\n",
      "[codecarbon INFO @ 17:13:07] Energy consumed for all CPUs : 0.008264 kWh. Total CPU Power : 54.785007556723905 W\n",
      "[codecarbon INFO @ 17:13:07] 0.057471 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:13:22] Energy consumed for RAM : 0.007251 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:13:22] Energy consumed for all GPUs : 0.043320 kWh. Total GPU Power : 280.49844681155184 W\n",
      "[codecarbon INFO @ 17:13:22] Energy consumed for all CPUs : 0.008494 kWh. Total CPU Power : 55.12162264806279 W\n",
      "[codecarbon INFO @ 17:13:22] 0.059065 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:13:37] Energy consumed for RAM : 0.007447 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:13:37] Energy consumed for all GPUs : 0.044481 kWh. Total GPU Power : 278.8679955035585 W\n",
      "[codecarbon INFO @ 17:13:37] Energy consumed for all CPUs : 0.008723 kWh. Total CPU Power : 55.102926025274435 W\n",
      "[codecarbon INFO @ 17:13:37] 0.060652 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/gemma-2b-bit/checkpoint-750\n",
      "[codecarbon INFO @ 17:13:52] Energy consumed for RAM : 0.007643 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:13:52] Energy consumed for all GPUs : 0.045635 kWh. Total GPU Power : 277.0307636632143 W\n",
      "[codecarbon INFO @ 17:13:52] Energy consumed for all CPUs : 0.008950 kWh. Total CPU Power : 54.50792438163637 W\n",
      "[codecarbon INFO @ 17:13:52] 0.062228 kWh of electricity used since the beginning.\n",
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
      "Model config GemmaConfig {\n",
      "  \"architectures\": [\n",
      "    \"GemmaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_activation\": null,\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 16384,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 18,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/gemma-2b-bit/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/gemma-2b-bit/checkpoint-750/special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 987\n",
      "  Batch size = 2\n",
      "[codecarbon INFO @ 17:14:07] Energy consumed for RAM : 0.007839 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:14:07] Energy consumed for all GPUs : 0.046743 kWh. Total GPU Power : 266.0039490798979 W\n",
      "[codecarbon INFO @ 17:14:07] Energy consumed for all CPUs : 0.009172 kWh. Total CPU Power : 53.202108762006056 W\n",
      "[codecarbon INFO @ 17:14:07] 0.063754 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:14:22] Energy consumed for RAM : 0.008035 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:14:22] Energy consumed for all GPUs : 0.047923 kWh. Total GPU Power : 283.38574798997126 W\n",
      "[codecarbon INFO @ 17:14:22] Energy consumed for all CPUs : 0.009405 kWh. Total CPU Power : 56.00617722842726 W\n",
      "[codecarbon INFO @ 17:14:22] 0.065363 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:14:37] Energy consumed for RAM : 0.008231 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:14:37] Energy consumed for all GPUs : 0.049105 kWh. Total GPU Power : 283.7751713197717 W\n",
      "[codecarbon INFO @ 17:14:37] Energy consumed for all CPUs : 0.009639 kWh. Total CPU Power : 56.05654339954874 W\n",
      "[codecarbon INFO @ 17:14:37] 0.066975 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:14:52] Energy consumed for RAM : 0.008427 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:14:52] Energy consumed for all GPUs : 0.050286 kWh. Total GPU Power : 283.6727444045134 W\n",
      "[codecarbon INFO @ 17:14:52] Energy consumed for all CPUs : 0.009873 kWh. Total CPU Power : 56.24489150602775 W\n",
      "[codecarbon INFO @ 17:14:52] 0.068587 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/gemma-2b-bit/checkpoint-750\n",
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
      "Model config GemmaConfig {\n",
      "  \"architectures\": [\n",
      "    \"GemmaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_activation\": null,\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 16384,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 18,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/gemma-2b-bit/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/gemma-2b-bit/checkpoint-750/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[codecarbon INFO @ 17:14:55] Energy consumed for RAM : 0.008466 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 17:14:55] Energy consumed for all GPUs : 0.050473 kWh. Total GPU Power : 226.37907762748515 W\n",
      "[codecarbon INFO @ 17:14:55] Energy consumed for all CPUs : 0.009912 kWh. Total CPU Power : 47.443147642624105 W\n",
      "[codecarbon INFO @ 17:14:55] 0.068852 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/gemma-2b-bit\n",
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
      "Model config GemmaConfig {\n",
      "  \"architectures\": [\n",
      "    \"GemmaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_activation\": null,\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 16384,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 18,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/gemma-2b-bit/tokenizer_config.json\n",
      "Special tokens file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/gemma-2b-bit/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rouge_score import rouge_scorer\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "print(torch.cuda.current_device())\n",
    "# import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "# Create a ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# make a data collator for the model\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments\n",
    "response_template = \"<start_of_turn>model\"\n",
    "instruction_template = \"<start_of_turn>user\"\n",
    "collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template,response_template=response_template, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset_chatml,\n",
    "        eval_dataset=val_dataset_chatml,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=512,\n",
    "        tokenizer=tokenizer,\n",
    "        args=args,\n",
    "        data_collator=collator\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# 'trainer.save_model()' is a method that saves the trained model locally.\n",
    "# The model will be saved in the directory specified by 'output_dir' in the training arguments.\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (0.18.7)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (8.1.6)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.15.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (2.19.0)\n",
      "Requirement already satisfied: setproctitle in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (68.0.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (4.11.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
      "Model config GemmaConfig {\n",
      "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
      "  \"architectures\": [\n",
      "    \"GemmaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_activation\": null,\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 16384,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 18,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "CUDA backend validation successful.\n",
      "loading weights file model.safetensors from cache at /home/sohampoddar/HDD2/hfcache/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/model.safetensors.index.json\n",
      "Instantiating GemmaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "CUDA backend validation successful.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.07s/it]\n",
      "All model checkpoint weights were used when initializing GemmaForCausalLM.\n",
      "\n",
      "All the weights of GemmaForCausalLM were initialized from the model checkpoint at google/gemma-2b-it.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GemmaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "model_path = \"/home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/gemma-2b-bit\"\n",
    "tokenizer_path = \"/home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/gemma-2b-bit\"\n",
    "# connect to device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # 'cuda' will point to the visible GPU (e.g., GPU 0 as set by CUDA_VISIBLE_DEVICES)\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=compute_dtype, trust_remote_code=True, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from rouge_score) (1.25.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.6)\n",
      "Requirement already satisfied: joblib in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from nltk->rouge_score) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from nltk->rouge_score) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from nltk->rouge_score) (4.65.0)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from evaluate import load  # Correct import for metrics\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 987/987 [00:00<00:00, 5583.20 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'text', 'labels', 'counter_argument', 'messages'],\n",
       "        num_rows: 937\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ID', 'text', 'labels', 'counter_argument', 'messages'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset_chatml=val_data.map(create_message_column_with_labels)\n",
    "val_dataset_chatml=val_dataset_chatml.map(format_dataset_chatml)\n",
    "dataset_chatml = val_dataset_chatml.train_test_split(test_size=0.05)\n",
    "dataset_chatml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=100, do_sample=True, num_beams=1, temperature=0.7, top_k=50, top_p=0.9,repetition_penalty=1.2, max_time= 50)\n",
    "    # print(outputs[0]['generated_text'])\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': '1330967872718290945t',\n",
       " 'text': '<bos><start_of_turn>user\\nGenerate Counter Argument for the tweet:\\n Tweet: Is there a legal loophole to avoid the covid 19 vaccine ? Asking for a friend .<end_of_turn>\\n<start_of_turn>model\\nWhile there may be some exemptions for medical or religious reasons, it is important to consider the greater good of public health. Avoiding the vaccine could put not only yourself at risk, but also those around you. It is important to follow public health guidelines and do your part to help stop the spread of COVID-19.<end_of_turn>\\n',\n",
       " 'labels': 'none',\n",
       " 'counter_argument': 'While there may be some exemptions for medical or religious reasons, it is important to consider the greater good of public health. Avoiding the vaccine could put not only yourself at risk, but also those around you. It is important to follow public health guidelines and do your part to help stop the spread of COVID-19.',\n",
       " 'messages': [{'content': 'Generate Counter Argument for the tweet:\\n Tweet: Is there a legal loophole to avoid the covid 19 vaccine ? Asking for a friend .',\n",
       "   'role': 'user'},\n",
       "  {'content': 'While there may be some exemptions for medical or religious reasons, it is important to consider the greater good of public health. Avoiding the vaccine could put not only yourself at risk, but also those around you. It is important to follow public health guidelines and do your part to help stop the spread of COVID-19.',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:37:22] Energy consumed for RAM : 0.012537 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:37:22] Energy consumed for all GPUs : 0.195854 kWh. Total GPU Power : 609.8920808754463 W\n",
      "[codecarbon INFO @ 18:37:22] Energy consumed for all CPUs : 0.017640 kWh. Total CPU Power : 60.944878536275255 W\n",
      "[codecarbon INFO @ 18:37:22] 0.226031 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "dataset_chatml['test'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'While it is true that some younger individuals may experience long-term effects from the vaccine, it is important to consider the overall benefits of getting vaccinated. Vaccines have been proven to be safe and effective in preventing the spread of deadly diseases. Additionally, the long-term effects of the vaccine are still being studied and understood, so it is not fair to assume that Pfizer would not have tested for any potential risks. It is also worth noting that the benefits of getting vaccinated, such as protecting oneself and others from serious illness, far outweigh the potential risks.\\n souverainment, it is not fair to assume that Pfizer would not have tested for potential risks in the vaccine. Vaccines go through rigorous testing and approval processes to ensure their safety and efficacy. It is important to trust the expertise of healthcare professionals and public health officials who have thoroughly reviewed and approved the vaccine.\\n\\nWhile it is true that the long-term effects of the vaccine are still being studied, it is important to consider the overall benefits of getting vaccinated. Vaccines have been proven to be safe and effective in preventing the spread of deadly diseases. Additionally, the long-term effects of the vaccine are still being studied and understood, so it is not fair to assume that Pfizer would not have tested for potential risks'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inference(dataset_chatml['test'][1]['messages'][0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rogue(row):\n",
    "    response = test_inference(row['messages'][0]['content'])\n",
    "    result = metric.compute(predictions=[response], references=[row['output']], use_stemmer=True)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result['response']=response\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nGenerate Counter Argument for the tweet:\\n Tweet: Is there a legal loophole to avoid the covid 19 vaccine ? Asking for a friend .<end_of_turn>\\n<start_of_turn>model\\nWhile there may be some exemptions for medical or religious reasons, it is important to consider the greater good of public health. Avoiding the vaccine could put not only yourself at risk, but also those around you. It is important to follow public health guidelines and do your part to help stop the spread of COVID-19.<end_of_turn>\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_chatml['test'][1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1: It's important to remember that the COVID-19 vaccines have been rigorously tested and approved by health authorities. Trusting in the science and data behind the vaccines is crucial in combating the spread of the virus and protecting public health. grÄƒ grÄƒ Actually, vaccines have been proven to be effective in preventing serious illness and death from COVID-19. Trusting in the scientific community and public health officials is essential in achieving herd immunity and ending the pandemic. grÄƒ grÄƒ\n",
      "Reference 1: It's important to remember that vaccines undergo rigorous testing and approval processes before being distributed to the public. Trusting in the science behind the vaccine can help protect not only yourself, but also those around you.\n",
      "---\n",
      "Prediction 2: There is no legal loophole to avoid getting vaccinated against COVID-19. Getting vaccinated is a crucial step in protecting yourself and others from the virus. It is not possible to legally opt out of receiving the vaccine. **Additionally:** Getting vaccinated is not about avoiding legal consequences, but about protecting public health. By choosing not to get vaccinated, you are putting others at risk and putting the community at risk. It is important to consider the greater good when making decisions about vaccination.\n",
      "Reference 2: While there may be some exemptions for medical or religious reasons, it is important to consider the greater good of public health. Avoiding the vaccine could put not only yourself at risk, but also those around you. It is important to follow public health guidelines and do your part to help stop the spread of COVID-19.\n",
      "---\n",
      "Prediction 3: While it is true that the efficacy rate of the COVID-19 vaccine may be lower than initially projected, it is still important to consider the overall benefits of vaccination in controlling the spread of the virus. Additionally, any information released by AstraZeneca, such as trial blueprints, can be viewed as transparency and accountability rather than something that should be dismissed as distrust. It is important to weigh the potential risks of contracting the virus against the potential benefits of being vaccinated. toppen The decision to move forward\n",
      "Reference 3: It is important to remember that the AstraZeneca vaccine still provides significant protection against COVID-19, and any vaccine with even 50% efficacy can make a major impact in reducing the spread of the virus and saving lives. It is also essential to have multiple vaccine options in order to reach as many people as possible and achieve herd immunity.\n",
      "---\n",
      "Prediction 4: It's important to remember that Pfizer is a reputable pharmaceutical company that has been developing vaccines for over 60 years. Their success with other vaccines, such as the COVID-19 vaccine, should not be dismissed based on speculation or unfounded conspiracy theories. Trusting in scientific research and experts is crucial in times of crisis like this. The connection between Pfizer and the Illuminati is baseless and has been debunked by numerous fact-checking organizations. It's important to\n",
      "Reference 4: While Pfizer may have made profits from their products in the past, it is important to focus on the potential benefits of the Covid vaccine they are developing. The efficacy of the vaccine should be the main priority, rather than focusing on conspiracy theories or past actions of the company.\n",
      "---\n",
      "Prediction 5: It is important to consider that individual experiences with vaccines vary greatly and not all vaccines cause severe side effects. It is possible that your eczema could have been caused by a separate underlying condition rather than the vaccine. Additionally, it is common for people to experience side effects from vaccines, but these typically resolve on their own within a few days. While it is true that individual experiences with vaccines can vary, it is crucial to remember that vaccines are designed to protect public health and prevent the spread\n",
      "Reference 5: It is unlikely that the yellow fever vaccine caused the eczema, as eczema is a common skin condition that can be triggered by a variety of factors such as stress, allergies, and genetics. It is important to consult with a healthcare professional to determine the true cause of the eczema.\n",
      "---\n",
      "Prediction 6: It is important to remember that vaccine trials are a complex and rigorous process that takes time to complete. It is not fair to blame Bill Gates for the shortcomings in the vaccine trials, as he is simply trying to save lives by providing access to a potentially life-saving vaccine. It is also worth noting that the FDA and other regulatory bodies have strict guidelines in place to ensure the safety and efficacy of vaccines before they are approved for public use. While it is true that vaccine trials are\n",
      "Reference 6: It is important to consider the source of this information and whether it is based on credible sources. Bill Gates has been a strong advocate for global health and has dedicated a significant amount of resources to vaccination efforts. It is unlikely that he would be apathetic towards widespread vaccine side effects. Further investigation and fact-checking may be necessary before jumping to conclusions.\n",
      "---\n",
      "Prediction 7: While it's true that the vaccine may not completely prevent contracting or spreading the virus, it does significantly reduce the severity of symptoms and likelihood of hospitalization. Additionally, the potential long-term complications of not receiving the vaccine should be considered, as they may be more severe than initially thought. The development and approval of vaccines are done with rigorous safety protocols and extensive testing to ensure their efficacy and minimal risks. It's important to trust the expertise of healthcare professionals and prioritize public health by getting vaccinated.\n",
      "Reference 7: While it is true that the vaccine may not completely stop transmission of the virus, it has been shown to significantly reduce the severity of illness and hospitalization. Additionally, the potential side effects, such as infertility, are extremely rare and the benefits of widespread vaccination far outweigh the risks. It is important to trust in the expertise of medical professionals and scientists who have extensively studied and developed these vaccines.\n",
      "---\n",
      "Prediction 8: Actually, there is overwhelming scientific evidence that shows the COVID-19 vaccines are safe and effective in preventing severe illness and death from the virus. It's important to trust the guidance of health experts and scientists, rather than spreading misinformation. tÃ¦t pÃ¥: Vaccines have been proven to significantly reduce the spread of diseases and save lives. It's crucial to follow the advice of healthcare professionals and public health measures to protect ourselves and our communities. It is not reasonable to dismiss the efficacy\n",
      "Reference 8: Actually, vaccines have historically played a significant role in reducing or eliminating the spread of diseases and improving public health. It is ignorant to dismiss the potential positive impacts of widespread vaccination.\n",
      "---\n",
      "Prediction 9: While it is important to prioritize waiting for the vaccine to develop before lifting the lockdown, it is also crucial to ensure public safety and prevent the spread of the virus. Allowing people to gather in large numbers without proper protection could potentially overwhelm healthcare systems and prolong the pandemic. It is better to err on the side of caution and follow guidelines that are in place to protect both individuals and the community as a whole. **Additionally, waiting for the vaccine to fully develop could result in even more variants emerging\n",
      "Reference 9: While it is frustrating that the situation with COVID-19 seems to be constantly changing, it is important to remember that vaccines are not a cure-all and it takes time for their effectiveness to be fully understood. Additionally, just because there are inconsistencies and challenges with vaccination efforts in one country, such as India, does not mean that individuals in other countries should stop following public health guidelines and precautions. It is crucial to continue to prioritize safety and follow recommendations from experts to prevent further spread of the virus.\n",
      "---\n",
      "Prediction 10: While the vaccine may have a high efficacy rate, it is still important to consider the potential long-term effects of the virus, even if the transmission rate decreases. Additionally, vaccinating the population as a whole can help protect those who are unable to be vaccinated, such as young children or immunocompromised individuals. It is not just about individual risk tolerance, but also about protecting the community as a whole. toppen Actually, the benefits of getting vaccinated far outweigh the risks. By getting vaccinated\n",
      "Reference 10: While the vaccine may have a high efficacy and some decrease in transmission, it is still important to continue following public health guidelines and precautionary measures. Just because someone is vaccinated does not mean they are completely immune or unable to spread the virus. It is important to consider the health and safety of others in addition to personal risk tolerance.\n",
      "---\n",
      "Prediction 11: While it is important to consider the potential risks associated with the vaccine, it is also crucial to recognize the benefits of widespread vaccination in preventing the spread of a deadly virus like Covid-19. The fact that the government has provided a free pass for the vaccine does not negate the importance of individual responsibility in protecting oneself and others. It is still important to follow public health guidelines and prioritize the safety and well-being of the community as a whole. The vaccine has been rigorously tested\n",
      "Reference 11: Counter argument: While there may be risks associated with taking the vaccine, it is important to consider the potential benefits of reducing the spread of Covid-19 and ultimately saving lives. The vaccine has gone through rigorous testing and approval processes to ensure its safety and effectiveness. Additionally, holding companies accountable for any negative side effects could deter them from developing life-saving vaccines in the future.\n",
      "---\n",
      "Prediction 12: While it is important to consider the perspectives of the majority, it is also crucial to acknowledge the potential consequences of not getting vaccinated, such as increased spread of the virus and putting vulnerable populations at risk. It is not solely the responsibility of the government to address public health issues, but rather a collective effort from individuals, businesses, and communities to prioritize public safety. tÃ¦t pÃ¥: The majority should not be solely blamed for the situation, as there are many factors that contribute to the current situation\n",
      "Reference 12: While it is important to understand the reasons why some individuals may be hesitant to get the vaccine, ultimately individual choice plays a role in public health outcomes. Blaming the government for individuals choosing not to get vaccinated overlooks personal responsibility and the impact of misinformation or personal beliefs on vaccination rates.\n",
      "---\n",
      "Prediction 13: It's important to consider the potential risks and benefits of each vaccine type. While mRNA vaccines may not have been tested as extensively, they have still undergone rigorous clinical trials and have been shown to be safe and effective in preventing severe illness from COVID-19. Additionally, the global rollout of vaccines is crucial in controlling the spread of the virus and saving lives. It's important to prioritize public health and safety in making decisions about vaccination. This argument overlooks the fact that mRNA\n",
      "Reference 13: While it is true that the Oxford vaccine uses more traditional technology, mRNA vaccines have actually undergone rigorous testing and have been shown to be safe and effective in clinical trials. It is important to base vaccination decisions on scientific evidence rather than personal fears or preferences. By choosing not to get vaccinated, you are not only putting yourself at risk, but also potentially spreading the virus to others and hindering efforts to control the pandemic.\n",
      "---\n",
      "Prediction 14: While it is important to critically analyze the information we consume, it is crucial to fact-check and verify the sources before sharing misinformation. The COVID-19 pandemic has been declared by public health organizations as a global health emergency and has resulted in widespread lockdowns and measures to prevent the spread of the virus. It is not accurate to claim that the crisis was planned years in advance and is being used to implement harmful policies. It is important to rely on credible sources and experts in the field of medicine and\n",
      "Reference 14: This tweet promotes a dangerous conspiracy theory with no evidence to support it. The idea that the COVID-19 pandemic was intentionally planned to destroy Western economies and implement police states is not only unfounded but also harmful misinformation. It is important to rely on credible sources and scientific evidence when discussing public health crises.\n",
      "---\n",
      "Prediction 15: While it is true that some survivors may not have sufficient immunity to protect themselves from future infections, it is important to recognize that the majority of people who have survived COVID-19 have developed some level of immunity. Vaccines have been shown to be effective in reducing the severity of symptoms and preventing hospitalization and death. It is essential to continue with vaccination efforts in order to achieve widespread immunity and end the pandemic. It is important to consider that while some individuals may have antibodies that do not\n",
      "Reference 15: It is inaccurate and irresponsible to label a potential vaccine as a Democrat conspiracy based on unfounded claims. The development of a vaccine is based on scientific research and testing, not political affiliations. Suggesting otherwise only fuels misinformation and undermines the efforts of healthcare professionals working to combat the virus.\n",
      "---\n",
      "Prediction 16: It's important to remember that allergic reactions to vaccines are extremely rare and the benefits of getting vaccinated far outweigh the risks. The vast majority of people who receive the Pfizer vaccine do not experience severe allergic reactions, and the overall safety profile of the vaccine has been thoroughly studied and approved by regulatory agencies. It's crucial to trust in the science and public health guidelines in order to protect ourselves and our communities from the spread of COVID-19. While it is true that allergic\n",
      "Reference 16: While it is concerning that a health worker in Alaska had a serious allergic reaction to the Pfizer vaccine, it is important to remember that allergic reactions to vaccines are extremely rare. The benefits of the vaccine, which can protect individuals from a potentially deadly virus, far outweigh the risks of a reaction. It is crucial to continue monitoring and investigating any potential safety implications, but this isolated incident should not deter people from getting vaccinated.\n",
      "---\n",
      "Prediction 17: It is important to consider that correlation does not always equal causation. It is possible that the son's death was due to an underlying health condition or other factors, rather than the Pfizer vaccine. Additionally, it is crucial to remember that millions of people have received the Pfizer vaccine without any adverse effects and it is unlikely that the vaccine was the direct cause of his death. While it is tragic that the son passed away after receiving the Pfizer vaccine, it is important to remember that correlation\n",
      "Reference 17: It is tragic that this father lost his son after getting vaccinated, but it is important to remember that the overwhelming majority of people who receive the Pfizer vaccine do not experience severe side effects. The benefits of vaccination in protecting against COVID-19 far outweigh the risks, and incidents like this are extremely rare. It is crucial to trust in the scientific consensus and data supporting the safety and efficacy of these vaccines.\n",
      "---\n",
      "Prediction 18: While it's important to consider the potential long-term health risks of both COVID-19 and vaccines, the overwhelming scientific consensus is that vaccines are safe and effective in preventing severe illness and death from COVID-19. The benefits of vaccination, including reducing transmission and protecting vulnerable populations, far outweigh the risks associated with the virus itself. It's crucial to trust the guidance of healthcare professionals and public health organizations in making informed decisions regarding vaccination. It's understandable to\n",
      "Reference 18: It's important to consider the fact that the long-term effects of COVID-19 are still largely unknown, and the risks of contracting the virus could potentially be more harmful than any potential side effects of the vaccine.\n",
      "---\n",
      "Prediction 19: While it is true that there have been reports of adverse reactions to vaccines, the vast majority of healthy children have not experienced these side effects. It is important to consider the benefits of vaccination in preventing serious illnesses and protecting public health. Additionally, correlation does not always equal causation, and other factors could be at play in the cases of neurological issues and vaccine reactions. It is important to acknowledge that while there may be rare instances of adverse reactions to vaccines, the benefits of widespread vaccination far\n",
      "Reference 19: While there may be cases of adverse reactions to vaccines, the vast majority of people who receive vaccines do not experience any negative effects. The benefits of vaccinations far outweigh the risks, as they help protect individuals and communities from serious and potentially deadly diseases. It is important to consider the overall public health impact of vaccines, rather than focusing solely on isolated incidents of adverse reactions.\n",
      "---\n",
      "Prediction 20: While it's true that some individuals may still be at risk of certain diseases even with the vaccine, the overall effectiveness of vaccines in preventing severe illness and death far outweighs the risks. Additionally, the vast majority of people who receive the vaccine do not experience any serious side effects, making it a crucial tool in controlling the spread of infectious diseases. It's important to focus on the broader public health benefits of vaccination rather than individual risks. Actually, the vaccines have been proven\n",
      "Reference 20: While it is true that everyone is at risk of dying in a car crash or getting cancer, those are risks that individuals have some control over through measures such as wearing a seatbelt or maintaining a healthy lifestyle. The risk of death from a vaccine-preventable disease can be significantly reduced by getting vaccinated, making it a more preventable and controllable risk compared to car accidents or cancer.\n",
      "---\n",
      "Prediction 21: While it is true that some countries in Asia have rejected the SinoVac vaccine, it is important to consider that they are likely experiencing logistical challenges with distribution and administration. Additionally, the COVID Shield vaccine may not be as effective as initially anticipated due to ongoing research and trials, but it is still a valuable tool in combating the pandemic. It is also worth noting that the development and distribution of vaccines are complex processes that involve multiple factors beyond just efficacy rates. The efficacy rate of a vaccine\n",
      "Reference 21: While Sinovac may have lower efficacy rates compared to other vaccines, it is still approved by several countries and provides some level of protection against COVID-19. Additionally, efficacy rates can vary depending on factors like age and underlying health conditions, so it is important to consider the overall benefits and risks of each vaccine option.\n",
      "---\n",
      "Prediction 22: While it is true that getting a vaccine through accelerated technology is important, it is still crucial to prioritize safety and thorough testing. The rigorous regulatory processes in place ensure that any vaccine that is approved for distribution is safe and effective. It is important to trust the expertise of healthcare professionals and scientists who have been working tirelessly to develop a solution to the COVID-19 pandemic. toppen van der Hoef (@topvanderhof) While it is understandable to have concerns about the speed at which\n",
      "Reference 22: While it is true that thorough testing is important for ensuring the safety of a vaccine, the urgency of the current global pandemic necessitates expedited processes. The FDA and other regulatory bodies have established strict protocols for emergency use authorization, allowing for a vaccine to be deemed safe and effective even if it has not undergone years of testing. Additionally, advancements in technology and research have enabled scientists to monitor vaccine safety and efficacy more efficiently than ever before. Ultimately, the risk of not having a vaccine far outweighs the potential risks associated with expediting the approval process.\n",
      "---\n",
      "Prediction 23: While it is true that the Moderna vaccine may not be as effective against the newer variants of the virus, it still provides significant protection against severe illness and death. Additionally, getting vaccinated with the Moderna vaccine can still help reduce the spread of the virus in the community. It is important to consider the overall benefits of vaccination in protecting public health, rather than focusing solely on the specific variants covered by the vaccine. tÃ¦t auf das Originaltext. The fact that the Moderna vaccine still provides significant protection\n",
      "Reference 23: Counter argument: It is unfair to call the Moderna president an \"asshat\" just because a booster shot may be necessary to combat new variants. Vaccines often need to be adjusted to address changing virus strains, and it is responsible for companies to be proactive in researching and providing solutions.\n",
      "---\n",
      "Prediction 24: It is important to remember that vaccines are crucial in protecting public health and preventing the spread of dangerous diseases. Dismissing the importance of vaccination without considering the benefits it provides is not only dangerous but also ignores the efforts of scientists and healthcare professionals who work tirelessly to keep us safe. While it is understandable to have concerns about vaccines, it is important to recognize that they have been scientifically proven to save lives and prevent the spread of diseases. Dismissing vaccines as dangerous and harmful without evidence is\n",
      "Reference 24: While the tweet raises concerns about potential dangers of vaccines, it is important to consider the overwhelming scientific consensus that vaccines are safe and effective at preventing the spread of diseases. Fear-mongering about vaccines only serves to perpetuate misinformation and put public health at risk.\n",
      "---\n",
      "Prediction 25: Just because someone dies after receiving the vaccine does not necessarily mean that the vaccine caused their death. There could be other underlying health conditions or factors at play. It is important to consider all possibilities before jumping to conclusions about the vaccine's safety. tÃ¦t pÃ¥: The vast majority of people who receive the COVID-19 vaccine do not experience severe side effects, and any potential risks are carefully monitored and studied before they are widely distributed. The benefits of the vaccine far outweigh the rare cases\n",
      "Reference 25: Just because two things happen around the same time does not mean they are causally linked. It is important to rely on scientific evidence and data when making assumptions about the relationship between Covid-19 and vaccinations.\n",
      "---\n",
      "Prediction 26: There is no evidence to suggest that Donald Trump is an agent of the Beast. Promoting the vaccine is a legitimate political decision, not a conspiracy theory. Actually, many celebrities and influencers have been vocal about the importance of getting vaccinated to protect public health and stop the spread of COVID-19. It's not necessarily an agent of the Beast, but rather a responsible action to protect others. Ajoutez : It is important to recognize that promoting the vaccine is a necessary\n",
      "Reference 26: It is illogical to equate promoting the vaccine with being an agent of the Beast. Promoting public health and safety should not be demonized or politicized.\n",
      "---\n",
      "Prediction 27: While it is true that some vaccines may contain ingredients from aborted fetal cells, it is important to note that these cells are removed before the vaccines are administered to the public. Additionally, the use of viruses like those found in chimpanzees in the Oxford vaccines is a common practice in vaccine development and has been approved by regulatory agencies worldwide. The goal of vaccination is to protect individuals and communities from deadly diseases, and the ingredients in vaccines are carefully regulated to ensure safety. It is unfair to\n",
      "Reference 27: Actually, vaccines do not contain aborted baby cells. The use of cell cultures in vaccine production is a common and safe practice that has been approved by regulatory bodies. Additionally, vaccines do not change a person's DNA as they work by stimulating the immune system to create antibodies. It is important to fact-check before spreading misinformation about vaccines.\n",
      "---\n",
      "Prediction 28: Actually, the moderna vaccine was developed by Pfizer and BioNTech, not the National Institutes of Health. The fact that it was funded by the NIH does not necessarily mean that it is a NIH vaccine. Additionally, just because a vaccine is developed by one institution does not automatically make it a better or more effective vaccine. It is important to consider all factors such as efficacy, safety, and regulatory approval when evaluating the effectiveness of any vaccine. The fact that the moderna vaccine was developed\n",
      "Reference 28: Actually, Moderna is a biotechnology company that developed the vaccine with funding from the NIH and NIAID. It is a collaborative effort between private industry and government agencies.\n",
      "---\n",
      "Prediction 29: While it is true that there is already a system in place to track vaccine injuries and deaths, it is important to remember that these systems are crucial for identifying any potential risks or complications with vaccines. By shutting down this system, the government is potentially putting public health at risk by failing to identify and address any potential issues with vaccines. It is important to prioritize transparency and accountability in order to protect the safety and well-being of the public. tÃ¦t pÃ¥: It is important to consider the\n",
      "Reference 29: Just because the federal government has shut down a system tracking vaccine injury and death does not necessarily mean they are hiding information or trying to discourage people from getting vaccinated. It could be for a variety of reasons such as updating the system or transferring data to a more efficient platform. It is important to consider all possibilities before jumping to conclusions.\n",
      "---\n",
      "Prediction 30: While it is tragic that this volunteer died from complications related to the vaccine, it is important to note that the COVID vaccine trials are rigorously monitored for safety and efficacy. The fact that the vaccine is being tested and has been deemed safe by health authorities shows that the risks of potential side effects are being carefully assessed. It is essential to continue the vaccine trials in order to save lives and bring an end to the pandemic. This statement downplays the importance of thorough safety measures and clinical trials\n",
      "Reference 30: While it is unfortunate that a volunteer in the COVID vaccine trial has died, it is important to remember that adverse reactions can occur with any medication or vaccine. The fact that testing is continuing shows that thorough safety protocols are in place to monitor and assess any potential risks. It is crucial to continue with vaccine trials in order to ensure the safety and efficacy of a potential vaccine for the greater population.\n",
      "---\n",
      "Prediction 31: While it is true that these companies are indemnified from side effects, it is important to consider that they are also responsible for ensuring the safety and efficacy of their products. By providing a guarantee of compensation for any adverse reactions, they incentivize manufacturers to prioritize the safety and well-being of their products. Additionally, the indemnification process helps to protect against legal costs associated with potential vaccine-related issues. This argument highlights the importance of not only protecting against financial losses but also ensuring\n",
      "Reference 31: While it is important to protect vaccine manufacturers from potential liabilities, it is also crucial to ensure that individuals who may experience adverse effects from the vaccines are properly compensated and taken care of. Holding manufacturers accountable for any side effects can help ensure that they prioritize safety and effectiveness in their product development.\n",
      "---\n",
      "Prediction 32: While it's true that the Pfizer vaccine may not have a proven mucosal response, it still offers significant protection against severe illness and death from COVID-19. Additionally, the vaccine has been shown to be effective in preventing breakthrough infections even if they are mild. Wearing an antimicrobial mask in conjunction with the vaccine can further enhance protection and reduce the risk of transmission to others. Ultimately, any vaccination is better than no vaccination at all. While the Pfizer vaccine may not have a proven\n",
      "Reference 32: While it is true that Pfizer's vaccine may not provide a mucosal response like some other candidates, it is still important to consider the overall effectiveness and safety of the vaccine. Pfizer's vaccine has been rigorously tested and shown to be highly effective in preventing severe illness and reducing transmission of COVID-19. Additionally, wearing a mask and practicing other preventive measures can further reduce the risk of infection, making the Pfizer vaccine a valuable tool in combating the pandemic.\n",
      "---\n",
      "Prediction 33: This is not true. The COVID-19 vaccine uses a variety of different ingredients, including proteins from the virus itself, not sharks or other marine animals. It is important to trust in scientific research and experts who are working to save lives and protect public health. It is important to fact check and verify the accuracy of information before spreading misinformation. The COVID-19 vaccine does not use shark-derived ingredients, and it is crucial to rely on credible sources for accurate information.\n",
      "Reference 33: Using sharks in medical research, including for the creation of vaccines, can help save human lives. It is important to prioritize finding a solution to the global pandemic over concerns about animal welfare.\n",
      "---\n",
      "Prediction 34: While it is important to consider all potential outcomes of getting vaccinated, it is crucial to prioritize public health and safety. Reporting every complication or death, regardless of the cause, may lead to misinformation and confusion in determining the spread of the virus. It is essential to focus on preventing the spread of Covid-19 and protecting vulnerable populations, rather than dwelling on individual cases. This argument is based on the assumption that reporting complications and deaths from the vaccine will impact public perception and understanding of\n",
      "Reference 34: It is important to remember that the vast majority of people who receive the Covid vaccine do not experience severe complications or death. The benefits of the vaccine in preventing illness and stopping the spread of the virus far outweigh the risks. Additionally, medical professionals carefully track any adverse reactions to vaccines to ensure safety. It is crucial to trust in the science behind vaccines and prioritize public health.\n",
      "---\n",
      "Prediction 35: Actually, vaccines have been proven to greatly reduce the severity of illness and prevent hospitalization and death from COVID-19. The benefits of getting vaccinated far outweigh the risks, and spreading misinformation only perpetuates fear and doubt in public health efforts. tÃ¦t pÃ¥: Vaccines help protect not only yourself, but also those around you by reducing the spread of the virus and helping to achieve herd immunity. It is important to trust in the science and the experts who have worked tirelessly to develop a safe and effective\n",
      "Reference 35: This tweet is spreading misinformation and fearmongering. Vaccines have been proven to be effective in preventing the spread and severity of viruses, including COVID-19. It is important to trust in science and listen to credible sources for information on vaccines.\n",
      "---\n",
      "Prediction 36: While it is true that no one can get smallpox in a filthy environment, that doesn't mean that the vaccine is unnecessary for preventing the spread of the disease in the general population. Vaccines help protect not only individuals but also the community as a whole by reducing the overall transmission rate of the virus. Additionally, vaccines can help build herd immunity, which can be crucial in controlling the spread of infectious diseases. tÃ¦t pÃ¥: Vaccines help control the spread of diseases by creating herd immunity and reducing the\n",
      "Reference 36: Actually, the reason we no longer see smallpox in the environment is precisely because of widespread vaccination efforts. Without vaccines, the disease would still be a major threat to public health. It's important to continue vaccinating to prevent the resurgence of deadly diseases like smallpox.\n",
      "---\n",
      "Prediction 37: It is important to remember that correlation does not equal causation. Just because the individuals in the article died with COVID-19 does not necessarily mean their deaths were caused by the virus itself. There could have been other underlying health conditions or factors at play. It is crucial to wait for conclusive evidence before jumping to conclusions about the cause of death. Actually, the AstraZeneca vaccine has been shown to be effective in preventing severe illness and hospitalization from COVID-19. The vast majority of\n",
      "Reference 37: Even if the individuals had underlying health issues, it is important to recognize that contracting COVID-19 likely exacerbated their conditions and ultimately led to their deaths. Downplaying the significance of COVID-19 in these cases undermines the seriousness of the virus and the need for continued caution and preventative measures. Additionally, it is important to consider that correlation does not always equal causation when it comes to vaccine side effects, and the benefits of vaccination often outweigh the potential risks.\n",
      "---\n",
      "Prediction 38: Just because a vaccine was developed quickly and efficiently does not mean it was unethical or harmful. The urgency of the COVID-19 pandemic required swift action, and the benefits of getting vaccinated far outweigh any potential negative consequences. It is important to remember that vaccines go through rigorous testing and approval processes to ensure their safety and efficacy. Rushing the development process does not necessarily mean the vaccine is unethical or harmful, but rather that caution and transparency are essential in such a rapidly evolving situation.\n",
      "Reference 38: It's important to recognize the significant scientific progress and collaboration that led to the development of the Pfizer and BioNTech vaccine. Dismissing their achievements as merely a \"gander\" undermines the hard work and dedication of countless researchers and scientists.\n",
      "---\n",
      "Prediction 39: Counter argument: Just because a product is labeled as a vaccine does not mean it is not a vaccine. Vaccines are designed to stimulate the immune system to protect against a specific disease, and mRNA technology is commonly used in vaccine development. It is important to recognize the science behind vaccines and the benefits they provide in preventing illness. While it is true that mRNA technology is commonly used in vaccine development, it is important to note that the Moderna vaccine uses a different approach compared to traditional mRNA vaccines\n",
      "Reference 39: Actually, mRNA vaccines have proven to be highly effective and safe in preventing severe illness and death from COVID-19. It is based on cutting-edge technology and has undergone rigorous testing before being approved for emergency use. It is not accurate to dismiss it as just \"more humanize mice.\"\n",
      "---\n",
      "Prediction 40: It is important to prioritize public health and safety over personal beliefs. Tainted vaccines pose a significant threat to public health and could lead to devastating consequences if not taken seriously. Death penalty is a severe punishment that should not be considered in this context. While it is important to consider public health and safety, the death penalty is a permanent and irreversible punishment that should not be used lightly. There are other ways to address concerns about tainted vaccines, such as implementing strict regulations and monitoring systems.\n",
      "Reference 40: Taking the vaccine is a personal choice, but advocating for the punishment of those who choose not to take it goes against individual freedoms and medical autonomy. Instead of seeking extreme penalties, efforts should be focused on educating and encouraging vaccination.\n",
      "---\n",
      "Prediction 41: It's important to trust the expertise of health professionals and regulatory bodies who have thoroughly reviewed and approved the Pfizer vaccine. Their safety measures and research should be enough to convince the public that it is safe and effective. This counter argument is valid as healthcare professionals and regulatory bodies are dedicated to ensuring the safety and efficacy of vaccines before they are approved for distribution. Trusting in the experts' expertise is crucial in combating the spread of COVID-19. It'\n",
      "Reference 41: It is not uncommon for pharmaceutical companies to conduct rigorous testing and research before releasing a vaccine to the public. Additionally, government regulatory bodies also conduct their own assessments to ensure the safety and efficacy of the vaccine. It is likely that Pfizer's vaccine has been thoroughly vetted by multiple sources before being deemed safe for distribution.\n",
      "---\n",
      "Prediction 42: It is important to separate the actions and intentions of individuals like Bill Gates from the scientific community and experts who contributed to the development of the Oxford vaccine. The focus should be on the rigorous testing and approval processes in place for vaccines, rather than attributing questionable motives to those involved in the development of the Oxford vaccine. tÃ¦t pÃ¥: The Oxford vaccine was developed by a team of scientists and researchers at the University of Oxford, not by Bill Gates or any other individual. The vaccine was rigorously\n",
      "Reference 42: While it is true that Bill Gates has provided funding to the Oxford lab, it does not mean that the vaccine being developed is part of a sinister plot to depopulate the world. The vaccine development process is rigorous and involves many scientific experts, not just Bill Gates. Additionally, vaccines go through strict testing and approval processes to ensure their safety and effectiveness before being distributed to the public. The Oxford vaccine is a result of hard work and collaboration from a team of dedicated researchers, not a covert plan for population control.\n",
      "---\n",
      "Prediction 43: While it is true that the COVID-19 vaccine has been in development for several years, it is important to note that the urgency of the pandemic has expedited the process and allowed for increased funding and resources to be allocated to research and development. Additionally, the vaccine has undergone rigorous testing in clinical trials and has been approved by regulatory agencies such as the FDA and WHO. The effectiveness of the vaccine in reducing the severity of symptoms and preventing hospitalization and death is well-documented, making it a valuable tool\n",
      "Reference 43: While it is true that the COVID-19 vaccine was developed much faster than previous vaccines, it's important to consider the urgency of the situation. The global pandemic has caused immense suffering and loss of life, so a rapid response was necessary. Additionally, the vaccine underwent rigorous testing and clinical trials to ensure its safety and efficacy. Taking the vaccine can significantly reduce the spread of the virus and ultimately save lives.\n",
      "---\n",
      "Prediction 44: While it is tragic to hear about this incident of blood-filled blisters on a woman's legs after receiving the AstraZeneca vaccine, it is important to remember that these cases are extremely rare and the benefits of the vaccine far outweigh the risks. The majority of people who receive the AstraZeneca vaccine do not experience such severe side effects, and the vaccine has been proven to be safe and effective in preventing severe illness and death from COVID-19. It is crucial to consider the overall public health impact of vaccines\n",
      "Reference 44: It is important to remember that the occurrence of side effects, such as blisters, after receiving a vaccine is extremely rare and should not deter individuals from getting vaccinated. The benefits of the vaccine in preventing the spread of COVID-19 far outweigh the risks of potential side effects.\n",
      "---\n",
      "Prediction 45: While it is true that lymphadenopathy can be seen after receiving a COVID shot, it is important to note that the occurrence of this condition is rare. The benefits of getting vaccinated far outweigh the risks, as they have been proven to significantly reduce the risk of contracting and spreading the virus. It is crucial to trust in the scientific evidence supporting the safety and efficacy of vaccines. It is important to consider the overall risk of lymphadenopathy compared to the risks of not being vaccinated against\n",
      "Reference 45: While it is important to acknowledge and monitor potential side effects of COVID vaccines, it is essential to put these occurrences into perspective. The benefits of vaccination, such as preventing severe illness and death from COVID-19, far outweigh the potential risks of rare side effects. Additionally, it is important to consider that correlation does not imply causation and further investigation is needed to determine the cause of any adverse events following vaccination.\n",
      "---\n",
      "Prediction 46: While it is understandable to be frustrated with the administration's handling of the situation, it is important to remember that they are working with limited resources and trying to make the best decisions possible during a global pandemic. Their actions may seem counterintuitive at times, but it is important to give them the benefit of the doubt until concrete evidence shows otherwise. It is also worth noting that the administration's actions are guided by scientific data and research, not political agendas. The goal is to\n",
      "Reference 46: While it is understandable to be frustrated with changing rules and restrictions, it is important to prioritize public health and safety during a global pandemic. The administration's efforts to pass a recovery bill and promote vaccination numbers show a commitment to combating the virus and ultimately allowing for more freedom in the long run. It is necessary to adapt to new information and guidelines in order to protect the well-being of all citizens.\n",
      "---\n",
      "Prediction 47: It is important to consider that vaccines have been proven to save lives and prevent the spread of deadly diseases. The potential risks of not getting vaccinated far outweigh the supposed dangers of the vaccines, and individuals should weigh the benefits of public health against any perceived risks. Actually, vaccines have been instrumental in eradicating diseases like smallpox and significantly reducing others like polio. Trusting in the science and following guidelines from healthcare professionals is crucial in protecting public health. grÄƒ While vaccines may have\n",
      "Reference 47: Counter argument: The development and distribution of vaccines for Covid-19 are essential in order to protect public health and prevent further spread of the virus. While there may be concerns about individual liberties, widespread vaccination is crucial in order to achieve herd immunity and ultimately save lives.\n",
      "---\n",
      "Prediction 48: It is important to consider that correlation does not always equal causation. It is possible that the nurse's sudden death was due to a pre-existing medical condition or other factors, rather than the Pfizer vaccine itself. It is crucial to wait for a thorough investigation before jumping to conclusions about the safety of the vaccine. While it is tragic that this nurse died after receiving the Pfizer vaccine, it is important to remember that millions of doses have been administered worldwide without any serious adverse effects.\n",
      "Reference 48: While any death following a vaccination is concerning, it is important to remember that millions of people have safely received the Pfizer vaccine with no severe side effects. It is crucial to wait for a thorough investigation and analysis of the nurse's death before jumping to conclusions about the safety of the vaccine.\n",
      "---\n",
      "Prediction 49: While it is true that the virus may be smaller than the pores in a mask, the main goal of wearing a mask is to prevent the spread of the virus to others. The CDC and other health experts have consistently recommended wearing masks to help slow the spread of COVID-19. It is important to trust in the science and recommendations of healthcare professionals rather than spreading misinformation. It is important to fact-check information before spreading it to others. The COVID-19 virus is\n",
      "Reference 49: While it is true that the COVID virus is small, the mask is not meant to completely block the virus, but rather reduce the spread of respiratory droplets that may contain the virus. Wearing a mask, along with practicing social distancing and proper hygiene, can still help prevent the spread of COVID-19. Additionally, the claims about the Moderna vaccine causing Bell's palsy, sterilization, and immune system shutdown are not supported by scientific evidence and have been debunked by reputable health organizations. It is important to fact-check information before spreading misinformation.\n",
      "---\n",
      "Prediction 50: While the vaccine may not be a cure, it is a significant step in controlling the spread of the virus and reducing the severity of symptoms. It is important to continue following guidelines and taking precautions to protect ourselves and others. tÃ¦t pÃ¥: Vaccines are not a cure, but rather a tool to help prevent the spread of disease and protect public health. It is important to follow the guidance of health experts and continue practicing good hygiene to stop the spread of COVID-19. grÄƒnde\n",
      "Reference 50: While the vaccine may not be a cure, it is an important tool in preventing the spread of the virus and reducing the severity of symptoms for those who do contract it. It plays a crucial role in building herd immunity and protecting vulnerable populations.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "num_samples=50\n",
    "prompts = [pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": dataset_chatml['test'][i]['messages'][0]['content']}], tokenize=False, add_generation_prompt=True)\n",
    "                                              for i in range(num_samples)]\n",
    "outputs = pipe(prompts, batch_size=1, max_new_tokens=200, do_sample=True, num_beams=1, temperature=0.7, top_k=50, top_p=0.9,repetition_penalty=1.2,\n",
    "                 )\n",
    "# print(outputs)\n",
    "preds = []\n",
    "for i in range(len(outputs)):\n",
    "    generated_text = outputs[i][0]['generated_text']\n",
    "    response = generated_text[len(prompts[i]):].split()\n",
    "    # print(response)\n",
    "    if len(response) > 1:\n",
    "        # Extract the counter argument\n",
    "        pred = \" \".join(response)\n",
    "    else:\n",
    "        pred = \"\"  # Handle case with no valid split\n",
    "    preds.append(pred)\n",
    "\n",
    "    # Print prediction and corresponding reference and tweet \n",
    "\n",
    "    # print(f\"Tweet {i + 1}: {dataset_chatml['test'][i]['text']}\")\n",
    "    print(f\"Prediction {i + 1}: {pred}\")\n",
    "    print(f\"Reference {i + 1}: {dataset_chatml['test'][i]['counter_argument']}\")\n",
    "    print(\"---\")  # Separator for clarity\n",
    "references= [dataset_chatml['test'][i]['counter_argument'] for i in range(len(outputs))]\n",
    "metric.add_batch(predictions=preds, references=references)\n",
    "tweets = [dataset_chatml['test'][i]['text'] for i in range(len(outputs))]\n",
    "\n",
    "# save in a csv file\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(list(zip(tweets, preds, references)), columns=['tweet', 'prediction', 'reference'])\n",
    "df.to_csv(\"/home/sohampoddar/HDD2/utsav/Cross_Experiment_2/gemma-2b-bit_finetuned_cross_2_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.add_batch(predictions=preds, references=references)\n",
    "result = metric.compute(use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge 1 Mean:  41.43466509100617\n",
      "Rouge 2 Mean:  16.98366220525522\n",
      "Rouge L Mean:  27.516073247298234\n",
      "Rouge Lsum Mean:  27.51243417756933\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming result contains your ROUGE scores\n",
    "rouge1_mean = np.mean(result['rouge1']) * 100\n",
    "rouge2_mean = np.mean(result['rouge2']) * 100\n",
    "rougeL_mean = np.mean(result['rougeL']) * 100\n",
    "rougeLsum_mean = np.mean(result['rougeLsum']) * 100\n",
    "\n",
    "print(\"Rouge 1 Mean: \", rouge1_mean)\n",
    "print(\"Rouge 2 Mean: \", rouge2_mean)\n",
    "print(\"Rouge L Mean: \", rougeL_mean)\n",
    "print(\"Rouge Lsum Mean: \", rougeLsum_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Using Bert Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-score in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (0.3.13)\n",
      "Requirement already satisfied: torch>=1.0.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (2.3.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (2.0.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (4.47.0.dev0)\n",
      "Requirement already satisfied: numpy in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (1.25.1)\n",
      "Requirement already satisfied: requests in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (4.65.0)\n",
      "Requirement already satisfied: matplotlib in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (3.7.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score) (2023.3)\n",
      "Requirement already satisfied: filelock in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (1.12)\n",
      "Requirement already satisfied: networkx in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (0.26.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (0.4.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (4.41.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests->bert-score) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests->bert-score) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests->bert-score) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests->bert-score) (2023.7.22)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->bert-score) (3.16.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install bert-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/vocab.json\n",
      "loading file merges.txt from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer.json\n",
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/model.safetensors\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 46.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.57 seconds, 88.44 sentences/sec\n",
      "BERTScore Precision: 0.8778289556503296\n",
      "BERTScore Recall: 0.8938378691673279\n",
      "BERTScore F1: 0.8856953382492065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "P, R, F1 = score(preds, references, lang=\"en\", verbose=True)\n",
    "\n",
    "# Display BERTScore results\n",
    "print(\"BERTScore Precision:\", P.mean().item())\n",
    "print(\"BERTScore Recall:\", R.mean().item())\n",
    "print(\"BERTScore F1:\", F1.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
