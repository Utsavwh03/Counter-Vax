{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the gpu \n",
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# # Generate\n",
    "# generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "# tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Generate counter argument for tweet: @jamesgraham521 @YouTube What does that sound like to you ? Vaccine programs inject half live viruses into a person . I think I â€™ ll chance my luck with my immune system Talk About potential side effects and adverse reactions associated with vaccines , such as anaphylaxis , blood clots , or other autoimmune disorders . # vaccineharm # vaccineadverseevents\\n\\n## Step 1: Identify the main claim of the tweet\\nThe main claim of the tweet is that vaccine programs inject half live viruses into a person, which raises concerns about potential side effects and adverse reactions.\\n\\n## Step 2: Identify the counterargument\\nThe counterargument is that vaccines are safe and do not inject half live viruses into a person.\\n\\n## Step 3: Provide evidence to support the counterargument\\nThere is no scientific evidence to support the claim that vaccines inject half live viruses into a person. In fact, vaccines work by introducing a small, harmless piece of a virus or bacteria to the body, which triggers an immune response without causing the full-blown disease.\\n\\n## Step 4: Address potential side effects and adverse reactions\\nWhile it is true that vaccines can cause side effects and adverse reactions, such as'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt = \"Generate counter argument for tweet: @jamesgraham521 @YouTube What does that sound like to you ? Vaccine programs inject half live viruses into a person . I think I â€™ ll chance my luck with my immune system Talk About potential side effects and adverse reactions associated with vaccines\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=250)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL DESCRIPTIONS\n",
    "\n",
    "labels_map = {}\n",
    "labels_map['religious'] = \"religious beliefs and their influence on views about vaccines\"\n",
    "labels_map['political'] = \"the political factors that affect perceptions of vaccine use\"\n",
    "labels_map['ingredients'] = \"concerns about the ingredients and chemical components in vaccines\"\n",
    "labels_map['unnecessary'] = \"the importance and necessity of getting vaccinated to prevent diseases\"\n",
    "labels_map['conspiracy'] = \"conspiracy theories suggesting hidden motives behind vaccination efforts\"\n",
    "labels_map['mandatory'] = \"the debate over personal choice versus mandates in vaccination policies\"\n",
    "labels_map['ineffective'] = \"evidence and reasons that support the effectiveness of vaccines\"\n",
    "labels_map['side-effect'] = \"potential side effects and adverse reactions associated with vaccines\"\n",
    "labels_map['pharma'] = \"the role of pharmaceutical companies and concerns about profit motives\"\n",
    "labels_map['rushed'] = \"claims that vaccines were approved or developed without sufficient testing\"\n",
    "labels_map['country'] = \"national biases and objections to vaccines produced by specific countries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_column_with_labels(row):\n",
    "        # Initialize an empty list to store the messages.\n",
    "    messages = []\n",
    "    # print(row)\n",
    "    prompt= f\"Generate Counter Argument for the anti-vaccine tweet:\\n Tweet: {row['text']}\\n Talk About \"\n",
    "\n",
    "    lab=row['labels'].split()\n",
    "    if isinstance(lab, list):\n",
    "            mapped_labels = \" and \".join([labels_map.get(l, \"\") for l in lab])\n",
    "    else:\n",
    "            mapped_labels = labels_map.get(lab, \"\")\n",
    "    # print(mapped_labels)\n",
    "    prompt+= mapped_labels\n",
    "    prompt+= \" ##Output: \"\n",
    "    user = {\n",
    "        \"content\": prompt,\n",
    "        \"role\": \"user\"\n",
    "    }\n",
    "    \n",
    "    # Append the 'user' message to the 'messages' list.\n",
    "    messages.append(user)\n",
    "    \n",
    "    # Create an 'assistant' message dictionary with 'content' and 'role' keys.\n",
    "    assistant = {\n",
    "        \"content\": f\"{row['counter_argument']}\",\n",
    "        \"role\": \"assistant\"\n",
    "    }\n",
    "    \n",
    "    # Append the 'assistant' message to the 'messages' list.\n",
    "    messages.append(assistant)\n",
    "    \n",
    "    # Return a dictionary with a 'messages' key and the 'messages' list as its value.\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def format_dataset_chatml(row):\n",
    "    return {\"text\": tokenizer.apply_chat_template(row[\"messages\"], add_generation_prompt=False, tokenize=False)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 3227.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "train_data= load_dataset('csv', data_files='/home/sohampoddar/HDD2/utsav/Data/train_CA_with_label_desc.csv',split='train')\n",
    "val_data= load_dataset('csv', data_files='/home/sohampoddar/HDD2/utsav/Data/val_data_with_labels_desc.csv',split='train')\n",
    "\n",
    "# train_data=train_data[:5]\n",
    "dataset_chatml = train_data.map(create_message_column_with_labels)\n",
    "dataset_chatml = dataset_chatml.map(format_dataset_chatml)\n",
    "\n",
    "val_dataset_chatml=val_data.map(create_message_column_with_labels)\n",
    "val_dataset_chatml=val_dataset_chatml.map(format_dataset_chatml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'text', 'labels', 'counter_argument', 'messages'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_chatml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_column_without_labels(row):\n",
    "        # Initialize an empty list to store the messages.\n",
    "    messages = []\n",
    "    \n",
    "    # Create a 'user' message dictionary with 'content' and 'role' keys.\n",
    "#     print(row['text'])\n",
    "    user = {\n",
    "        \"content\": f\"Generate Counter Argument for the anti-vaccine tweet:\\n Tweet: {row['text']}\\n##Output: \",\n",
    "        \"role\": \"user\"\n",
    "    }\n",
    "    \n",
    "    # Append the 'user' message to the 'messages' list.\n",
    "    messages.append(user)\n",
    "    \n",
    "    # Create an 'assistant' message dictionary with 'content' and 'role' keys.\n",
    "    assistant = {\n",
    "        \"content\": f\"{row['counter_argument']}\",\n",
    "        \"role\": \"assistant\"\n",
    "    }\n",
    "    \n",
    "    # Append the 'assistant' message to the 'messages' list.\n",
    "    messages.append(assistant)\n",
    "    \n",
    "    # Return a dictionary with a 'messages' key and the 'messages' list as its value.\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def format_dataset_chatml(row):\n",
    "    return {\"text\": tokenizer.apply_chat_template(row[\"messages\"], add_generation_prompt=False, tokenize=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "train_data= load_dataset('csv', data_files='/home/sohampoddar/HDD2/utsav/Data/train_CA_without_labels.csv',split='train')\n",
    "val_data= load_dataset('csv', data_files='/home/sohampoddar/HDD2/utsav/Data/val_CA_without_labels.csv',split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:00<00:00, 11972.98 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:00<00:00, 5492.09 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 987/987 [00:00<00:00, 5804.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_chatml = train_data.map(create_message_column_with_labels)\n",
    "dataset_chatml = dataset_chatml.map(format_dataset_chatml)\n",
    "\n",
    "val_dataset_chatml=val_data.map(create_message_column_with_labels)\n",
    "val_dataset_chatml=val_dataset_chatml.map(format_dataset_chatml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import peft ,trl\n",
    "\n",
    "# 'load_dataset' is a function from the 'datasets' library by Hugging Face which allows you to load a dataset.\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, TaskType, PeftModel\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "# 'SFTTrainer' is a class from the 'trl' library that provides a trainer for soft fine-tuning.\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flash_attention_2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 'torch.cuda.is_bf16_supported()' is a function that checks if bfloat16 is supported on the current CUDA device.\n",
    "# If bfloat16 is supported, 'compute_dtype' is set to 'torch.bfloat16' and 'attn_implementation' is set to 'flash_attention_2'.\n",
    "if torch.cuda.is_bf16_supported():\n",
    "  compute_dtype = torch.bfloat16\n",
    "  attn_implementation = 'flash_attention_2'\n",
    "# If bfloat16 is not supported, 'compute_dtype' is set to 'torch.float16' and 'attn_implementation' is set to 'sdpa'.\n",
    "else:\n",
    "  compute_dtype = torch.float16\n",
    "  attn_implementation = 'sdpa'\n",
    "\n",
    "# # This line of code is used to print the value of 'attn_implementation', which indicates the chosen attention implementation.\n",
    "print(attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as padding token if none is defined\n",
    "\n",
    "# Set the pad_token_id\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "# Set padding side if needed\n",
    "tokenizer.padding_side = 'right'\n",
    "# set cuda available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # 'cuda' will point to the visible GPU (e.g., GPU 0 as set by CUDA_VISIBLE_DEVICES)\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "use_4bit = True\n",
    "# 'bnb_4bit_compute_dtype' is the data type that should be used for computations with the 4-bit base model. In this case, it is set to 'bfloat16'.\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "\n",
    "# 'bnb_4bit_quant_type' is the type of quantization that should be used for the 4-bit base model. In this case, it is set to 'nf4'.\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# 'use_double_quant' is a boolean that controls whether nested quantization should be used for the 4-bit base model.\n",
    "use_double_quant = True\n",
    "bnb_config= BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_use_double_quant=True,\n",
    "      bnb_4bit_quant_type=\"nf4\",\n",
    "      bnb_4bit_compute_dtype=torch.bfloat16\n",
    "  )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_id, torch_dtype=compute_dtype, trust_remote_code=True, quantization_config=bnb_config, device_map=\"auto\",\n",
    "#           attn_implementation=attn_implementation\n",
    ")\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    "peft_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=target_modules,\n",
    ")\n",
    "# model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/transformers/training_args.py:1570: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = TrainingArguments(\n",
    "        output_dir=\"/home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-3B-fine-tuned\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        do_eval=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        log_level=\"debug\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=1e-4,\n",
    "        logging_dir=None,  # Directory for storing logs\n",
    "        logging_steps=100,     # Log after every 100 steps\n",
    "        logging_strategy=\"steps\",  # Log by steps instead of epochs\n",
    "        fp16=True,\n",
    "#         bf16 = torch.cuda.is_bf16_supported(),\n",
    "        eval_steps=100,\n",
    "        num_train_epochs=2,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "#     report_to=\"wandb\",\n",
    "        seed=42,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (0.18.7)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (8.1.6)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.15.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (2.19.0)\n",
      "Requirement already satisfied: setproctitle in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (68.0.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from wandb) (4.11.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f5f93fbb820>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "import os\n",
    "\n",
    "os.environ[\"PROJECT\"]=\"DecoderModels\"\n",
    "project_name = \"DecoderModels\"\n",
    "\n",
    "wandb.init(project=project_name, name = \"DecoderModels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/transformers/training_args.py:1570: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/transformers/training_args.py:2065: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/transformers/training_args.py:1570: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/transformers/training_args.py:2065: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:00<00:00, 6782.76 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 987/987 [00:00<00:00, 7905.44 examples/s]\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:402: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Using auto half precision backend\n",
      "[codecarbon INFO @ 18:10:36] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 18:10:36] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 18:10:36] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 18:10:36] [setup] CPU Tracking...\n",
      "[codecarbon INFO @ 18:10:36] Tracking Intel CPU via RAPL interface\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[codecarbon INFO @ 18:10:37] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 18:10:37]   Platform system: Linux-5.15.0-124-generic-x86_64-with-glibc2.31\n",
      "[codecarbon INFO @ 18:10:37]   Python version: 3.9.17\n",
      "[codecarbon INFO @ 18:10:37]   CodeCarbon version: 2.3.2\n",
      "[codecarbon INFO @ 18:10:37]   Available RAM : 125.517 GB\n",
      "[codecarbon INFO @ 18:10:37]   CPU count: 40\n",
      "[codecarbon INFO @ 18:10:37]   CPU model: Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz\n",
      "[codecarbon INFO @ 18:10:37]   GPU count: 4\n",
      "[codecarbon INFO @ 18:10:37]   GPU model: 2 x NVIDIA RTX A60002 x NVIDIA RTX A5000\n",
      "Currently training with a batch size of: 2\n",
      "***** Running training *****\n",
      "  Num examples = 1,500\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 750\n",
      "  Number of trainable parameters = 6,078,464\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutsavdhanuka03\u001b[0m (\u001b[33mutsavdhanuka03-iit-kharagpur\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/antpc/HDD2/sohampoddar/utsav/Model_Codes_with_labels/wandb/run-20250117_181044-m696atn3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/utsavdhanuka03-iit-kharagpur/huggingface/runs/m696atn3' target=\"_blank\">/home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-3B-fine-tuned</a></strong> to <a href='https://wandb.ai/utsavdhanuka03-iit-kharagpur/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/utsavdhanuka03-iit-kharagpur/huggingface' target=\"_blank\">https://wandb.ai/utsavdhanuka03-iit-kharagpur/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/utsavdhanuka03-iit-kharagpur/huggingface/runs/m696atn3' target=\"_blank\">https://wandb.ai/utsavdhanuka03-iit-kharagpur/huggingface/runs/m696atn3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 13:49, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.960129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.758000</td>\n",
       "      <td>0.946076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:11:01] Energy consumed for RAM : 0.000196 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:11:01] Energy consumed for all GPUs : 0.001342 kWh. Total GPU Power : 321.9128188686631 W\n",
      "[codecarbon INFO @ 18:11:01] Energy consumed for all CPUs : 0.000227 kWh. Total CPU Power : 54.47777160708885 W\n",
      "[codecarbon INFO @ 18:11:01] 0.001765 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:16] Energy consumed for RAM : 0.000392 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:11:16] Energy consumed for all GPUs : 0.002721 kWh. Total GPU Power : 331.23928676549497 W\n",
      "[codecarbon INFO @ 18:11:16] Energy consumed for all CPUs : 0.000453 kWh. Total CPU Power : 54.20254813421698 W\n",
      "[codecarbon INFO @ 18:11:16] 0.003566 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:31] Energy consumed for RAM : 0.000588 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:11:31] Energy consumed for all GPUs : 0.004106 kWh. Total GPU Power : 332.57767643113687 W\n",
      "[codecarbon INFO @ 18:11:31] Energy consumed for all CPUs : 0.000679 kWh. Total CPU Power : 54.31574626580952 W\n",
      "[codecarbon INFO @ 18:11:31] 0.005374 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:46] Energy consumed for RAM : 0.000784 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:11:46] Energy consumed for all GPUs : 0.005511 kWh. Total GPU Power : 337.33133617607905 W\n",
      "[codecarbon INFO @ 18:11:46] Energy consumed for all CPUs : 0.000906 kWh. Total CPU Power : 54.386669166973434 W\n",
      "[codecarbon INFO @ 18:11:46] 0.007201 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:12:01] Energy consumed for RAM : 0.000980 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:12:01] Energy consumed for all GPUs : 0.006928 kWh. Total GPU Power : 339.83271951293347 W\n",
      "[codecarbon INFO @ 18:12:01] Energy consumed for all CPUs : 0.001133 kWh. Total CPU Power : 54.36803780002519 W\n",
      "[codecarbon INFO @ 18:12:01] 0.009041 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:12:16] Energy consumed for RAM : 0.001176 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:12:16] Energy consumed for all GPUs : 0.008348 kWh. Total GPU Power : 341.1616959223473 W\n",
      "[codecarbon INFO @ 18:12:16] Energy consumed for all CPUs : 0.001360 kWh. Total CPU Power : 54.6502233711276 W\n",
      "[codecarbon INFO @ 18:12:16] 0.010885 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:12:31] Energy consumed for RAM : 0.001372 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:12:31] Energy consumed for all GPUs : 0.009771 kWh. Total GPU Power : 341.5454269688598 W\n",
      "[codecarbon INFO @ 18:12:31] Energy consumed for all CPUs : 0.001587 kWh. Total CPU Power : 54.43679586522669 W\n",
      "[codecarbon INFO @ 18:12:31] 0.012731 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:12:46] Energy consumed for RAM : 0.001568 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:12:46] Energy consumed for all GPUs : 0.011188 kWh. Total GPU Power : 340.2573446491044 W\n",
      "[codecarbon INFO @ 18:12:46] Energy consumed for all CPUs : 0.001814 kWh. Total CPU Power : 54.50074558886922 W\n",
      "[codecarbon INFO @ 18:12:46] 0.014570 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:13:01] Energy consumed for RAM : 0.001764 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:13:01] Energy consumed for all GPUs : 0.012599 kWh. Total GPU Power : 338.9087570786191 W\n",
      "[codecarbon INFO @ 18:13:01] Energy consumed for all CPUs : 0.002041 kWh. Total CPU Power : 54.50233854335171 W\n",
      "[codecarbon INFO @ 18:13:01] 0.016405 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:13:16] Energy consumed for RAM : 0.001960 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:13:16] Energy consumed for all GPUs : 0.013994 kWh. Total GPU Power : 334.87807649961553 W\n",
      "[codecarbon INFO @ 18:13:16] Energy consumed for all CPUs : 0.002269 kWh. Total CPU Power : 54.575027502277344 W\n",
      "[codecarbon INFO @ 18:13:16] 0.018223 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:13:31] Energy consumed for RAM : 0.002156 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:13:31] Energy consumed for all GPUs : 0.015393 kWh. Total GPU Power : 335.95169344792026 W\n",
      "[codecarbon INFO @ 18:13:31] Energy consumed for all CPUs : 0.002496 kWh. Total CPU Power : 54.545693838359774 W\n",
      "[codecarbon INFO @ 18:13:31] 0.020045 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:13:46] Energy consumed for RAM : 0.002352 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:13:46] Energy consumed for all GPUs : 0.016789 kWh. Total GPU Power : 335.3076129647661 W\n",
      "[codecarbon INFO @ 18:13:46] Energy consumed for all CPUs : 0.002722 kWh. Total CPU Power : 54.4235103228589 W\n",
      "[codecarbon INFO @ 18:13:46] 0.021863 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:14:01] Energy consumed for RAM : 0.002548 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:14:01] Energy consumed for all GPUs : 0.018173 kWh. Total GPU Power : 332.3831671708011 W\n",
      "[codecarbon INFO @ 18:14:01] Energy consumed for all CPUs : 0.002949 kWh. Total CPU Power : 54.47636373088879 W\n",
      "[codecarbon INFO @ 18:14:01] 0.023671 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:14:16] Energy consumed for RAM : 0.002744 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:14:16] Energy consumed for all GPUs : 0.019565 kWh. Total GPU Power : 334.07443361880166 W\n",
      "[codecarbon INFO @ 18:14:16] Energy consumed for all CPUs : 0.003177 kWh. Total CPU Power : 54.551353777244564 W\n",
      "[codecarbon INFO @ 18:14:16] 0.025485 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:14:31] Energy consumed for RAM : 0.002940 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:14:31] Energy consumed for all GPUs : 0.020955 kWh. Total GPU Power : 333.83714130586606 W\n",
      "[codecarbon INFO @ 18:14:31] Energy consumed for all CPUs : 0.003405 kWh. Total CPU Power : 54.748150162191905 W\n",
      "[codecarbon INFO @ 18:14:31] 0.027299 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:14:46] Energy consumed for RAM : 0.003136 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:14:46] Energy consumed for all GPUs : 0.022347 kWh. Total GPU Power : 334.22994822323324 W\n",
      "[codecarbon INFO @ 18:14:46] Energy consumed for all CPUs : 0.003632 kWh. Total CPU Power : 54.563793455930465 W\n",
      "[codecarbon INFO @ 18:14:46] 0.029114 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:15:01] Energy consumed for RAM : 0.003332 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:15:01] Energy consumed for all GPUs : 0.023735 kWh. Total GPU Power : 333.44936239878336 W\n",
      "[codecarbon INFO @ 18:15:01] Energy consumed for all CPUs : 0.003858 kWh. Total CPU Power : 54.36365344491877 W\n",
      "[codecarbon INFO @ 18:15:01] 0.030926 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:15:16] Energy consumed for RAM : 0.003528 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:15:16] Energy consumed for all GPUs : 0.025134 kWh. Total GPU Power : 335.85993113902674 W\n",
      "[codecarbon INFO @ 18:15:16] Energy consumed for all CPUs : 0.004086 kWh. Total CPU Power : 54.55156886153196 W\n",
      "[codecarbon INFO @ 18:15:16] 0.032747 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:15:31] Energy consumed for RAM : 0.003724 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:15:31] Energy consumed for all GPUs : 0.026530 kWh. Total GPU Power : 335.4054337134483 W\n",
      "[codecarbon INFO @ 18:15:31] Energy consumed for all CPUs : 0.004313 kWh. Total CPU Power : 54.477660183110615 W\n",
      "[codecarbon INFO @ 18:15:31] 0.034567 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:15:46] Energy consumed for RAM : 0.003920 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:15:46] Energy consumed for all GPUs : 0.027931 kWh. Total GPU Power : 336.3000865594172 W\n",
      "[codecarbon INFO @ 18:15:46] Energy consumed for all CPUs : 0.004540 kWh. Total CPU Power : 54.55434813635365 W\n",
      "[codecarbon INFO @ 18:15:46] 0.036390 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:16:01] Energy consumed for RAM : 0.004116 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:16:01] Energy consumed for all GPUs : 0.029333 kWh. Total GPU Power : 336.5771812791227 W\n",
      "[codecarbon INFO @ 18:16:01] Energy consumed for all CPUs : 0.004767 kWh. Total CPU Power : 54.49641190429561 W\n",
      "[codecarbon INFO @ 18:16:01] 0.038215 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:16:16] Energy consumed for RAM : 0.004312 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:16:16] Energy consumed for all GPUs : 0.030727 kWh. Total GPU Power : 334.8629895436064 W\n",
      "[codecarbon INFO @ 18:16:16] Energy consumed for all CPUs : 0.004995 kWh. Total CPU Power : 54.632323641731446 W\n",
      "[codecarbon INFO @ 18:16:16] 0.040033 kWh of electricity used since the beginning.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 987\n",
      "  Batch size = 2\n",
      "[codecarbon INFO @ 18:16:31] Energy consumed for RAM : 0.004507 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:16:31] Energy consumed for all GPUs : 0.032127 kWh. Total GPU Power : 336.60274604244125 W\n",
      "[codecarbon INFO @ 18:16:31] Energy consumed for all CPUs : 0.005222 kWh. Total CPU Power : 54.739775591692855 W\n",
      "[codecarbon INFO @ 18:16:31] 0.041856 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:16:46] Energy consumed for RAM : 0.004703 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:16:46] Energy consumed for all GPUs : 0.033565 kWh. Total GPU Power : 345.4337967742845 W\n",
      "[codecarbon INFO @ 18:16:46] Energy consumed for all CPUs : 0.005454 kWh. Total CPU Power : 55.686500032484496 W\n",
      "[codecarbon INFO @ 18:16:46] 0.043723 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:17:01] Energy consumed for RAM : 0.004899 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:17:01] Energy consumed for all GPUs : 0.034994 kWh. Total GPU Power : 342.9773048230412 W\n",
      "[codecarbon INFO @ 18:17:01] Energy consumed for all CPUs : 0.005686 kWh. Total CPU Power : 55.65951046809088 W\n",
      "[codecarbon INFO @ 18:17:01] 0.045579 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:17:16] Energy consumed for RAM : 0.005095 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:17:16] Energy consumed for all GPUs : 0.036427 kWh. Total GPU Power : 344.31597050355373 W\n",
      "[codecarbon INFO @ 18:17:16] Energy consumed for all CPUs : 0.005918 kWh. Total CPU Power : 55.62190607498995 W\n",
      "[codecarbon INFO @ 18:17:16] 0.047440 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:17:31] Energy consumed for RAM : 0.005291 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:17:31] Energy consumed for all GPUs : 0.037848 kWh. Total GPU Power : 341.0824618082855 W\n",
      "[codecarbon INFO @ 18:17:31] Energy consumed for all CPUs : 0.006150 kWh. Total CPU Power : 55.6715569406029 W\n",
      "[codecarbon INFO @ 18:17:31] 0.049288 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-3B-fine-tuned/checkpoint-375\n",
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-3B-fine-tuned/checkpoint-375/tokenizer_config.json\n",
      "Special tokens file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-3B-fine-tuned/checkpoint-375/special_tokens_map.json\n",
      "/home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "[codecarbon INFO @ 18:17:46] Energy consumed for RAM : 0.005487 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:17:46] Energy consumed for all GPUs : 0.039195 kWh. Total GPU Power : 323.5246889103322 W\n",
      "[codecarbon INFO @ 18:17:46] Energy consumed for all CPUs : 0.006374 kWh. Total CPU Power : 53.7627775437641 W\n",
      "[codecarbon INFO @ 18:17:46] 0.051056 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:18:01] Energy consumed for RAM : 0.005683 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:18:01] Energy consumed for all GPUs : 0.040611 kWh. Total GPU Power : 340.0397258710366 W\n",
      "[codecarbon INFO @ 18:18:01] Energy consumed for all CPUs : 0.006601 kWh. Total CPU Power : 54.588673187818316 W\n",
      "[codecarbon INFO @ 18:18:01] 0.052895 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:18:16] Energy consumed for RAM : 0.005879 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:18:16] Energy consumed for all GPUs : 0.042019 kWh. Total GPU Power : 338.05743265703546 W\n",
      "[codecarbon INFO @ 18:18:16] Energy consumed for all CPUs : 0.006828 kWh. Total CPU Power : 54.55409779070459 W\n",
      "[codecarbon INFO @ 18:18:16] 0.054726 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:18:31] Energy consumed for RAM : 0.006075 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:18:31] Energy consumed for all GPUs : 0.043429 kWh. Total GPU Power : 338.6975602659661 W\n",
      "[codecarbon INFO @ 18:18:31] Energy consumed for all CPUs : 0.007055 kWh. Total CPU Power : 54.33588151283538 W\n",
      "[codecarbon INFO @ 18:18:31] 0.056559 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:18:46] Energy consumed for RAM : 0.006271 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:18:46] Energy consumed for all GPUs : 0.044840 kWh. Total GPU Power : 338.84295627824804 W\n",
      "[codecarbon INFO @ 18:18:46] Energy consumed for all CPUs : 0.007281 kWh. Total CPU Power : 54.31518180626904 W\n",
      "[codecarbon INFO @ 18:18:46] 0.058392 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:19:01] Energy consumed for RAM : 0.006467 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:19:01] Energy consumed for all GPUs : 0.046256 kWh. Total GPU Power : 340.02030548026073 W\n",
      "[codecarbon INFO @ 18:19:01] Energy consumed for all CPUs : 0.007510 kWh. Total CPU Power : 55.022107551279575 W\n",
      "[codecarbon INFO @ 18:19:01] 0.060233 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:19:16] Energy consumed for RAM : 0.006663 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:19:16] Energy consumed for all GPUs : 0.047675 kWh. Total GPU Power : 340.88046573026674 W\n",
      "[codecarbon INFO @ 18:19:16] Energy consumed for all CPUs : 0.007737 kWh. Total CPU Power : 54.42117669511856 W\n",
      "[codecarbon INFO @ 18:19:16] 0.062075 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:19:31] Energy consumed for RAM : 0.006859 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:19:31] Energy consumed for all GPUs : 0.049093 kWh. Total GPU Power : 340.51354526499614 W\n",
      "[codecarbon INFO @ 18:19:31] Energy consumed for all CPUs : 0.007964 kWh. Total CPU Power : 54.428922019694355 W\n",
      "[codecarbon INFO @ 18:19:31] 0.063916 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:19:46] Energy consumed for RAM : 0.007055 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:19:46] Energy consumed for all GPUs : 0.050520 kWh. Total GPU Power : 342.5206769211214 W\n",
      "[codecarbon INFO @ 18:19:46] Energy consumed for all CPUs : 0.008190 kWh. Total CPU Power : 54.382093223359234 W\n",
      "[codecarbon INFO @ 18:19:46] 0.065765 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:20:01] Energy consumed for RAM : 0.007251 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:20:01] Energy consumed for all GPUs : 0.051930 kWh. Total GPU Power : 338.7543065217959 W\n",
      "[codecarbon INFO @ 18:20:01] Energy consumed for all CPUs : 0.008417 kWh. Total CPU Power : 54.40053092623804 W\n",
      "[codecarbon INFO @ 18:20:01] 0.067598 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:20:16] Energy consumed for RAM : 0.007447 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:20:16] Energy consumed for all GPUs : 0.053344 kWh. Total GPU Power : 339.485698074559 W\n",
      "[codecarbon INFO @ 18:20:16] Energy consumed for all CPUs : 0.008644 kWh. Total CPU Power : 54.645786299491256 W\n",
      "[codecarbon INFO @ 18:20:16] 0.069435 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:20:31] Energy consumed for RAM : 0.007643 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:20:31] Energy consumed for all GPUs : 0.054759 kWh. Total GPU Power : 339.89400809195126 W\n",
      "[codecarbon INFO @ 18:20:31] Energy consumed for all CPUs : 0.008871 kWh. Total CPU Power : 54.418322463422435 W\n",
      "[codecarbon INFO @ 18:20:31] 0.071273 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:20:46] Energy consumed for RAM : 0.007839 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:20:46] Energy consumed for all GPUs : 0.056174 kWh. Total GPU Power : 339.6009012597855 W\n",
      "[codecarbon INFO @ 18:20:46] Energy consumed for all CPUs : 0.009098 kWh. Total CPU Power : 54.38685445388833 W\n",
      "[codecarbon INFO @ 18:20:46] 0.073110 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:21:01] Energy consumed for RAM : 0.008035 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:21:01] Energy consumed for all GPUs : 0.057583 kWh. Total GPU Power : 338.51195186052826 W\n",
      "[codecarbon INFO @ 18:21:01] Energy consumed for all CPUs : 0.009325 kWh. Total CPU Power : 54.459519366797046 W\n",
      "[codecarbon INFO @ 18:21:01] 0.074942 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:21:16] Energy consumed for RAM : 0.008230 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:21:16] Energy consumed for all GPUs : 0.059002 kWh. Total GPU Power : 340.76719657735595 W\n",
      "[codecarbon INFO @ 18:21:16] Energy consumed for all CPUs : 0.009551 kWh. Total CPU Power : 54.42836591011199 W\n",
      "[codecarbon INFO @ 18:21:16] 0.076784 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:21:31] Energy consumed for RAM : 0.008426 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:21:31] Energy consumed for all GPUs : 0.060417 kWh. Total GPU Power : 339.6857770219166 W\n",
      "[codecarbon INFO @ 18:21:31] Energy consumed for all CPUs : 0.009778 kWh. Total CPU Power : 54.361614604787235 W\n",
      "[codecarbon INFO @ 18:21:31] 0.078621 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:21:46] Energy consumed for RAM : 0.008622 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:21:46] Energy consumed for all GPUs : 0.061835 kWh. Total GPU Power : 340.6144230026109 W\n",
      "[codecarbon INFO @ 18:21:46] Energy consumed for all CPUs : 0.010004 kWh. Total CPU Power : 54.23878385487228 W\n",
      "[codecarbon INFO @ 18:21:46] 0.080461 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:22:01] Energy consumed for RAM : 0.008818 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:22:01] Energy consumed for all GPUs : 0.063261 kWh. Total GPU Power : 342.36313125493484 W\n",
      "[codecarbon INFO @ 18:22:01] Energy consumed for all CPUs : 0.010230 kWh. Total CPU Power : 54.289375893349494 W\n",
      "[codecarbon INFO @ 18:22:01] 0.082309 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:22:16] Energy consumed for RAM : 0.009014 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:22:16] Energy consumed for all GPUs : 0.064682 kWh. Total GPU Power : 341.2921287430394 W\n",
      "[codecarbon INFO @ 18:22:16] Energy consumed for all CPUs : 0.010456 kWh. Total CPU Power : 54.25275648547775 W\n",
      "[codecarbon INFO @ 18:22:16] 0.084152 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:22:31] Energy consumed for RAM : 0.009210 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:22:31] Energy consumed for all GPUs : 0.066104 kWh. Total GPU Power : 341.4774193999615 W\n",
      "[codecarbon INFO @ 18:22:31] Energy consumed for all CPUs : 0.010682 kWh. Total CPU Power : 54.20043907362372 W\n",
      "[codecarbon INFO @ 18:22:31] 0.085996 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:22:46] Energy consumed for RAM : 0.009406 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:22:46] Energy consumed for all GPUs : 0.067508 kWh. Total GPU Power : 337.0441454389737 W\n",
      "[codecarbon INFO @ 18:22:46] Energy consumed for all CPUs : 0.010908 kWh. Total CPU Power : 54.40444949648465 W\n",
      "[codecarbon INFO @ 18:22:46] 0.087822 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:23:01] Energy consumed for RAM : 0.009602 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:23:01] Energy consumed for all GPUs : 0.068924 kWh. Total GPU Power : 340.14701566305393 W\n",
      "[codecarbon INFO @ 18:23:01] Energy consumed for all CPUs : 0.011134 kWh. Total CPU Power : 54.27867716915242 W\n",
      "[codecarbon INFO @ 18:23:01] 0.089661 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:23:16] Energy consumed for RAM : 0.009798 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:23:16] Energy consumed for all GPUs : 0.070344 kWh. Total GPU Power : 340.92174834192514 W\n",
      "[codecarbon INFO @ 18:23:16] Energy consumed for all CPUs : 0.011360 kWh. Total CPU Power : 54.21872227458786 W\n",
      "[codecarbon INFO @ 18:23:16] 0.091502 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-3B-fine-tuned/checkpoint-750\n",
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-3B-fine-tuned/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-3B-fine-tuned/checkpoint-750/special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 987\n",
      "  Batch size = 2\n",
      "[codecarbon INFO @ 18:23:31] Energy consumed for RAM : 0.009994 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:23:31] Energy consumed for all GPUs : 0.071643 kWh. Total GPU Power : 312.05949757109425 W\n",
      "[codecarbon INFO @ 18:23:31] Energy consumed for all CPUs : 0.011572 kWh. Total CPU Power : 50.72209088664969 W\n",
      "[codecarbon INFO @ 18:23:31] 0.093209 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:23:46] Energy consumed for RAM : 0.010190 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:23:46] Energy consumed for all GPUs : 0.073118 kWh. Total GPU Power : 354.10610867794884 W\n",
      "[codecarbon INFO @ 18:23:46] Energy consumed for all CPUs : 0.011802 kWh. Total CPU Power : 55.221197555248665 W\n",
      "[codecarbon INFO @ 18:23:46] 0.095109 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:24:01] Energy consumed for RAM : 0.010386 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:24:01] Energy consumed for all GPUs : 0.074576 kWh. Total GPU Power : 350.2129312858079 W\n",
      "[codecarbon INFO @ 18:24:01] Energy consumed for all CPUs : 0.012031 kWh. Total CPU Power : 55.174308134725834 W\n",
      "[codecarbon INFO @ 18:24:01] 0.096994 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:24:16] Energy consumed for RAM : 0.010582 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:24:16] Energy consumed for all GPUs : 0.076022 kWh. Total GPU Power : 347.1226056346158 W\n",
      "[codecarbon INFO @ 18:24:16] Energy consumed for all CPUs : 0.012262 kWh. Total CPU Power : 55.43961510832372 W\n",
      "[codecarbon INFO @ 18:24:16] 0.098866 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:24:31] Energy consumed for RAM : 0.010778 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:24:31] Energy consumed for all GPUs : 0.077460 kWh. Total GPU Power : 345.3101279002553 W\n",
      "[codecarbon INFO @ 18:24:31] Energy consumed for all CPUs : 0.012493 kWh. Total CPU Power : 55.32651827946028 W\n",
      "[codecarbon INFO @ 18:24:31] 0.100731 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-3B-fine-tuned/checkpoint-750\n",
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-3B-fine-tuned/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-3B-fine-tuned/checkpoint-750/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[codecarbon INFO @ 18:24:37] Energy consumed for RAM : 0.010860 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 18:24:37] Energy consumed for all GPUs : 0.077982 kWh. Total GPU Power : 298.0011292391246 W\n",
      "[codecarbon INFO @ 18:24:37] Energy consumed for all CPUs : 0.012581 kWh. Total CPU Power : 50.419400516834216 W\n",
      "[codecarbon INFO @ 18:24:37] 0.101424 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-3B-fine-tuned\n",
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-3B-fine-tuned/tokenizer_config.json\n",
      "Special tokens file saved in /home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-3B-fine-tuned/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "print(torch.cuda.current_device())\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "response_template = \"assistant<|end_header_id|>\"\n",
    "instruction_template = \"user<|end_header_id|>\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "        # Round the values for better readability\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset= dataset_chatml,\n",
    "        eval_dataset=val_dataset_chatml,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=512,\n",
    "#         compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        args=args,\n",
    "        data_collator=collator,\n",
    "\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# 'trainer.save_model()' is a method that saves the trained model locally.\n",
    "# The model will be saved in the directory specified by 'output_dir' in the training arguments.\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "CUDA backend validation successful.\n",
      "loading weights file model.safetensors from cache at /home/sohampoddar/HDD2/hfcache/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "CUDA backend validation successful.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.28s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "loading file tokenizer.json\n",
      "loading file tokenizer.model\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "model_path = \"/home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-3B-fine-tuned\"\n",
    "tokenizer_path = \"/home/sohampoddar/HDD2/utsav/Cross_Experiment_2/Models/llama-3.2-3B-fine-tuned\"\n",
    "# connect to device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # 'cuda' will point to the visible GPU (e.g., GPU 0 as set by CUDA_VISIBLE_DEVICES)\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=compute_dtype, trust_remote_code=True, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from rouge_score) (1.25.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.6)\n",
      "Requirement already satisfied: joblib in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from nltk->rouge_score) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from nltk->rouge_score) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from nltk->rouge_score) (4.65.0)\n",
      "\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 19:38:46] Energy consumed for RAM : 0.021360 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 19:38:46] Energy consumed for all GPUs : 0.086135 kWh. Total GPU Power : 109.25316238443722 W\n",
      "[codecarbon INFO @ 19:38:46] Energy consumed for all CPUs : 0.019200 kWh. Total CPU Power : 32.64548429808942 W\n",
      "[codecarbon INFO @ 19:38:46] 0.126695 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from evaluate import load  # Correct import for metrics\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 987/987 [00:00<00:00, 5458.78 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'text', 'labels', 'counter_argument', 'messages'],\n",
       "        num_rows: 740\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ID', 'text', 'labels', 'counter_argument', 'messages'],\n",
       "        num_rows: 247\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset_chatml=val_data.map(create_message_column_with_labels)\n",
    "val_dataset_chatml=val_dataset_chatml.map(format_dataset_chatml)\n",
    "dataset_chatml = val_dataset_chatml.train_test_split(test_size=0.25)\n",
    "dataset_chatml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=100, do_sample=True, num_beams=1, temperature=0.7, top_k=50, top_p=0.9,repetition_penalty=1.2, max_time= 50)\n",
    "    # print(outputs[0]['generated_text'])\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': \"['country', 'rushed']\",\n",
       " 'text': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 15 Dec 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nGenerate Counter Argument for the anti-vaccine tweet:\\n Tweet: Generate a brief counter argument for the anti-vaccine tweet:\\nsomething's fishy with his insistence . i think that vaccine is still on clinical trial and whoever takes that will be sort of a guinea pig . nobody trusts chinese vaccines . i trust the UK vaccine which is still in development . unahin nya sarili nya magvaccine sa september !\\nTalk About national biases and objections to vaccines produced by specific countries and claims that vaccines were approved or developed without sufficient testing\\n Talk About  and  ##Output:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nWhile it's understandable to have hesitations about vaccines from certain countries due to national biases, it's crucial to rely on rigorous scientific evaluations rather than speculative assumptions. Vaccines, including those produced by Chinese manufacturers, undergo extensive clinical trials and scrutiny by global health organizations, ensuring their safety and efficacy. Claims suggesting that specific vaccines are still in clinical trials often stem from misunderstandings about their approval processes and post-marketing surveillance. Furthermore, the UK vaccine, while trusted, also faced its own controversies during development. Encouraging vaccinations based on national origin rather than data undermines the collaborative international effort to combat public health crises. Itâ€™s essential to base decisions on comprehensive evidence rather than national biases or unfounded fears.<|eot_id|>\",\n",
       " 'counter_argument': \"While it's understandable to have hesitations about vaccines from certain countries due to national biases, it's crucial to rely on rigorous scientific evaluations rather than speculative assumptions. Vaccines, including those produced by Chinese manufacturers, undergo extensive clinical trials and scrutiny by global health organizations, ensuring their safety and efficacy. Claims suggesting that specific vaccines are still in clinical trials often stem from misunderstandings about their approval processes and post-marketing surveillance. Furthermore, the UK vaccine, while trusted, also faced its own controversies during development. Encouraging vaccinations based on national origin rather than data undermines the collaborative international effort to combat public health crises. Itâ€™s essential to base decisions on comprehensive evidence rather than national biases or unfounded fears.\",\n",
       " 'messages': [{'content': \"Generate Counter Argument for the anti-vaccine tweet:\\n Tweet: Generate a brief counter argument for the anti-vaccine tweet:\\nsomething's fishy with his insistence . i think that vaccine is still on clinical trial and whoever takes that will be sort of a guinea pig . nobody trusts chinese vaccines . i trust the UK vaccine which is still in development . unahin nya sarili nya magvaccine sa september !\\nTalk About national biases and objections to vaccines produced by specific countries and claims that vaccines were approved or developed without sufficient testing\\n Talk About  and  ##Output: \",\n",
       "   'role': 'user'},\n",
       "  {'content': \"While it's understandable to have hesitations about vaccines from certain countries due to national biases, it's crucial to rely on rigorous scientific evaluations rather than speculative assumptions. Vaccines, including those produced by Chinese manufacturers, undergo extensive clinical trials and scrutiny by global health organizations, ensuring their safety and efficacy. Claims suggesting that specific vaccines are still in clinical trials often stem from misunderstandings about their approval processes and post-marketing surveillance. Furthermore, the UK vaccine, while trusted, also faced its own controversies during development. Encouraging vaccinations based on national origin rather than data undermines the collaborative international effort to combat public health crises. Itâ€™s essential to base decisions on comprehensive evidence rather than national biases or unfounded fears.\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_chatml['test'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'While itâ€™s important to respect individual choices, public health measures like vaccinations serve not just personal interests but collective safety. Vaccines have been proven to reduce transmission rates of diseases, protecting vulnerable populations who cannot be vaccinated due to medical reasons or allergies. Mandates exist not only because governments want to safeguard citizens but also as a means to achieve herd immunity, which ultimately benefits everyone. Balancing personal freedom with community responsibility is crucial; opting out could lead to prolonged lockdowns, economic hardship, and increased healthcare'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inference(dataset_chatml['test'][1]['messages'][0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rogue(row):\n",
    "    response = test_inference(row['messages'][0]['content'])\n",
    "    result = metric.compute(predictions=[response], references=[row['output']], use_stemmer=True)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result['response']=response\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1: While it's understandable to have concerns about vaccines, it's important to consider that millions of people around the world have already received their COVID-19 shots without any major issues. Trusting in the science and data behind the vaccine can help protect yourself and others from a potentially deadly virus. #TrustTheScience #VaccinesSaveLives\n",
      "Reference 1: It's important to remember that vaccines undergo rigorous testing and approval processes before being distributed to the public. Trusting in the science behind the vaccine can help protect not only yourself, but also those around you.\n",
      "---\n",
      "Prediction 2: While it is understandable that someone may be hesitant about getting vaccinated, refusing to get the COVID-19 vaccine can have serious consequences not only for yourself but also for those around you. It is important to consider the well-being of others and make decisions based on public health guidelines rather than personal preference alone. Additionally, being informed about any potential risks or side effects before making an decision is crucial in order to protect your own health as well as the community at large. Refusing the vaccine without proper justification could\n",
      "Reference 2: While there may be some exemptions for medical or religious reasons, it is important to consider the greater good of public health. Avoiding the vaccine could put not only yourself at risk, but also those around you. It is important to follow public health guidelines and do your part to help stop the spread of COVID-19.\n",
      "---\n",
      "Prediction 3: While it may be concerning that AstraZeneca's trial results showed only 50% efficacy, it is important to remember that any level of protection against COVID-19 is better than none. Additionally, releasing trial blueprints shows transparency and accountability on the part of the company. It is essential to consider all factors before jumping to conclusions about the safety and effectiveness of the vaccine. Trusting science and following public health guidelines should take precedence over skepticism based solely on one metric. #TrustTheScience #\n",
      "Reference 3: It is important to remember that the AstraZeneca vaccine still provides significant protection against COVID-19, and any vaccine with even 50% efficacy can make a major impact in reducing the spread of the virus and saving lives. It is also essential to have multiple vaccine options in order to reach as many people as possible and achieve herd immunity.\n",
      "---\n",
      "Prediction 4: Just because Pfizer has made profits from past events does not mean their new COVID-19 vaccine should be dismissed. The development process is rigorous and based on scientific evidence, unlike conspiracy theories surrounding Illuminati ownership or sinister intentions. Trust in medical professionals and research rather than unfounded claims. #TrustScience #PublicHealthFirst</s> #VaccinesSaveLives</s> #COVID19PreventionIsKey</s> <s>This post was generated using AI technology.</s></s>\n",
      "Reference 4: While Pfizer may have made profits from their products in the past, it is important to focus on the potential benefits of the Covid vaccine they are developing. The efficacy of the vaccine should be the main priority, rather than focusing on conspiracy theories or past actions of the company.\n",
      "---\n",
      "Prediction 5: It is important to remember that correlation does not always equal causation. It is possible that your symptoms could have been caused by other factors unrelated to the vaccine, such as allergies or environmental exposures. Additionally, it's worth noting that the development of a new condition after receiving a vaccination can occur due to various reasons beyond just the vaccine itself. Further investigation would be necessary to determine the actual cause of your eczema symptoms. #VaccinesSaveLives #TrustInScience.\n",
      "Reference 5: It is unlikely that the yellow fever vaccine caused the eczema, as eczema is a common skin condition that can be triggered by a variety of factors such as stress, allergies, and genetics. It is important to consult with a healthcare professional to determine the true cause of the eczema.\n",
      "---\n",
      "Prediction 6: It's important to consider that no medical treatment comes without risks. The benefits of widespread vaccination far outweigh any potential negative side effects, especially when it comes to protecting public health during a global pandemic like COVID-19. It's irresponsible to spread unfounded claims about the safety of vaccines based on isolated incidents or biased reporting. Vaccines have been proven time and again to be crucial tools in controlling infectious diseases and saving lives. #TrustInVaccines #PublicHealthFirst</s> While it's\n",
      "Reference 6: It is important to consider the source of this information and whether it is based on credible sources. Bill Gates has been a strong advocate for global health and has dedicated a significant amount of resources to vaccination efforts. It is unlikely that he would be apathetic towards widespread vaccine side effects. Further investigation and fact-checking may be necessary before jumping to conclusions.\n",
      "---\n",
      "Prediction 7: While it's true that no vaccine completely prevents transmission, they are proven to reduce severe illness and hospitalization. The benefits of vaccination far outweigh the risks of rare side effects, especially when considering the greater public health impact on preventing the spread of the virus. Additionally, there have been numerous studies showing the safety and efficacy of various vaccines over time. It's important to weigh the scientific consensus against individual concerns about specific conditions like infertility. #VaccinesSaveLives #PublicHealthMatters</s\n",
      "Reference 7: While it is true that the vaccine may not completely stop transmission of the virus, it has been shown to significantly reduce the severity of illness and hospitalization. Additionally, the potential side effects, such as infertility, are extremely rare and the benefits of widespread vaccination far outweigh the risks. It is important to trust in the expertise of medical professionals and scientists who have extensively studied and developed these vaccines.\n",
      "---\n",
      "Prediction 8: Actually, vaccines have been proven to significantly reduce the spread of diseases and protect individuals from severe illness. It is important to trust in scientific research and public health measures rather than spreading misinformation. #TrustTheScience.\n",
      "Reference 8: Actually, vaccines have historically played a significant role in reducing or eliminating the spread of diseases and improving public health. It is ignorant to dismiss the potential positive impacts of widespread vaccination.\n",
      "---\n",
      "Prediction 9: It's important to remember that vaccines are not just about changing the course of illness, but also preventing transmission. Variants may arise regardless of vaccination rates, so it's crucial to continue following public health guidelines such as wearing masks and staying home if symptoms appear. The goal is to protect both individuals and communities from further spread of disease. Additionally, large gatherings like sporting events can contribute to outbreaks by bringing together groups with varying levels of immunity. It's essential to consider the bigger picture rather than focusing solely\n",
      "Reference 9: While it is frustrating that the situation with COVID-19 seems to be constantly changing, it is important to remember that vaccines are not a cure-all and it takes time for their effectiveness to be fully understood. Additionally, just because there are inconsistencies and challenges with vaccination efforts in one country, such as India, does not mean that individuals in other countries should stop following public health guidelines and precautions. It is crucial to continue to prioritize safety and follow recommendations from experts to prevent further spread of the virus.\n",
      "---\n",
      "Prediction 10: While it's true that the vaccine does show a high level of efficacy, it's important to consider other factors such as long-term protection, potential side effects, and individual differences in immune response. Additionally, public health measures like mask-wearing and social distancing still play crucial roles in controlling the spread of disease even with vaccination efforts underway. Ultimately, promoting widespread vaccination helps protect those who may be unable or unwilling to get vaccinated themselves. It's about creating herd immunity and keeping our community safe collectively. #\n",
      "Reference 10: While the vaccine may have a high efficacy and some decrease in transmission, it is still important to continue following public health guidelines and precautionary measures. Just because someone is vaccinated does not mean they are completely immune or unable to spread the virus. It is important to consider the health and safety of others in addition to personal risk tolerance.\n",
      "---\n",
      "Prediction 11: While it is true that individuals receiving the vaccine may still contract and potentially spread COVID-19, this does not necessarily mean they cannot hold the companies responsible. The vaccine manufacturers could face legal consequences even in cases where someone dies after receiving the vaccine, especially if there was negligence involved during production or distribution. Additionally, relying solely on immunity from lawsuits to ensure widespread vaccination overlooks the importance of accountability for pharmaceutical companies and their actions. Holding these companies accountable helps prevent similar situations in the future by incentivizing thorough\n",
      "Reference 11: Counter argument: While there may be risks associated with taking the vaccine, it is important to consider the potential benefits of reducing the spread of Covid-19 and ultimately saving lives. The vaccine has gone through rigorous testing and approval processes to ensure its safety and effectiveness. Additionally, holding companies accountable for any negative side effects could deter them from developing life-saving vaccines in the future.\n",
      "---\n",
      "Prediction 12: While there may be factors contributing to low vaccination rates, placing all responsibility solely on government policies overlooks individual choices and personal autonomy. People have different reasons for choosing not to receive vaccines, whether due to medical concerns or simply mistrust in authority figures. Blaming individuals alone oversimplifies complex issues surrounding public health measures. #PersonalResponsibilityInVaccination.\n",
      "Reference 12: While it is important to understand the reasons why some individuals may be hesitant to get the vaccine, ultimately individual choice plays a role in public health outcomes. Blaming the government for individuals choosing not to get vaccinated overlooks personal responsibility and the impact of misinformation or personal beliefs on vaccination rates.\n",
      "---\n",
      "Prediction 13: Actually, all three types of vaccines have undergone rigorous testing before being authorized for emergency use. The mRNA vaccines may have less data on long-term effects compared to traditional vaccines, but they still follow strict guidelines and protocols set by health authorities worldwide. It is important to trust scientific research and recommendations from experts when considering vaccination options. Additionally, getting vaccinated can help protect yourself and others from the spread of COVID-19, regardless of your personal travel plans. Rushing to judgment on the type of vaccine based solely\n",
      "Reference 13: While it is true that the Oxford vaccine uses more traditional technology, mRNA vaccines have actually undergone rigorous testing and have been shown to be safe and effective in clinical trials. It is important to base vaccination decisions on scientific evidence rather than personal fears or preferences. By choosing not to get vaccinated, you are not only putting yourself at risk, but also potentially spreading the virus to others and hindering efforts to control the pandemic.\n",
      "---\n",
      "Prediction 14: While it's important to be critical of government actions during times of crisis, spreading baseless conspiracy theories without evidence only perpetuates misinformation and fear. It's crucial to rely on credible sources and scientific research when assessing the impact of global events like the pandemic. Believing in such extreme scenarios only hinders progress towards recovery and understanding of public health measures. #factcheckingmatters #skepticalthinking.\n",
      "Reference 14: This tweet promotes a dangerous conspiracy theory with no evidence to support it. The idea that the COVID-19 pandemic was intentionally planned to destroy Western economies and implement police states is not only unfounded but also harmful misinformation. It is important to rely on credible sources and scientific evidence when discussing public health crises.\n",
      "---\n",
      "Prediction 15: It is important to remember that having antibodies from previous infection or vaccination does provide some level of immunity, even if it may not be as strong as natural immunity. Additionally, getting vaccinated with a COVID-19 vaccine can help boost overall immune system function and increase protection against the virus long-term. Calling this situation \"Chinese Three Card Monte\" undermines the importance of scientific research and public health measures aimed at controlling the spread of the pandemic. It is crucial to trust expert advice and prioritize protecting oneself and others through\n",
      "Reference 15: It is inaccurate and irresponsible to label a potential vaccine as a Democrat conspiracy based on unfounded claims. The development of a vaccine is based on scientific research and testing, not political affiliations. Suggesting otherwise only fuels misinformation and undermines the efforts of healthcare professionals working to combat the virus.\n",
      "---\n",
      "Prediction 16: While it's concerning that one individual experienced a serious allergic reaction to the Pfizer COVID-19 vaccine, it's important to remember that these types of incidents are extremely rare. The benefits of widespread vaccination far outweigh the risks of potentially life-threatening allergic reactions. It's crucial to consider the overall effectiveness and safety of vaccines when weighing their advantages over other preventive measures. Additionally, healthcare workers who receive vaccinations help protect not only themselves but also vulnerable populations within the community they serve. Monitoring cases like this helps improve understanding\n",
      "Reference 16: While it is concerning that a health worker in Alaska had a serious allergic reaction to the Pfizer vaccine, it is important to remember that allergic reactions to vaccines are extremely rare. The benefits of the vaccine, which can protect individuals from a potentially deadly virus, far outweigh the risks of a reaction. It is crucial to continue monitoring and investigating any potential safety implications, but this isolated incident should not deter people from getting vaccinated.\n",
      "---\n",
      "Prediction 17: While it is tragic that this occurred, correlation does not necessarily imply causation. It is important to consider other factors such as underlying health conditions or accidents before jumping to conclusions about the vaccine being directly responsible for the death of the young man. Vaccines have been proven safe and effective by numerous scientific studies and should be seen as a vital tool in preventing the spread of deadly diseases. Making false claims without evidence only serves to sow fear and misinformation among the public. #VaccinationSavesLives\n",
      "Reference 17: It is tragic that this father lost his son after getting vaccinated, but it is important to remember that the overwhelming majority of people who receive the Pfizer vaccine do not experience severe side effects. The benefits of vaccination in protecting against COVID-19 far outweigh the risks, and incidents like this are extremely rare. It is crucial to trust in the scientific consensus and data supporting the safety and efficacy of these vaccines.\n",
      "---\n",
      "Prediction 18: While it is understandable to be cautious, the overwhelming majority of experts agree that getting vaccinated greatly reduces the likelihood of severe illness from COVID-19. The benefits of protecting yourself and others by getting vaccinated far outweigh any theoretical risks associated with receiving the shot. It's important to consider not only your own health but also the impact on public health as a whole. Trust in scientific research and data when making this decision! #GetVaccinated #ProtectYourselfAndOthers\n",
      "Reference 18: It's important to consider the fact that the long-term effects of COVID-19 are still largely unknown, and the risks of contracting the virus could potentially be more harmful than any potential side effects of the vaccine.\n",
      "---\n",
      "Prediction 19: While it is true that there have been cases of neurological issues in healthy children following vaccination, it is important to consider that these instances are extremely rare compared to the overall benefits of widespread vaccination. Additionally, the flu vaccine does not guarantee complete immunity against the flu virus, as no vaccine can provide 100% protection. It is also worth noting that individuals may still experience symptoms from past vaccinations or diseases even if they receive them, highlighting the complexities involved in understanding immune responses to various stimuli. Ultimately, weighing\n",
      "Reference 19: While there may be cases of adverse reactions to vaccines, the vast majority of people who receive vaccines do not experience any negative effects. The benefits of vaccinations far outweigh the risks, as they help protect individuals and communities from serious and potentially deadly diseases. It is important to consider the overall public health impact of vaccines, rather than focusing solely on isolated incidents of adverse reactions.\n",
      "---\n",
      "Prediction 20: While it's true that there are risks involved in any activity, comparing the risk of contracting COVID-19 to other illnesses like car accidents or cancer may not be entirely accurate. Each situation has its own unique factors and consequences. It's important to acknowledge the severity of the pandemic and take necessary precautions rather than dismissing all risks as being equal. #StaySafe #ProtectYourselfAndOthers\n",
      "Reference 20: While it is true that everyone is at risk of dying in a car crash or getting cancer, those are risks that individuals have some control over through measures such as wearing a seatbelt or maintaining a healthy lifestyle. The risk of death from a vaccine-preventable disease can be significantly reduced by getting vaccinated, making it a more preventable and controllable risk compared to car accidents or cancer.\n",
      "---\n",
      "Prediction 21: Actually, Sinopharm's COVID-19 vaccine was approved by China's National Medical Products Administration earlier this year. It may not have been widely adopted by other countries initially due to regulatory differences or concerns about its safety, but it still showed promising results in clinical trials. Additionally, securing an order from many countries does not necessarily mean they will ultimately reject the vaccine if it proves effective in preventing severe illness and death. It's important to consider all factors before making judgments based on limited information. #Sin\n",
      "Reference 21: While Sinovac may have lower efficacy rates compared to other vaccines, it is still approved by several countries and provides some level of protection against COVID-19. Additionally, efficacy rates can vary depending on factors like age and underlying health conditions, so it is important to consider the overall benefits and risks of each vaccine option.\n",
      "---\n",
      "Prediction 22: While it's true that getting a new drug onto the market typically requires extensive testing, the urgency of the current global pandemic has necessitated expedited approval processes. The AstraZeneca vaccine still underwent rigorous safety assessments before being granted emergency use authorization, ensuring its effectiveness in preventing severe illness from COVID-19. Additionally, accelerating technological advancements allows researchers to adapt their methods while maintaining strict standards for safety and efficacy. Ultimately, prioritizing public health during this crisis demands flexibility in regulatory approaches to save lives quickly\n",
      "Reference 22: While it is true that thorough testing is important for ensuring the safety of a vaccine, the urgency of the current global pandemic necessitates expedited processes. The FDA and other regulatory bodies have established strict protocols for emergency use authorization, allowing for a vaccine to be deemed safe and effective even if it has not undergone years of testing. Additionally, advancements in technology and research have enabled scientists to monitor vaccine safety and efficacy more efficiently than ever before. Ultimately, the risk of not having a vaccine far outweighs the potential risks associated with expediting the approval process.\n",
      "---\n",
      "Prediction 23: It's important to note that while there may have been some initial limitations in coverage against certain variants, ongoing research and updates from health authorities suggest that moderna's vaccine still offers significant protection. Additionally, getting a booster shot can help ensure continued immunity against evolving strains of the virus. It's not necessarily accurate to label someone as an \"asshole\" based on one statement without considering all available information. #factcheckingmatters #publichealthfirst.\n",
      "Reference 23: Counter argument: It is unfair to call the Moderna president an \"asshat\" just because a booster shot may be necessary to combat new variants. Vaccines often need to be adjusted to address changing virus strains, and it is responsible for companies to be proactive in researching and providing solutions.\n",
      "---\n",
      "Prediction 24: This type of fear-mongering is dangerous and irresponsible. The chances of being vaccinated and ending up as one of only two survivors in such extreme scenarios are extremely low, so spreading baseless conspiracy theories like this does more harm than good. It's important to focus on evidence-based information rather than sensationalized claims. #ScienceOverFear #PublicHealthMatters.\n",
      "Reference 24: While the tweet raises concerns about potential dangers of vaccines, it is important to consider the overwhelming scientific consensus that vaccines are safe and effective at preventing the spread of diseases. Fear-mongering about vaccines only serves to perpetuate misinformation and put public health at risk.\n",
      "---\n",
      "Prediction 25: It is important to consider other underlying health conditions or factors at play when attributing death solely to COVID-19. Additionally, correlation does not equal causation; just because two events occur in close proximity does not mean one causes the other. The reporting system may simply be more rigorous when dealing with cases where a clear link between vaccination and death can be established. It is crucial to rely on scientific evidence and expert analysis rather than jumping to conclusions based on superficial observations. #TrustTheScience.\n",
      "Reference 25: Just because two things happen around the same time does not mean they are causally linked. It is important to rely on scientific evidence and data when making assumptions about the relationship between Covid-19 and vaccinations.\n",
      "---\n",
      "Prediction 26: It's not necessarily accurate to label someone as an \"agent\" based solely on their promotion of a vaccine. Promoting public health measures, including vaccines, can be seen as responsible and caring rather than nefarious. It's important to consider context and motivations behind actions before jumping to conclusions about individuals' intentions. #VaccinesSaveLives #PublicHealthMatters.\n",
      "Reference 26: It is illogical to equate promoting the vaccine with being an agent of the Beast. Promoting public health and safety should not be demonized or politicized.\n",
      "---\n",
      "Prediction 27: While it's true that some vaccines use aborted fetal tissue or may have ethical implications, the benefits of vaccination far outweigh these concerns. Vaccines help protect individuals and communities from deadly diseases, ultimately saving lives and preventing outbreaks. It's important to consider the greater good when evaluating the safety and efficacy of vaccines like those developed by Oxford University. Making baseless accusations without proper evidence only serves to spread misinformation and hinder efforts towards public health. #VaccinesSaveLives #TrustTheScience.\n",
      "Reference 27: Actually, vaccines do not contain aborted baby cells. The use of cell cultures in vaccine production is a common and safe practice that has been approved by regulatory bodies. Additionally, vaccines do not change a person's DNA as they work by stimulating the immune system to create antibodies. It is important to fact-check before spreading misinformation about vaccines.\n",
      "---\n",
      "Prediction 28: Actually, Moderna's COVID-19 vaccine went through rigorous testing and was approved by regulatory agencies before being distributed. The NIH played a role in funding research on vaccines during the pandemic but did not directly develop the Moderna vaccine itself. Accusing it of being an NIH vaccine oversimplifies complex scientific processes. #factchecking matters in public health discourse.\n",
      "Reference 28: Actually, Moderna is a biotechnology company that developed the vaccine with funding from the NIH and NIAID. It is a collaborative effort between private industry and government agencies.\n",
      "---\n",
      "Prediction 29: The government shutting down a tracking system is actually meant to protect individuals' privacy, rather than hiding information about rare but serious adverse events. It's important to prioritize transparency in medical research so we can learn from any negative outcomes and improve future vaccination efforts. Trusting scientific evidence over conspiracy theories will ultimately lead to better health decisions. #TrustScience #PublicHealthMatters.\n",
      "Reference 29: Just because the federal government has shut down a system tracking vaccine injury and death does not necessarily mean they are hiding information or trying to discourage people from getting vaccinated. It could be for a variety of reasons such as updating the system or transferring data to a more efficient platform. It is important to consider all possibilities before jumping to conclusions.\n",
      "---\n",
      "Prediction 30: It is important to remember that millions of people have already received the AstraZeneca vaccine without any serious complications. The benefits of widespread vaccination far outweigh the risks, as it helps protect not only individuals but also communities from the spread of COVID-19. It is crucial to trust in the rigorous scientific process behind vaccine trials and continue to prioritize public safety during this global pandemic. #TrustTheScience #PublicHealthFirst.\n",
      "Reference 30: While it is unfortunate that a volunteer in the COVID vaccine trial has died, it is important to remember that adverse reactions can occur with any medication or vaccine. The fact that testing is continuing shows that thorough safety protocols are in place to monitor and assess any potential risks. It is crucial to continue with vaccine trials in order to ensure the safety and efficacy of a potential vaccine for the greater population.\n",
      "---\n",
      "Prediction 31: While it is true that these companies have been granted legal protection against liability in case of any unforeseen negative consequences, this does not necessarily mean they will be completely immune to accountability. There could still be regulatory oversight and public scrutiny surrounding their actions, even if legally protected. Additionally, granting immunity may discourage them from prioritizing safety above profits, ultimately benefiting public health as a whole. It's important to consider both short-term gains and long-term implications when evaluating vaccine development processes. #PublicHealthFirst.\n",
      "Reference 31: While it is important to protect vaccine manufacturers from potential liabilities, it is also crucial to ensure that individuals who may experience adverse effects from the vaccines are properly compensated and taken care of. Holding manufacturers accountable for any side effects can help ensure that they prioritize safety and effectiveness in their product development.\n",
      "---\n",
      "Prediction 32: While alternative candidates may have some advantages, such as eliciting a mucosal response, they are not necessarily safer or more effective than established options like Pfizer. Additionally, relying solely on natural immunity through antibodies from previous infections can be unpredictable and potentially dangerous. Boosters are crucial in maintaining protection against new variants and ensuring overall public health. Wearing masks also plays a significant role in reducing transmission rates. Trusting in scientific research and expert recommendations is important in combating the spread of COVID-19 effectively. #\n",
      "Reference 32: While it is true that Pfizer's vaccine may not provide a mucosal response like some other candidates, it is still important to consider the overall effectiveness and safety of the vaccine. Pfizer's vaccine has been rigorously tested and shown to be highly effective in preventing severe illness and reducing transmission of COVID-19. Additionally, wearing a mask and practicing other preventive measures can further reduce the risk of infection, making the Pfizer vaccine a valuable tool in combating the pandemic.\n",
      "---\n",
      "Prediction 33: Using sharks as a means of creating a vaccine is not only unethical, but it also goes against scientific principles. Vaccines are made by scientists who have dedicated their lives to understanding and combating diseases, not by exploiting animals like sharks. It's important to trust experts in healthcare rather than spreading baseless conspiracy theories. #TrustScience #VaccinationIsKey #ProtectPublicHealth.\n",
      "Reference 33: Using sharks in medical research, including for the creation of vaccines, can help save human lives. It is important to prioritize finding a solution to the global pandemic over concerns about animal welfare.\n",
      "---\n",
      "Prediction 34: While it is important to consider all possible outcomes, including those related to vaccinations, it's crucial to remember that severe complications from COVID-19 far outweigh any risks of rare adverse events from receiving a vaccine. Vaccines have been proven to significantly reduce hospitalizations and deaths from the virus, ultimately saving more lives in the long run than potentially causing harm through rare occurrences. It's essential to weigh the benefits against the risks when making decisions about vaccination. #TrustTheScience #GetVaccinatedToProtect\n",
      "Reference 34: It is important to remember that the vast majority of people who receive the Covid vaccine do not experience severe complications or death. The benefits of the vaccine in preventing illness and stopping the spread of the virus far outweigh the risks. Additionally, medical professionals carefully track any adverse reactions to vaccines to ensure safety. It is crucial to trust in the science behind vaccines and prioritize public health.\n",
      "---\n",
      "Prediction 35: There is no scientific evidence to support this claim that the COVID-19 vaccine increases susceptibility to the virus. The vaccines have been rigorously tested and proven to be safe and effective in preventing severe illness, hospitalization, and death from COVID-19. It's important to rely on credible sources of information rather than spreading misinformation. #TrustTheScience #VaccinesSaveLives.\n",
      "Reference 35: This tweet is spreading misinformation and fearmongering. Vaccines have been proven to be effective in preventing the spread and severity of viruses, including COVID-19. It is important to trust in science and listen to credible sources for information on vaccines.\n",
      "---\n",
      "Prediction 36: While it is true that washing our hands helps prevent the spread of many illnesses, vaccines are still necessary because they provide long-term immunity against specific diseases. Just because someone may not contract an illness like smallpox does not mean that others who have been exposed or at risk cannot benefit from vaccination. Vaccines help protect those who may be more vulnerable to serious complications if infected with these diseases. #GetVaccinated #PublicHealthMatters\n",
      "Reference 36: Actually, the reason we no longer see smallpox in the environment is precisely because of widespread vaccination efforts. Without vaccines, the disease would still be a major threat to public health. It's important to continue vaccinating to prevent the resurgence of deadly diseases like smallpox.\n",
      "---\n",
      "Prediction 37: While it's true that some individuals may have had pre-existing conditions that contributed to their deaths, it's important to acknowledge that contracting COVID-19 can still pose significant risks even if someone has other health concerns. Additionally, while there are valid concerns surrounding certain side effects from specific vaccines like AstraZeneca, overall evidence suggests that these benefits far outweigh the risks in terms of protecting against severe illness or death from COVID-19. It's crucial to continue promoting vaccination efforts as a means of controlling the\n",
      "Reference 37: Even if the individuals had underlying health issues, it is important to recognize that contracting COVID-19 likely exacerbated their conditions and ultimately led to their deaths. Downplaying the significance of COVID-19 in these cases undermines the seriousness of the virus and the need for continued caution and preventative measures. Additionally, it is important to consider that correlation does not always equal causation when it comes to vaccine side effects, and the benefits of vaccination often outweigh the potential risks.\n",
      "---\n",
      "Prediction 38: While it is true that the development process may seem swift, it's important to remember that years of research and testing are still required before a vaccine can be deemed safe and effective. It's better to trust in scientific expertise rather than dismissing progress just because it seems rapid. Additionally, comparing the situation to \"Gander\" or \"goose\" could be seen as belittling and dismissive. Let's focus on acknowledging the hard work and dedication of scientists involved in creating a potential solution to the\n",
      "Reference 38: It's important to recognize the significant scientific progress and collaboration that led to the development of the Pfizer and BioNTech vaccine. Dismissing their achievements as merely a \"gander\" undermines the hard work and dedication of countless researchers and scientists.\n",
      "---\n",
      "Prediction 39: Actually, mRNA technology has been shown to be effective in multiple clinical trials and has undergone rigorous testing before being approved by regulatory agencies. Calling it \"not a vaccine\" is inaccurate based on scientific evidence. Additionally, comparing COVID-19 treatment options should focus on their proven efficacy rather than making baseless accusations about animal studies. #TrustTheScience #COVID19TreatmentOptionsMatter\n",
      "Reference 39: Actually, mRNA vaccines have proven to be highly effective and safe in preventing severe illness and death from COVID-19. It is based on cutting-edge technology and has undergone rigorous testing before being approved for emergency use. It is not accurate to dismiss it as just \"more humanize mice.\"\n",
      "---\n",
      "Prediction 40: While it is important to prioritize public health and safety, punishing individuals who choose not to take the vaccine by imposing death row could set a dangerous precedent. It undermines individual autonomy and freedom of choice, potentially driving more people away from getting vaccinated altogether. Additionally, this approach does not address underlying issues or concerns about the vaccine itself that might be influencing someone's decision not to get it. A more effective solution would involve addressing these concerns through education, transparency, and accessible information rather than resorting to extreme measures\n",
      "Reference 40: Taking the vaccine is a personal choice, but advocating for the punishment of those who choose not to take it goes against individual freedoms and medical autonomy. Instead of seeking extreme penalties, efforts should be focused on educating and encouraging vaccination.\n",
      "---\n",
      "Prediction 41: While it's understandable to have concerns about the safety of a new vaccine, it's important to remember that rigorous testing and regulatory processes are in place to ensure its safety. Trusting the expertise of scientists and health authorities is crucial in combating the spread of infectious diseases. Ministers must prioritize public health over personal anxieties when making decisions on vaccinations. #TrustTheScience #PublicHealthFirst.\n",
      "Reference 41: It is not uncommon for pharmaceutical companies to conduct rigorous testing and research before releasing a vaccine to the public. Additionally, government regulatory bodies also conduct their own assessments to ensure the safety and efficacy of the vaccine. It is likely that Pfizer's vaccine has been thoroughly vetted by multiple sources before being deemed safe for distribution.\n",
      "---\n",
      "Prediction 42: It is important to separate individuals from their actions. Just because someone may have connections with an organization does not automatically mean they support harmful goals such as population control. The development of a vaccine during a global pandemic should be seen as a positive achievement, regardless of where the funding came from. It is crucial to focus on the scientific advancements being made rather than making assumptions based on personal relationships. #VaccinesSaveLives #ScienceOverSpeculation\n",
      "Reference 42: While it is true that Bill Gates has provided funding to the Oxford lab, it does not mean that the vaccine being developed is part of a sinister plot to depopulate the world. The vaccine development process is rigorous and involves many scientific experts, not just Bill Gates. Additionally, vaccines go through strict testing and approval processes to ensure their safety and effectiveness before being distributed to the public. The Oxford vaccine is a result of hard work and collaboration from a team of dedicated researchers, not a covert plan for population control.\n",
      "---\n",
      "Prediction 43: The COVID-19 pandemic is a global health crisis, not a natural occurrence like other diseases have been in the past. The urgency behind developing a vaccine quickly was necessary due to the rapid spread of the virus and its potential devastating impact on public health. Additionally, advancements in technology and scientific research during the current era may allow for quicker development processes compared to previous vaccinations. While there will always be some level of risk associated with any medical intervention, taking the vaccine can help prevent overwhelming healthcare systems and ultimately save\n",
      "Reference 43: While it is true that the COVID-19 vaccine was developed much faster than previous vaccines, it's important to consider the urgency of the situation. The global pandemic has caused immense suffering and loss of life, so a rapid response was necessary. Additionally, the vaccine underwent rigorous testing and clinical trials to ensure its safety and efficacy. Taking the vaccine can significantly reduce the spread of the virus and ultimately save lives.\n",
      "---\n",
      "Prediction 44: It is important to consider that these types of rare side effects can occur with any medication, including vaccinations. The benefits of receiving a COVID-19 vaccine far outweigh the risks of experiencing temporary skin irritation or other minor issues. It is crucial to trust in the rigorous testing process and scientific research behind vaccines to protect public health. #TrustVaccines #PublicHealthMatters\n",
      "Reference 44: It is important to remember that the occurrence of side effects, such as blisters, after receiving a vaccine is extremely rare and should not deter individuals from getting vaccinated. The benefits of the vaccine in preventing the spread of COVID-19 far outweigh the risks of potential side effects.\n",
      "---\n",
      "Prediction 45: While lymphadenopathy may be reported in some individuals after receiving COVID-19 vaccinations, it's important to consider the overall benefits of vaccination. The majority of people experience mild or no side effects from COVID-19 vaccines compared to potentially severe illness caused by the virus itself. It's also worth noting that there has been extensive research and monitoring of vaccine safety, including the case you mentioned. Incidental findings like lymphadenopathy do not necessarily indicate a serious health risk. Vaccinations save lives and prevent the\n",
      "Reference 45: While it is important to acknowledge and monitor potential side effects of COVID vaccines, it is essential to put these occurrences into perspective. The benefits of vaccination, such as preventing severe illness and death from COVID-19, far outweigh the potential risks of rare side effects. Additionally, it is important to consider that correlation does not imply causation and further investigation is needed to determine the cause of any adverse events following vaccination.\n",
      "---\n",
      "Prediction 46: While it's understandable to feel frustrated with the constant changes in rules, it's important to recognize that these measures are put in place to protect public health. The government has a responsibility to prioritize safety during a global pandemic, and allowing complete freedom without any regulations could result in more harm than good. It's essential to trust in the expertise of healthcare professionals and follow guidelines to help control the spread of COVID-19. #PublicHealthFirst.\n",
      "Reference 46: While it is understandable to be frustrated with changing rules and restrictions, it is important to prioritize public health and safety during a global pandemic. The administration's efforts to pass a recovery bill and promote vaccination numbers show a commitment to combating the virus and ultimately allowing for more freedom in the long run. It is necessary to adapt to new information and guidelines in order to protect the well-being of all citizens.\n",
      "---\n",
      "Prediction 47: While some may have concerns about the safety and efficacy of COVID-19 vaccines, it is important to consider that widespread vaccination has been crucial in controlling the spread of the virus and saving lives. It is essential to trust scientific research and experts in public health to protect both individual and community well-being. Forcing vaccinations could ultimately lead to more harm than good if not implemented properly. #TrustTheScience #PublicHealthFirst\n",
      "Reference 47: Counter argument: The development and distribution of vaccines for Covid-19 are essential in order to protect public health and prevent further spread of the virus. While there may be concerns about individual liberties, widespread vaccination is crucial in order to achieve herd immunity and ultimately save lives.\n",
      "---\n",
      "Prediction 48: It is important to remember that correlation does not equal causation. Just because a person died shortly after receiving the Pfizer vaccine does not necessarily mean that the vaccine caused their death. It is crucial to wait for thorough investigation and evidence before jumping to conclusions about the safety of the vaccine. Vaccines go through rigorous testing and have been proven to be effective in preventing illness, so it is essential to trust in scientific research rather than anecdotal reports. #TrustInScience #PublicHealthMatters.\n",
      "Reference 48: While any death following a vaccination is concerning, it is important to remember that millions of people have safely received the Pfizer vaccine with no severe side effects. It is crucial to wait for a thorough investigation and analysis of the nurse's death before jumping to conclusions about the safety of the vaccine.\n",
      "---\n",
      "Prediction 49: While it's true that some masks may have a lower efficacy rating, wearing any form of protective gear can still help reduce transmission of the virus. Additionally, there have been numerous studies showing the effectiveness and safety of COVID-19 vaccines such as Moderna. It's important to trust scientific research rather than spreading misinformation about rare occurrences like Bellâ€™s palsy or claiming false connections between vaccines and negative outcomes. Vaccines save lives by preventing severe illness and death from the virus. #TrustTheScience #Protect\n",
      "Reference 49: While it is true that the COVID virus is small, the mask is not meant to completely block the virus, but rather reduce the spread of respiratory droplets that may contain the virus. Wearing a mask, along with practicing social distancing and proper hygiene, can still help prevent the spread of COVID-19. Additionally, the claims about the Moderna vaccine causing Bell's palsy, sterilization, and immune system shutdown are not supported by scientific evidence and have been debunked by reputable health organizations. It is important to fact-check information before spreading misinformation.\n",
      "---\n",
      "Prediction 50: Actually, getting vaccinated can greatly reduce the likelihood of contracting COVID-19 or experiencing severe symptoms if infected. It's important to follow public health guidelines and get vaccinated as part of overall prevention strategies against the virus. #GetVaccinated #ProtectYourselfAndOthers.\n",
      "Reference 50: While the vaccine may not be a cure, it is an important tool in preventing the spread of the virus and reducing the severity of symptoms for those who do contract it. It plays a crucial role in building herd immunity and protecting vulnerable populations.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "num_samples=50\n",
    "prompts = [pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": dataset_chatml['test'][i]['messages'][0]['content']}], tokenize=False, add_generation_prompt=True)\n",
    "                                              for i in range(num_samples)]\n",
    "outputs = pipe(prompts, batch_size=1, max_new_tokens=100, do_sample=True, num_beams=1, temperature=0.7, top_k=50, top_p=0.9,repetition_penalty=1.2,\n",
    "                   max_time= 180)\n",
    "# print(outputs)\n",
    "preds = []\n",
    "for i in range(len(outputs)):\n",
    "    generated_text = outputs[i][0]['generated_text']\n",
    " \n",
    "    response = generated_text[len(prompts[i]):].split()\n",
    "    # print(response)\n",
    "    if len(response) > 1:\n",
    "        # Extract the counter argument\n",
    "        pred = \" \".join(response)\n",
    "    else:\n",
    "        pred = \"\"  # Handle case with no valid split\n",
    "    preds.append(pred)\n",
    "\n",
    "    # Print prediction and corresponding reference\n",
    "    print(f\"Prediction {i + 1}: {pred}\")\n",
    "    print(f\"Reference {i + 1}: {dataset_chatml['test'][i]['counter_argument']}\")\n",
    "    print(\"---\")  # Separator for clarity\n",
    "references= [dataset_chatml['test'][i]['counter_argument'] for i in range(len(outputs))]\n",
    "metric.add_batch(predictions=preds, references=references)\n",
    "tweets = [dataset_chatml['test'][i]['text'] for i in range(len(outputs))]\n",
    "\n",
    "# store the tweets predictions and references in a csv file \n",
    "import pandas as pd\n",
    "df = pd.DataFrame(list(zip(tweets, preds, references)), columns =['tweets', 'predictions', 'references'])\n",
    "df.to_csv('/home/sohampoddar/HDD2/utsav/Cross_Experiment_2/llama-3.2-3B_cross_2_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'text', 'counter_argument', 'messages'],\n",
       "    num_rows: 247\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_chatml['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['side-effect']\n",
      "1\n",
      "['country', 'rushed']\n",
      "2\n",
      "['conspiracy', 'unnecessary']\n",
      "3\n",
      "['conspiracy', 'political']\n",
      "4\n",
      "['rushed']\n",
      "5\n",
      "['side-effect']\n",
      "6\n",
      "['side-effect']\n",
      "7\n",
      "['side-effect']\n",
      "8\n",
      "['none']\n",
      "9\n",
      "['ineffective']\n",
      "10\n",
      "['side-effect']\n",
      "11\n",
      "['political']\n",
      "12\n",
      "['ingredients', 'side-effect']\n",
      "13\n",
      "['ingredients', 'side-effect']\n",
      "14\n",
      "['rushed']\n",
      "15\n",
      "['side-effect']\n",
      "16\n",
      "['mandatory']\n",
      "17\n",
      "['ineffective']\n",
      "18\n",
      "['side-effect']\n",
      "19\n",
      "['ineffective']\n",
      "20\n",
      "['rushed', 'side-effect']\n",
      "21\n",
      "['rushed']\n",
      "22\n",
      "['side-effect']\n",
      "23\n",
      "['conspiracy', 'side-effect']\n",
      "24\n",
      "['side-effect']\n",
      "25\n",
      "['ineffective']\n",
      "26\n",
      "['side-effect']\n",
      "27\n",
      "['rushed']\n",
      "28\n",
      "['ingredients', 'side-effect']\n",
      "29\n",
      "['side-effect']\n",
      "30\n",
      "['pharma']\n",
      "31\n",
      "['ineffective']\n",
      "32\n",
      "['side-effect', 'unnecessary']\n",
      "33\n",
      "['conspiracy', 'pharma']\n",
      "34\n",
      "['none']\n",
      "35\n",
      "['side-effect']\n",
      "36\n",
      "['pharma']\n",
      "37\n",
      "['pharma', 'rushed']\n",
      "38\n",
      "['rushed']\n",
      "39\n",
      "['side-effect']\n",
      "40\n",
      "['pharma']\n",
      "41\n",
      "['side-effect']\n",
      "42\n",
      "['side-effect']\n",
      "43\n",
      "['none']\n",
      "44\n",
      "['pharma']\n",
      "45\n",
      "['rushed']\n",
      "46\n",
      "['rushed']\n",
      "47\n",
      "['side-effect']\n",
      "48\n",
      "['side-effect']\n",
      "49\n",
      "['side-effect']\n",
      "50\n",
      "['unnecessary']\n",
      "51\n",
      "['none']\n",
      "52\n",
      "['rushed']\n",
      "53\n",
      "['side-effect']\n",
      "54\n",
      "['mandatory', 'political', 'rushed']\n",
      "55\n",
      "['none']\n",
      "56\n",
      "['mandatory']\n",
      "57\n",
      "['side-effect']\n",
      "58\n",
      "['pharma']\n",
      "59\n",
      "['side-effect']\n"
     ]
    }
   ],
   "source": [
    "for i in range(60):\n",
    "    # extract the tweet from the dataset print only <|user|> part  and print the index also\n",
    "    print(i)\n",
    "    print(dataset_chatml['test']['labels'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.add_batch(predictions=preds, references=references)\n",
    "result = metric.compute(use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge 1 Mean:  40.74679080969241\n",
      "Rouge 2 Mean:  13.048979913641409\n",
      "Rouge L Mean:  24.546677017482402\n",
      "Rouge Lsum Mean:  24.576874683476557\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming result contains your ROUGE scores\n",
    "rouge1_mean = np.mean(result['rouge1']) * 100\n",
    "rouge2_mean = np.mean(result['rouge2']) * 100\n",
    "rougeL_mean = np.mean(result['rougeL']) * 100\n",
    "rougeLsum_mean = np.mean(result['rougeLsum']) * 100\n",
    "\n",
    "print(\"Rouge 1 Mean: \", rouge1_mean)\n",
    "print(\"Rouge 2 Mean: \", rouge2_mean)\n",
    "print(\"Rouge L Mean: \", rougeL_mean)\n",
    "print(\"Rouge Lsum Mean: \", rougeLsum_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-score in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (0.3.13)\n",
      "Requirement already satisfied: torch>=1.0.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (2.3.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (2.0.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (4.47.0.dev0)\n",
      "Requirement already satisfied: numpy in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (1.25.1)\n",
      "Requirement already satisfied: requests in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (4.65.0)\n",
      "Requirement already satisfied: matplotlib in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (3.7.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from bert-score) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score) (2023.3)\n",
      "Requirement already satisfied: filelock in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (1.12)\n",
      "Requirement already satisfied: networkx in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (0.26.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (0.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (4.41.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from matplotlib->bert-score) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests->bert-score) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests->bert-score) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests->bert-score) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from requests->bert-score) (2023.7.22)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->bert-score) (3.16.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/sohampoddar/anaconda3/envs/dl/lib/python3.9/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n",
      "\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 19:55:31] Energy consumed for RAM : 0.034489 kWh. RAM Power : 47.068857192993164 W\n",
      "[codecarbon INFO @ 19:55:31] Energy consumed for all GPUs : 0.115896 kWh. Total GPU Power : 74.01231344139399 W\n",
      "[codecarbon INFO @ 19:55:31] Energy consumed for all CPUs : 0.030937 kWh. Total CPU Power : 32.94468448069056 W\n",
      "[codecarbon INFO @ 19:55:31] 0.181323 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/vocab.json\n",
      "loading file merges.txt from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer.json\n",
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/sohampoddar/HDD2/hfcache/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/model.safetensors\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 53.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.45 seconds, 110.59 sentences/sec\n",
      "BERTScore Precision: 0.8752923607826233\n",
      "BERTScore Recall: 0.8953772783279419\n",
      "BERTScore F1: 0.8851920962333679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "P, R, F1 = score(preds, references, lang=\"en\", verbose=True)\n",
    "\n",
    "# Display BERTScore results\n",
    "print(\"BERTScore Precision:\", P.mean().item())\n",
    "print(\"BERTScore Recall:\", R.mean().item())\n",
    "print(\"BERTScore F1:\", F1.mean().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
